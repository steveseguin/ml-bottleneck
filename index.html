<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ML System Bottleneck Analyzer - Hardware Performance Analysis Tool</title>
    
    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAzMiAzMiI+CiAgPGRlZnM+CiAgICA8bGluZWFyR3JhZGllbnQgaWQ9ImdyYWQiIHgxPSIwJSIgeTE9IjAlIiB4Mj0iMTAwJSIgeTI9IjEwMCUiPgogICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdHlsZT0ic3RvcC1jb2xvcjojNjM2NmYxO3N0b3Atb3BhY2l0eToxIiAvPgogICAgICA8c3RvcCBvZmZzZXQ9IjEwMCUiIHN0eWxlPSJzdG9wLWNvbG9yOiM0ZjQ2ZTU7c3RvcC1vcGFjaXR5OjEiIC8+CiAgICA8L2xpbmVhckdyYWRpZW50PgogIDwvZGVmcz4KICA8cmVjdCB4PSI0IiB5PSI0IiB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHJ4PSI0IiBmaWxsPSJ1cmwoI2dyYWQpIi8+CiAgPHBhdGggZD0iTTEwIDE2IEwxNCAyMCBMMjIgMTIiIHN0cm9rZT0id2hpdGUiIHN0cm9rZS13aWR0aD0iMiIgZmlsbD0ibm9uZSIvPgogIDxwYXRoIGQ9Ik04IDIyIEwyNCAyMiIgc3Ryb2tlPSJ3aGl0ZSIgc3Ryb2tlLXdpZHRoPSIyIiBmaWxsPSJub25lIi8+CiAgPHBhdGggZD0iTTggMjYgTDIwIDI2IiBzdHJva2U9IndoaXRlIiBzdHJva2Utd2lkdGg9IjEuNSIgZmlsbD0ibm9uZSIgb3BhY2l0eT0iMC43Ii8+Cjwvc3ZnPg==">

    <!-- Primary Meta Tags -->
    <meta name="title" content="ML System Bottleneck Analyzer - Hardware Performance Analysis Tool">
    <meta name="description" content="Analyze and visualize hardware bottlenecks in machine learning systems. Get real-time insights into memory, bandwidth, and compute utilization across multiple devices.">
    <meta name="keywords" content="ML, machine learning, hardware bottleneck, performance analysis, GPU, CPU, token rate, memory utilization, bandwidth analysis, compute utilization">
    <meta name="author" content="Steve Seguin">
    <meta name="theme-color" content="#6366f1">

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://mlbottleneck.com/">
    <meta property="og:title" content="ML System Bottleneck Analyzer">
    <meta property="og:description" content="Web-based tool for analyzing hardware bottlenecks in ML systems. Visualize performance limitations across distributed setups - right in your browser!">
    <meta property="og:image" content="https://mlbottleneck.com/logo.png">

    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://mlbottleneck.com/">
    <meta property="twitter:title" content="ML System Bottleneck Analyzer">
    <meta property="twitter:description" content="Web-based tool for analyzing hardware bottlenecks in ML systems. Visualize performance limitations across distributed setups - right in your browser!">
    <meta property="twitter:image" content="https://mlbottleneck.com/logo.png">
    <meta name="twitter:creator" content="@xyster">

    <!-- Additional Meta -->
    <meta name="application-name" content="ML Bottleneck Analyzer">
    <meta name="apple-mobile-web-app-title" content="ML Bottleneck">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="default">
    <meta name="format-detection" content="telephone=no">
    <meta name="mobile-web-app-capable" content="yes">

    <!-- Preload Critical Resources -->
    <link rel="preconnect" href="https://cdnjs.cloudflare.com">

    <!-- Chart.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/3.7.0/chart.min.js"></script>
	<!-- Structured Data for Google -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "WebApplication",
      "name": "ML System Bottleneck Analyzer",
      "description": "Web-based tool for analyzing hardware bottlenecks in machine learning systems. Visualize performance limitations across distributed setups - right in your browser!",
      "url": "https://mlbottleneck.com",
      "author": {
        "@type": "Person",
        "name": "Steve Seguin"
      },
      "applicationCategory": "Machine Learning Tools",
      "operatingSystem": "Any",
      "browserRequirements": "Requires JavaScript",
      "offers": {
        "@type": "Offer",
        "price": "0",
        "priceCurrency": "USD"
      }
    }
    </script>
    <style>
	
@media (prefers-color-scheme: dark) {
:root {
    --primary: #6366f1;
    --primary-hover: #4f46e5;
    --destructive: #ef4444;
    --destructive-hover: #dc2626;
    --border: #1e293b;
    --background: #0b0f1a;
    --card: #1e293b;
    --text: #f8fafc;
    --text-secondary: #94a3b8;
    --radius: 0.5rem;
    --shadow: 0 4px 6px -1px rgb(0 0 0 / 0.3), 0 2px 4px -2px rgb(0 0 0 / 0.2);
    --transition: all 0.2s cubic-bezier(0.4, 0, 0.2, 1);
}

* {
    box-sizing: border-box;
}

body {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, system-ui, sans-serif;
    margin: 0;
    padding: 2rem;
    max-width: 1400px;
    margin: 0 auto;
    background: var(--background);
    color: var(--text);
    line-height: 1.5;
}

h1 {
    font-size: 2.25rem;
    font-weight: 700;
    margin: 2rem 0;
    background: linear-gradient(135deg, var(--primary), var(--primary-hover));
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    letter-spacing: -0.025em;
}

h2 {
    font-size: 1.5rem;
    font-weight: 600;
    margin: 1.5rem 0;
    color: var(--text);
    letter-spacing: -0.025em;
}

h3 {
    font-size: 1.25rem;
    font-weight: 600;
    margin: 1rem 0;
    color: var(--text);
}

a {
    color: var(--primary);
    text-decoration: none;
    transition: var(--transition);
}

a:hover {
    color: var(--primary-hover);
    text-decoration: underline;
}

.card {
    background: var(--card);
    border: 1px solid var(--border);
    border-radius: var(--radius);
    padding: 1.5rem;
    margin-bottom: 1.5rem;
    box-shadow: var(--shadow);
    transition: var(--transition);
    backdrop-filter: blur(8px);
}

.card:hover {
    transform: translateY(-2px);
    box-shadow: 0 10px 15px -3px rgb(0 0 0 / 0.3), 0 4px 6px -4px rgb(0 0 0 / 0.2);
    border-color: var(--primary);
}

.grid {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(min(400px, 100%), 1fr));
    gap: 1.5rem;
    margin: 2rem 0;
}

.device {
    background: var(--background);
    border: 1px solid var(--border);
    padding: 1.5rem;
    margin-bottom: 1rem;
    border-radius: var(--radius);
    transition: var(--transition);
}

.device:hover {
    border-color: var(--primary);
    transform: translateY(-1px);
}

.button {
    background: var(--primary);
    color: white;
    border: none;
    padding: 0.75rem 1.5rem;
    border-radius: var(--radius);
    cursor: pointer;
    font-weight: 500;
    transition: var(--transition);
    font-size: 0.875rem;
    line-height: 1.25rem;
}

.button:hover {
    background: var(--primary-hover);
    transform: translateY(-1px);
}

.button:active {
    transform: translateY(0);
}

.button:disabled {
    opacity: 0.5;
    cursor: not-allowed;
}

.button-destructive {
    background: var(--destructive);
}

.button-destructive:hover {
    background: var(--destructive-hover);
}

.input, select {
    width: 100%;
    padding: 0.75rem;
    border: 1px solid var(--border);
    border-radius: var(--radius);
    margin-bottom: 1rem;
    transition: var(--transition);
    color: var(--text);
    background: var(--background);
    font-size: 0.875rem;
    line-height: 1.25rem;
}

.input:hover, select:hover {
    border-color: var(--primary);
}

.input:focus, select:focus {
    outline: none;
    border-color: var(--primary);
    box-shadow: 0 0 0 3px rgba(99, 102, 241, 0.1);
}

.input:disabled, select:disabled {
    background: #334155;
    cursor: not-allowed;
    opacity: 0.75;
}

.alert {
    background: #881337;
    border: 1px solid var(--destructive);
    padding: 1.5rem;
    border-radius: var(--radius);
    margin-bottom: 1.5rem;
    color: #fecdd3;
}

.device-grid {
    display: grid;
    grid-template-columns: repeat(2, 1fr);
    gap: 1rem;
}

label {
    display: block;
    margin-bottom: 0.5rem;
    font-size: 0.875rem;
    font-weight: 500;
    color: var(--text-secondary);
}

.chart-container {
    height: 400px;
    margin-bottom: 1.5rem;
    padding: 1rem;
    background: var(--background);
    border-radius: var(--radius);
    border: 1px solid var(--border);
    transition: var(--transition);
}

.chart-container:hover {
    border-color: var(--primary);
}

.inline-select-group {
    display: flex;
    gap: 1rem;
    margin-bottom: 1rem;
    align-items: center;
}

.inline-select-group label {
    margin-bottom: 0;
    margin-right: 0.5rem;
    flex-shrink: 0;
    width: auto;
    white-space: nowrap;
}

.inline-select-group select {
    flex-grow: 1;
    margin-bottom: 0;
    min-width: 0;
}

table {
    width: 100%;
    margin: 1.5rem 0;
    border-collapse: separate;
    border-spacing: 0;
    border: 1px solid var(--border);
    border-radius: var(--radius);
    overflow: hidden;
    background: var(--card);
}

th, td {
    padding: 0.75rem 1rem;
    text-align: left;
    border-bottom: 1px solid var(--border);
    font-size: 0.875rem;
}

th {
    background: var(--background);
    font-weight: 600;
    position: relative;
    cursor: pointer;
    user-select: none;
    transition: var(--transition);
}

th:hover {
    background: #334155;
}

th::after {
    content: '';
    position: absolute;
    right: 0.75rem;
    top: 50%;
    transform: translateY(-50%);
    opacity: 0.5;
}

th.asc::after {
    content: '▲';
}

th.desc::after {
    content: '▼';
}

tr:last-child td {
    border-bottom: none;
}

tr:nth-child(even) {
    background-color: rgba(30, 41, 59, 0.5);
}

tr:hover {
    background-color: rgba(99, 102, 241, 0.1);
}

.filters {
    margin: 1.5rem 0;
    display: flex;
    gap: 1rem;
    flex-wrap: wrap;
    background: var(--card);
    padding: 1.5rem;
    border-radius: var(--radius);
    border: 1px solid var(--border);
}

.filter-group {
    display: flex;
    flex-direction: column;
    gap: 0.5rem;
    flex: 1;
    min-width: 200px;
}

@media (max-width: 768px) {
    body {
        padding: 1rem;
    }

    .grid {
        grid-template-columns: 1fr;
    }

    .device-grid {
        grid-template-columns: 1fr;
    }

    .inline-select-group {
        flex-direction: column;
        gap: 0.5rem;
    }

    .inline-select-group select {
        width: 100%;
    }

    .filters {
        flex-direction: column;
    }

    .filter-group {
        width: 100%;
    }

    h1 {
        font-size: 1.75rem;
    }

    h2 {
        font-size: 1.25rem;
    }
}
}
@media (prefers-color-scheme: light) {
 :root {
    --primary: #6366f1;
    --primary-hover: #4f46e5;
    --destructive: #ef4444;
    --destructive-hover: #dc2626;
    --border: #e2e8f0;
    --background: #ffffff;
    --card: #ffffff;
    --text: #0f172a;
    --text-secondary: #475569;
    --radius: 0.75rem;
    --shadow: 0 4px 6px -1px rgb(0 0 0 / 0.1), 0 2px 4px -2px rgb(0 0 0 / 0.1);
}

body {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, system-ui, sans-serif;
    margin: 0;
    padding: 2rem;
    max-width: 1400px;
    margin: 0 auto;
    background: #f8fafc;
    color: var(--text);
}

h1 {
    font-size: 2.25rem;
    font-weight: 700;
    margin-bottom: 2rem;
    background: linear-gradient(to right, var(--primary), var(--primary-hover));
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
}

h2 {
    font-size: 1.5rem;
    font-weight: 600;
    margin-bottom: 1.5rem;
    color: var(--text);
}

h3 {
    font-size: 1.25rem;
    font-weight: 600;
    margin-bottom: 1rem;
    color: var(--text);
}

.card {
    background: var(--card);
    border: 1px solid var(--border);
    border-radius: var(--radius);
    padding: 1.5rem;
    margin-bottom: 1.5rem;
    box-shadow: var(--shadow);
    transition: transform 0.2s, box-shadow 0.2s;
}

.card:hover {
    transform: translateY(-2px);
    box-shadow: 0 10px 15px -3px rgb(0 0 0 / 0.1), 0 4px 6px -4px rgb(0 0 0 / 0.1);
}

.grid {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
    gap: 1.5rem;
}

.device {
    background: var(--background);
    border: 1px solid var(--border);
    padding: 1.5rem;
    margin-bottom: 1rem;
    border-radius: var(--radius);
    transition: border-color 0.2s;
}

.device:hover {
    border-color: var(--primary);
}

.button {
    background: var(--primary);
    color: white;
    border: none;
    padding: 0.75rem 1.5rem;
    border-radius: var(--radius);
    cursor: pointer;
    font-weight: 500;
    transition: background-color 0.2s;
}

.button:hover {
    background: var(--primary-hover);
}

.button-destructive {
    background: var(--destructive);
}

.button-destructive:hover {
    background: var(--destructive-hover);
}

.input, select {
    width: 100%;
    padding: 0.75rem;
    border: 1px solid var(--border);
    border-radius: var(--radius);
    margin-bottom: 1rem;
    transition: border-color 0.2s, box-shadow 0.2s;
    color: var(--text);
    background: var(--background);
}

.input:focus, select:focus {
    outline: none;
    border-color: var(--primary);
    box-shadow: 0 0 0 3px rgba(99, 102, 241, 0.1);
}

.input:disabled, select:disabled {
    background: #f1f5f9;
    cursor: not-allowed;
}

.alert {
    background: #fef2f2;
    border: 1px solid var(--destructive);
    padding: 1.5rem;
    border-radius: var(--radius);
    margin-bottom: 1.5rem;
    color: var(--destructive);
}

.device-grid {
    display: grid;
    grid-template-columns: repeat(2, 1fr);
    gap: 1rem;
}

label {
    display: block;
    margin-bottom: 0.5rem;
    font-size: 0.875rem;
    font-weight: 500;
    color: var(--text-secondary);
}

.chart-container {
    height: 400px;
    margin-bottom: 1.5rem;
    padding: 1rem;
    background: var(--background);
    border-radius: var(--radius);
    border: 1px solid var(--border);
}

.inline-select-group {
    display: flex;
    gap: 1rem;
    margin-bottom: 1rem;
    align-items: center;
}

.inline-select-group label {
    margin-bottom: 0;
    margin-right: 0.5rem;
    flex-shrink: 0;
    width: auto;
}

.inline-select-group select {
    flex-grow: 1;
    margin-bottom: 0;
}

table {
	border-collapse: collapse;
	width: 100%;
	margin: 20px 0;
	font-family: Arial, sans-serif;
}
th, td {
	border: 1px solid #ddd;
	padding: 8px;
	text-align: left;
}
th {
	background-color: #f5f5f5;
	position: relative;
	cursor: pointer;
}
th:hover {
	background-color: #eee;
}
th::after {
	content: '';
	position: absolute;
	right: 8px;
	top: 50%;
	transform: translateY(-50%);
}
th.asc::after {
	content: '▲';
}
th.desc::after {
	content: '▼';
}
tr:nth-child(even) {
	background-color: #f9f9f9;
}
.filters {
	margin: 20px 0;
	display: flex;
	gap: 10px;
	flex-wrap: wrap;
}
.filter-group {
	display: flex;
	flex-direction: column;
	gap: 5px;
}
input, select {
	padding: 5px;
	border: 1px solid #ddd;
	border-radius: 4px;
}

.device {
    position: relative;
    transition: all 0.2s ease;
}
.device.custom-device {
    border-color: var(--primary);
    background: rgba(99, 102, 241, 0.05);
}
.custom-badge {
    position: absolute;
    top: 0.5rem;
    right: 0.5rem;
    background: var(--primary);
    color: white;
    font-size: 0.7rem;
    padding: 0.1rem 0.4rem;
    border-radius: 0.25rem;
    opacity: 0.8;
}
.device-actions {
    display: flex;
    gap: 0.5rem;
}
.device-actions button {
    padding: 0.5rem 0.75rem;
    font-size: 0.8rem;
}

/* Collapsible section styles */
.collapsible-header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    cursor: pointer;
    padding: 0.5rem 0;
    user-select: none;
}
.collapsible-header h2 {
    margin: 0;
    display: flex;
    align-items: center;
    gap: 0.5rem;
}
.collapsible-header .toggle-icon {
    font-size: 0.8rem;
    color: var(--text-secondary);
    transition: transform 0.2s ease;
}
.collapsible-header.collapsed .toggle-icon {
    transform: rotate(-90deg);
}
.collapsible-content {
    overflow: hidden;
    transition: max-height 0.3s ease-out, opacity 0.2s ease-out;
}
.collapsible-content.collapsed {
    max-height: 0;
    opacity: 0;
    padding: 0;
}

/* Quick summary badge */
.quick-summary {
    font-size: 0.75rem;
    color: var(--text-secondary);
    background: rgba(255, 255, 255, 0.1);
    padding: 0.25rem 0.5rem;
    border-radius: 4px;
    font-weight: normal;
}

/* Improved card transitions */
.card {
    transition: all 0.2s ease;
}
.card:hover {
    border-color: var(--primary);
}

/* Progress indicator */
.progress-bar {
    height: 4px;
    background: var(--border);
    border-radius: 2px;
    overflow: hidden;
    margin-top: 0.5rem;
}
.progress-bar-fill {
    height: 100%;
    background: linear-gradient(90deg, #4ade80, #22c55e);
    transition: width 0.3s ease;
}
.progress-bar-fill.warning {
    background: linear-gradient(90deg, #fbbf24, #f59e0b);
}
.progress-bar-fill.danger {
    background: linear-gradient(90deg, #f87171, #ef4444);
}

@media (max-width: 768px) {
    body {
        padding: 1rem;
    }

    .grid {
        grid-template-columns: 1fr;
    }

    .device-grid {
        grid-template-columns: 1fr;
    }

    .inline-select-group {
        flex-direction: column;
        gap: 0.5rem;
    }

    .inline-select-group select {
        width: 100%;
    }
}
</style>
</head>
<body>
    <h1>ML System Bottleneck Analyzer</h1>

    <div class="grid">
        <div class="card">
            <div class="collapsible-header" onclick="toggleCollapsible(this)">
                <h2>Model Configuration <span class="toggle-icon">▼</span></h2>
                <span class="quick-summary" id="modelSummary">Llama3 8b @ Q4</span>
            </div>
            <div class="collapsible-content" id="modelConfigContent">
                <div class="inline-select-group">
                    <label for="modelPreset">Model Preset</label>
                    <select id="modelPreset" class="input">
                        <option value="">Custom</option>
                        <optgroup label="Llama">
                            <option value="llama3_8b" selected>Llama 3 8B</option>
                            <option value="llama3_70b">Llama 3 70B</option>
                            <option value="llama3.3_70b">Llama 3.3 70B</option>
                            <option value="llama3.2_3b">Llama 3.2 3B</option>
                            <option value="llama3.2_1b">Llama 3.2 1B</option>
                            <option value="llama3.2_90b_vision">Llama 3.2 90B Vision</option>
                            <option value="llama3.2_11b_vision">Llama 3.2 11B Vision</option>
                        </optgroup>
                        <optgroup label="DeepSeek">
                            <option value="deepseek_r1">DeepSeek R1 (671B MoE)</option>
                            <option value="deepseek_v3_671b">DeepSeek V3 (671B)</option>
                            <option value="deepseek_r1_distill_70b">DeepSeek R1 Distill 70B</option>
                            <option value="deepseek_r1_distill_32b">DeepSeek R1 Distill 32B</option>
                            <option value="deepseek_r1_distill_14b">DeepSeek R1 Distill 14B</option>
                            <option value="deepseek_r1_distill_8b">DeepSeek R1 Distill 8B</option>
                            <option value="deepseek_r1_distill_1.5b">DeepSeek R1 Distill 1.5B</option>
                        </optgroup>
                        <optgroup label="Qwen">
                            <option value="qwen2.5_72b">Qwen 2.5 72B</option>
                            <option value="qwen2.5_32b">Qwen 2.5 32B</option>
                            <option value="qwen2.5_14b">Qwen 2.5 14B</option>
                            <option value="qwen2.5_7b">Qwen 2.5 7B</option>
                            <option value="qwen2.5_3b">Qwen 2.5 3B</option>
                            <option value="qwen2.5_coder_32b">Qwen 2.5 Coder 32B</option>
                        </optgroup>
                        <optgroup label="Mistral">
                            <option value="mistral_7b">Mistral 7B</option>
                            <option value="mistral_large_2_123b">Mistral Large 2 (123B)</option>
                            <option value="mistral_nemo_12b">Mistral Nemo 12B</option>
                            <option value="mixtral_8x7b">Mixtral 8x7B</option>
                            <option value="mixtral_8x22b">Mixtral 8x22B</option>
                        </optgroup>
                        <optgroup label="Other LLMs">
                            <option value="glm4_9b">GLM-4 9B</option>
                            <option value="command_r_plus_104b">Command R+ 104B</option>
                            <option value="gemma3_27b">Gemma 3 27B</option>
                            <option value="phi3_14b">Phi-3 14B</option>
                            <option value="phi3_3.8b">Phi-3 3.8B</option>
                        </optgroup>
                        <optgroup label="Image Generation">
                            <option value="sdxl_base">SDXL Base (3.5B)</option>
                            <option value="flux1_dev">Flux.1 Dev (12B)</option>
                            <option value="sd3_medium">SD3 Medium (2B)</option>
                        </optgroup>
                        <optgroup label="Test Models">
                            <option value="large_model_400b">Large Model (400B+)</option>
                            <option value="very_large_model_1kb">Very Large Model (1T+)</option>
                        </optgroup>
                    </select>

                    <label for="quantizationType">Quantization</label>
                    <select id="quantizationType" class="input">
                        <option value="q4" selected>Q4 (Recommended)</option>
                        <option value="int8">INT8</option>
                        <option value="float16">FP16</option>
                        <option value="bfloat16">BF16</option>
                        <option value="float32">FP32</option>
                    </select>
                </div>

                <label>Total Parameters (B)</label>
                <input type="number" id="totalParamsB" class="input" value="400">

                <label>Batch Size</label>
                <input type="number" id="batchSize" class="input" value="1">

                <label>Sequence Length</label>
                <input type="number" id="seqLength" class="input" value="2048">

                <label>Hidden Size</label>
                <input type="number" id="hiddenSize" class="input" value="16384">

                <label>Number of Layers</label>
                <input type="number" id="numLayers" class="input" value="120">

                <label>Number of Heads</label>
                <input type="number" id="numHeads" class="input" value="128">

                <label style="display: none;">Data Type</label>
                <select id="dtype" class="input" style="display: none;">
                    <option value="float32">float32</option>
                    <option value="bfloat16">bfloat16</option>
                    <option value="float16">float16</option>
                    <option value="int8">int8</option>
                    <option value="q4" selected>q4</option>
                </select>

                <label>Parallelism Strategy</label>
                <select id="parallelismStrategy" class="input">
                    <option value="auto">AUTO (Find Optimal)</option>
                    <option value="pipeline">Pipeline Parallelism (PP)</option>
                    <option value="tensor">Tensor Parallelism (TP)</option>
                    <option value="data">Data Parallelism (DP)</option>
                    <option value="expert">Expert Parallelism (EP - MoE)</option>
                    <option value="sequence">Sequence Parallelism (SP)</option>
                    <option value="context">Context Parallelism (CP)</option>
                    <option value="hybrid_tp_pp">Hybrid TP+PP</option>
                    <option value="hybrid_tp_dp">Hybrid TP+DP</option>
                </select>

                <label>Optimization</label>
                <select id="optimizationMode" class="input">
                    <option value="none">Standard Inference</option>
                    <option value="speculative">Speculative Decoding (2-3x speedup)</option>
                    <option value="continuous_batching">Continuous Batching</option>
                    <option value="paged_attention">PagedAttention (vLLM)</option>
                    <option value="flash_attention">Flash Attention</option>
                    <option value="exo_phase_split">EXO Phase Split (Prefill/Decode)</option>
                </select>

                <label>Batch Size</label>
                <select id="batchSize" class="input">
                    <option value="1">1 (Interactive)</option>
                    <option value="4">4 (Small batch)</option>
                    <option value="8">8 (Medium batch)</option>
                    <option value="16">16 (Large batch)</option>
                    <option value="32">32 (High throughput)</option>
                    <option value="64">64 (Max throughput)</option>
                </select>
            </div>
        </div>
        </div>

	<div class="card">
	    <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 1rem;">
	        <h2>Devices</h2>
	    </div>
	    <div style="margin-bottom: 1rem;">
	        <label style="font-size: 0.875rem;">Quick Scenario Preset</label>
	        <select id="scenarioPreset" class="input" onchange="loadScenarioPreset(this.value)" style="margin-bottom: 0;">
	            <option value="">Custom Configuration</option>
	            <optgroup label="RTX 50-Series Multi-GPU">
	                <option value="5090_ddr5">RTX 5090 + DDR5 Overflow (36GB model)</option>
	                <option value="5090_5060ti_x1">RTX 5090 x16 + RTX 5060 Ti x1 (slow PCIe)</option>
	                <option value="5090_5060ti_x8">RTX 5090 x8 + RTX 5060 Ti x8 (balanced)</option>
	                <option value="5090_oculink">RTX 5090 x16 + GPU via Oculink (x4)</option>
	                <option value="dual_5090">2x RTX 5090 x16 (theoretical)</option>
	            </optgroup>
	            <optgroup label="RTX 40-Series">
	                <option value="4090_ddr5">RTX 4090 + DDR5 Overflow</option>
	                <option value="4090_4080">RTX 4090 + RTX 4080 (both x8)</option>
	            </optgroup>
	            <optgroup label="Apple Silicon">
	                <option value="m4_max">Mac M4 Max (128GB)</option>
	                <option value="m3_ultra">Mac M3 Ultra (512GB)</option>
	            </optgroup>
	            <optgroup label="Data Center">
	                <option value="h100_single">Single H100</option>
	                <option value="h100_x8">8x H100 (NVLink)</option>
	            </optgroup>
	        </select>
	    </div>
	    <div id="devices"></div>
	</div>
    </div>
	
    <div id="alerts"></div>

    <div class="grid">
        <div class="card">
            <h2>Resource Utilization</h2>
            <div class="chart-container">
                <canvas id="utilizationChart"></canvas>
            </div>
        </div>

        <div class="card">
            <h2>System Analysis <small>(Token rates are approximations)</small></h2>
            <div id="systemAnalysis"></div>
        </div>
    </div>

    <!-- Topology Visualization -->
    <div class="card" style="margin-bottom: 2rem;">
        <h2>System Topology <small>(Connection diagram)</small></h2>
        <div id="topologyContainer" style="position: relative; min-height: 300px; background: var(--background); border-radius: var(--radius); border: 1px solid var(--border); overflow: hidden;">
            <canvas id="topologyCanvas" style="width: 100%; height: 100%;"></canvas>
        </div>
        <div style="display: flex; gap: 1rem; margin-top: 0.75rem; flex-wrap: wrap; font-size: 0.75rem; color: var(--text-secondary);">
            <div><span style="display: inline-block; width: 20px; height: 3px; background: #4ade80; margin-right: 4px;"></span>NVLink (300+ GB/s)</div>
            <div><span style="display: inline-block; width: 20px; height: 3px; background: #60a5fa; margin-right: 4px;"></span>PCIe 5.0 (32-64 GB/s)</div>
            <div><span style="display: inline-block; width: 20px; height: 3px; background: #fbbf24; margin-right: 4px;"></span>PCIe 4.0 (8-32 GB/s)</div>
            <div><span style="display: inline-block; width: 20px; height: 3px; background: #f87171; margin-right: 4px;"></span>PCIe 3.0/DDR5 (&lt;16 GB/s)</div>
        </div>
    </div>

    <h1>Real-world results are below for reference</h1>
    <div class="filters">
        <div class="filter-group">
            <label for="modelFilter">Model:</label>
            <input type="text" id="modelFilter" placeholder="Filter models...">
        </div>
        <div class="filter-group">
            <label for="hardwareFilter">Hardware:</label>
            <input type="text" id="hardwareFilter" placeholder="Filter hardware...">
        </div>
        <div class="filter-group">
            <label for="quantizationFilter">Quantization:</label>
            <input type="text" id="quantizationFilter" placeholder="Filter quantization...">
        </div>
    </div>

    <table id="llmTable">
        <thead>
            <tr>
                <th data-sort="model">Model</th>
                <th data-sort="quantization">Quantization</th>
                <th data-sort="framework">Framework</th>
                <th data-sort="hardware">Hardware</th>
                <th data-sort="batchSize">Batch Size</th>
                <th data-sort="seqLength">Sequence Length</th>
                <th data-sort="tokenRateBatch">Token Rate (Batch)</th>
                <th data-sort="tokenRateSingle">Token Rate (Single)</th>
                <th data-sort="source">Source</th>
            </tr>
        </thead>
        <tbody></tbody>
    </table>

    <script>
        const DTYPE_SIZES = {
            'float32': 4,
            'bfloat16': 2,
            'float16': 2,
            'int8': 1,
            'q4': 0.5
        };

        // PCIe bandwidth in GB/s per direction (theoretical max)
        const PCIE_BANDWIDTH = {
            'pcie3': { x1: 1.0, x4: 4.0, x8: 8.0, x16: 16.0 },
            'pcie4': { x1: 2.0, x4: 8.0, x8: 16.0, x16: 32.0 },
            'pcie5': { x1: 4.0, x4: 16.0, x8: 32.0, x16: 64.0 }
        };

        // All interconnect types with bandwidth in GB/s
        const INTERCONNECT_BANDWIDTH = {
            // PCIe Variants
            'pcie3_x1': 1, 'pcie3_x4': 4, 'pcie3_x8': 8, 'pcie3_x16': 16,
            'pcie4_x1': 2, 'pcie4_x4': 8, 'pcie4_x8': 16, 'pcie4_x16': 32,
            'pcie5_x1': 4, 'pcie5_x4': 16, 'pcie5_x8': 32, 'pcie5_x16': 64,
            'pcie6_x16': 128,
            // Thunderbolt
            'thunderbolt3': 5,       // TB3: 40 Gbps = ~5 GB/s usable
            'thunderbolt4': 5,       // TB4: 40 Gbps = ~5 GB/s usable
            'thunderbolt5': 10,      // TB5: 80 Gbps = ~10 GB/s usable
            'thunderbolt5_rdma': 40, // TB5 with RDMA enabled (~40 GB/s, used by EXO)
            // Oculink
            'oculink_x4': 8,         // Oculink PCIe 4.0 x4
            'oculink_x8': 16,        // Oculink PCIe 4.0 x8
            // USB4
            'usb4_40': 5,            // USB4 40 Gbps
            'usb4_80': 10,           // USB4 80 Gbps
            'usb4_120': 15,          // USB4 120 Gbps (asymmetric)
            // Ethernet / Network
            '1gbe': 0.125,           // 1 Gbps
            '10gbe': 1.25,           // 10 Gbps
            '25gbe': 3.125,          // 25 Gbps
            '40gbe': 5,              // 40 Gbps
            '100gbe': 12.5,          // 100 Gbps
            '200gbe': 25,            // 200 Gbps
            '400gbe': 50,            // 400 Gbps
            '800gbe': 100,           // 800 Gbps
            // InfiniBand
            'ib_fdr': 6.8,           // FDR 56 Gbps
            'ib_edr': 12.5,          // EDR 100 Gbps
            'ib_hdr': 25,            // HDR 200 Gbps
            'ib_ndr': 50,            // NDR 400 Gbps
            'ib_xdr': 100,           // XDR 800 Gbps
            // NVLink / Proprietary
            'nvlink3': 300,          // NVLink 3.0 (A100): 600 GB/s bidirectional
            'nvlink4': 450,          // NVLink 4.0 (H100): 900 GB/s bidirectional
            'nvlink5': 900,          // NVLink 5.0 (B200): 1.8 TB/s bidirectional
            'nvswitch': 450,         // Through NVSwitch
            'infinity_fabric': 100,  // AMD Infinity Fabric (approx)
            'ultrapath': 600,        // Intel Ultrapath Interconnect
            // Apple Specific
            'ultrafusion': 2500      // Apple UltraFusion (M1/M2/M3 Ultra)
        };

        // Common overflow bandwidth scenarios in GB/s
        const OVERFLOW_BANDWIDTH = {
            'ddr5_7200': 115,      // Dual-channel DDR5-7200
            'ddr5_6400': 102,      // Dual-channel DDR5-6400
            'ddr5_5600': 90,       // Dual-channel DDR5-5600
            'ddr4_3200': 51,       // Dual-channel DDR4-3200
            'nvme_gen5': 14,       // PCIe 5.0 x4 NVMe
            'nvme_gen4': 7,        // PCIe 4.0 x4 NVMe
            'oculink': 8           // Oculink (PCIe 4.0 x4)
        };

        // Model types for different workloads
        const MODEL_TYPES = {
            'llm': {
                name: 'Large Language Model',
                metric: 'tokens/second',
                bottleneck: 'memory_bandwidth'
            },
            'image_gen': {
                name: 'Image Generation',
                metric: 'images/second',
                bottleneck: 'compute'
            }
        };

	const MODEL_PRESETS = {
	  'llama3_8b': {
	    totalParamsB: 8,
	    hiddenSize: 3072,
	    numLayers: 32,
	    numHeads: 32
	  },
	  'llama3_70b': {
	    totalParamsB: 70,
	    hiddenSize: 8192,
	    numLayers: 80,
	    numHeads: 64
	  },
	  'mistral_7b': {
	    totalParamsB: 7,
	    hiddenSize: 4096,
	    numLayers: 32,
	    numHeads: 32
	  },
	  'mistral_small_3.1_24b': {
	    totalParamsB: 24,
	    hiddenSize: 5120,
	    numLayers: 40,
	    numHeads: 32,
	    numKVHeads: 8,
	    intermediateSize: 32768,
	    maxPositionEmbeddings: 131072,
	    hasVision: true,
	    visionHiddenSize: 1024,
	    visionNumLayers: 24,
	    visionNumHeads: 16
	  },
	  'deepseek_v3_671b': {
	    totalParamsB: 671,
	    hiddenSize: 7168,
	    numLayers: 61,
	    numHeads: 128,
	    numKVHeads: 128,
	    isMoE: true,
	    numExperts: 256,
	    activeExperts: 8,
	    activeParamsB: 37
	  },
	  'phi3_14b': {
	    totalParamsB: 14,
	    hiddenSize: 5120,
	    numLayers: 32,
	    numHeads: 32,
	    numKVHeads: 8,
	    intermediateSize: 20480
	  },
	  'phi3_3.8b': {
	    totalParamsB: 3.8,
	    hiddenSize: 3072,
	    numLayers: 32,
	    numHeads: 32,
	    numKVHeads: 8,
	    intermediateSize: 12288
	  },
	  'mixtral_8x7b': {
	    totalParamsB: 47,
	    hiddenSize: 4096,
	    numLayers: 32,
	    numHeads: 32,
	    numKVHeads: 8,
	    isMoE: true,
	    numExperts: 8,
	    activeExperts: 2,
	    activeParamsB: 12.9
	  },
	  'mixtral_8x22b': {
	    totalParamsB: 141,
	    hiddenSize: 6144,
	    numLayers: 56,
	    numHeads: 48,
	    numKVHeads: 8,
	    isMoE: true,
	    numExperts: 8,
	    activeExperts: 2,
	    activeParamsB: 39
	  },
	  'gemma_7b': {
	    totalParamsB: 8.5,
	    hiddenSize: 3072,
	    numLayers: 28,
	    numHeads: 16,
	    numKVHeads: 1,
	    intermediateSize: 24576
	  },
	  'gemma_2b': {
	    totalParamsB: 2.5,
	    hiddenSize: 2048,
	    numLayers: 18,
	    numHeads: 8,
	    numKVHeads: 1,
	    intermediateSize: 16384
	  },
	  'gemma3_27b': {
	    totalParamsB: 27,
	    hiddenSize: 5376,
	    numLayers: 62,
	    numHeads: 32,
	    numKVHeads: 16,
	    intermediateSize: 21504,
	    slidingWindow: 1024,
	    hasVision: true,
	    visionHiddenSize: 1152,
	    visionNumLayers: 27,
	    visionNumHeads: 16,
	    imageSize: 896,
	    ropeScalingFactor: 8.0
	  },
	  'yi_34b': {
	    totalParamsB: 34,
	    hiddenSize: 7168,
	    numLayers: 60,
	    numHeads: 56,
	    numKVHeads: 8
	  },
	  'yi_large_200b': {
	    totalParamsB: 200,
	    hiddenSize: 12288,
	    numLayers: 80,
	    numHeads: 96,
	    numKVHeads: 12
	  },
	  'falcon_40b': {
	    totalParamsB: 40,
	    hiddenSize: 8192,
	    numLayers: 60,
	    numHeads: 128,
	    intermediateSize: 22016
	  },
	  'bloom_176b': {
	    totalParamsB: 176,
	    hiddenSize: 14336,
	    numLayers: 70,
	    numHeads: 112,
	    intermediateSize: 57344
	  },
	  'gpt_neox_20b': {
	    totalParamsB: 20,
	    hiddenSize: 6144,
	    numLayers: 44,
	    numHeads: 64
	  },
	  'mpt_30b': {
	    totalParamsB: 30,
	    hiddenSize: 7168,
	    numLayers: 48,
	    numHeads: 64
	  },
	  'vicuna_13b': {
	    totalParamsB: 13,
	    hiddenSize: 5120,
	    numLayers: 40,
	    numHeads: 40
	  },
	  'large_model_400b': {
	    totalParamsB: 400,
	    hiddenSize: 16384,
	    numLayers: 120,
	    numHeads: 128
	  },
	  'very_large_model_1kb': {
	    totalParamsB: 1000,
	    hiddenSize: 32768,
	    numLayers: 200,
	    numHeads: 256
	  },
	  // DeepSeek Family
	  'deepseek_r1': {
	    totalParamsB: 671,
	    hiddenSize: 7168,
	    numLayers: 61,
	    numHeads: 128,
	    numKVHeads: 128,
	    isMoE: true,
	    numExperts: 256,
	    activeExperts: 8,
	    activeParamsB: 37
	  },
	  'deepseek_r1_distill_70b': {
	    totalParamsB: 70,
	    hiddenSize: 8192,
	    numLayers: 80,
	    numHeads: 64,
	    numKVHeads: 8
	  },
	  'deepseek_r1_distill_32b': {
	    totalParamsB: 32,
	    hiddenSize: 5120,
	    numLayers: 64,
	    numHeads: 40,
	    numKVHeads: 8
	  },
	  'deepseek_r1_distill_14b': {
	    totalParamsB: 14,
	    hiddenSize: 5120,
	    numLayers: 48,
	    numHeads: 40,
	    numKVHeads: 8
	  },
	  'deepseek_r1_distill_8b': {
	    totalParamsB: 8,
	    hiddenSize: 3584,
	    numLayers: 28,
	    numHeads: 28,
	    numKVHeads: 4
	  },
	  'deepseek_r1_distill_1.5b': {
	    totalParamsB: 1.5,
	    hiddenSize: 1536,
	    numLayers: 28,
	    numHeads: 12,
	    numKVHeads: 2
	  },
	  // Qwen 2.5 Family
	  'qwen2.5_72b': {
	    totalParamsB: 72,
	    hiddenSize: 8192,
	    numLayers: 80,
	    numHeads: 64,
	    numKVHeads: 8,
	    intermediateSize: 29568
	  },
	  'qwen2.5_32b': {
	    totalParamsB: 32,
	    hiddenSize: 5120,
	    numLayers: 64,
	    numHeads: 40,
	    numKVHeads: 8
	  },
	  'qwen2.5_14b': {
	    totalParamsB: 14,
	    hiddenSize: 5120,
	    numLayers: 48,
	    numHeads: 40,
	    numKVHeads: 8
	  },
	  'qwen2.5_7b': {
	    totalParamsB: 7,
	    hiddenSize: 3584,
	    numLayers: 28,
	    numHeads: 28,
	    numKVHeads: 4
	  },
	  'qwen2.5_3b': {
	    totalParamsB: 3,
	    hiddenSize: 2048,
	    numLayers: 36,
	    numHeads: 16,
	    numKVHeads: 2
	  },
	  'qwen2.5_coder_32b': {
	    totalParamsB: 32,
	    hiddenSize: 5120,
	    numLayers: 64,
	    numHeads: 40,
	    numKVHeads: 8
	  },
	  // Llama 3.2/3.3 Family
	  'llama3.3_70b': {
	    totalParamsB: 70,
	    hiddenSize: 8192,
	    numLayers: 80,
	    numHeads: 64,
	    numKVHeads: 8
	  },
	  'llama3.2_3b': {
	    totalParamsB: 3,
	    hiddenSize: 3072,
	    numLayers: 28,
	    numHeads: 24,
	    numKVHeads: 8
	  },
	  'llama3.2_1b': {
	    totalParamsB: 1,
	    hiddenSize: 2048,
	    numLayers: 16,
	    numHeads: 32,
	    numKVHeads: 8
	  },
	  'llama3.2_90b_vision': {
	    totalParamsB: 90,
	    hiddenSize: 8192,
	    numLayers: 80,
	    numHeads: 64,
	    numKVHeads: 8,
	    hasVision: true
	  },
	  'llama3.2_11b_vision': {
	    totalParamsB: 11,
	    hiddenSize: 4096,
	    numLayers: 32,
	    numHeads: 32,
	    numKVHeads: 8,
	    hasVision: true
	  },
	  // Mistral Large 2 and others
	  'mistral_large_2_123b': {
	    totalParamsB: 123,
	    hiddenSize: 12288,
	    numLayers: 88,
	    numHeads: 96,
	    numKVHeads: 8
	  },
	  'mistral_nemo_12b': {
	    totalParamsB: 12,
	    hiddenSize: 5120,
	    numLayers: 40,
	    numHeads: 32,
	    numKVHeads: 8
	  },
	  // GLM-4
	  'glm4_9b': {
	    totalParamsB: 9,
	    hiddenSize: 4096,
	    numLayers: 40,
	    numHeads: 32,
	    numKVHeads: 2
	  },
	  // Command R+
	  'command_r_plus_104b': {
	    totalParamsB: 104,
	    hiddenSize: 12288,
	    numLayers: 64,
	    numHeads: 96,
	    numKVHeads: 8
	  },
	  // Image Generation Models
	  'sdxl_base': {
	    totalParamsB: 3.5,
	    hiddenSize: 2048,
	    numLayers: 70,
	    numHeads: 20,
	    isImageGen: true,
	    imageSize: 1024,
	    flopsPerImage: 2.5e12
	  },
	  'flux1_dev': {
	    totalParamsB: 12,
	    hiddenSize: 3072,
	    numLayers: 57,
	    numHeads: 24,
	    isImageGen: true,
	    imageSize: 1024,
	    flopsPerImage: 12e12
	  },
	  'sd3_medium': {
	    totalParamsB: 2,
	    hiddenSize: 1536,
	    numLayers: 24,
	    numHeads: 24,
	    isImageGen: true,
	    imageSize: 1024,
	    flopsPerImage: 4e12
	  },
	  // Gemma 3 Family
	  'gemma3_27b': {
	    totalParamsB: 27,
	    hiddenSize: 4608,
	    numLayers: 46,
	    numHeads: 32,
	    numKVHeads: 16,
	    contextLength: 128000,
	    hasVision: true
	  },
	  'gemma3_12b': {
	    totalParamsB: 12,
	    hiddenSize: 3840,
	    numLayers: 34,
	    numHeads: 16,
	    numKVHeads: 8,
	    contextLength: 128000,
	    hasVision: true
	  },
	  'gemma3_4b': {
	    totalParamsB: 4,
	    hiddenSize: 2560,
	    numLayers: 26,
	    numHeads: 8,
	    numKVHeads: 4,
	    contextLength: 128000,
	    hasVision: true
	  },
	  'gemma3_1b': {
	    totalParamsB: 1,
	    hiddenSize: 1536,
	    numLayers: 18,
	    numHeads: 8,
	    numKVHeads: 4,
	    contextLength: 32000
	  },
	  // Gemma 2 Family
	  'gemma2_27b': {
	    totalParamsB: 27,
	    hiddenSize: 4608,
	    numLayers: 46,
	    numHeads: 32,
	    numKVHeads: 16
	  },
	  'gemma2_9b': {
	    totalParamsB: 9,
	    hiddenSize: 3584,
	    numLayers: 42,
	    numHeads: 16,
	    numKVHeads: 8
	  },
	  'gemma2_2b': {
	    totalParamsB: 2,
	    hiddenSize: 2304,
	    numLayers: 26,
	    numHeads: 8,
	    numKVHeads: 4
	  },
	  // Phi-4 Family
	  'phi4_14b': {
	    totalParamsB: 14,
	    hiddenSize: 5120,
	    numLayers: 40,
	    numHeads: 40,
	    numKVHeads: 10,
	    contextLength: 16000
	  },
	  'phi4_mini_3.8b': {
	    totalParamsB: 3.8,
	    hiddenSize: 3072,
	    numLayers: 32,
	    numHeads: 24,
	    numKVHeads: 8,
	    contextLength: 128000
	  },
	  'phi4_multimodal_5.6b': {
	    totalParamsB: 5.6,
	    hiddenSize: 3072,
	    numLayers: 32,
	    numHeads: 24,
	    numKVHeads: 8,
	    hasVision: true,
	    hasAudio: true,
	    contextLength: 128000
	  },
	  // Phi-3 Family
	  'phi3_medium_14b': {
	    totalParamsB: 14,
	    hiddenSize: 5120,
	    numLayers: 40,
	    numHeads: 40,
	    numKVHeads: 10
	  },
	  'phi3_small_7b': {
	    totalParamsB: 7,
	    hiddenSize: 4096,
	    numLayers: 32,
	    numHeads: 32,
	    numKVHeads: 8
	  },
	  'phi3_mini_3.8b': {
	    totalParamsB: 3.8,
	    hiddenSize: 3072,
	    numLayers: 32,
	    numHeads: 32,
	    numKVHeads: 8,
	    contextLength: 128000
	  },
	  // Mixtral 8x22B (MoE)
	  'mixtral_8x22b': {
	    totalParamsB: 176,
	    hiddenSize: 6144,
	    numLayers: 56,
	    numHeads: 48,
	    numKVHeads: 8,
	    isMoE: true,
	    numExperts: 8,
	    activeExperts: 2,
	    activeParamsB: 44,
	    contextLength: 65000
	  },
	  'mixtral_8x7b': {
	    totalParamsB: 56,
	    hiddenSize: 4096,
	    numLayers: 32,
	    numHeads: 32,
	    numKVHeads: 8,
	    isMoE: true,
	    numExperts: 8,
	    activeExperts: 2,
	    activeParamsB: 14
	  },
	  // DBRX (Databricks MoE)
	  'dbrx_132b': {
	    totalParamsB: 132,
	    hiddenSize: 6144,
	    numLayers: 40,
	    numHeads: 48,
	    numKVHeads: 8,
	    isMoE: true,
	    numExperts: 16,
	    activeExperts: 4,
	    activeParamsB: 36
	  },
	  // Arctic (Snowflake MoE)
	  'arctic_480b': {
	    totalParamsB: 480,
	    hiddenSize: 7168,
	    numLayers: 128,
	    numHeads: 56,
	    numKVHeads: 8,
	    isMoE: true,
	    numExperts: 128,
	    activeExperts: 2,
	    activeParamsB: 17
	  },
	  // Falcon 2 Family
	  'falcon2_180b': {
	    totalParamsB: 180,
	    hiddenSize: 14848,
	    numLayers: 80,
	    numHeads: 232,
	    numKVHeads: 8
	  },
	  'falcon2_11b': {
	    totalParamsB: 11,
	    hiddenSize: 4096,
	    numLayers: 60,
	    numHeads: 32,
	    numKVHeads: 8,
	    hasVision: true
	  },
	  // StarCoder 2
	  'starcoder2_15b': {
	    totalParamsB: 15,
	    hiddenSize: 6144,
	    numLayers: 40,
	    numHeads: 48,
	    numKVHeads: 4,
	    contextLength: 16000
	  },
	  'starcoder2_7b': {
	    totalParamsB: 7,
	    hiddenSize: 4608,
	    numLayers: 32,
	    numHeads: 36,
	    numKVHeads: 4,
	    contextLength: 16000
	  },
	  'starcoder2_3b': {
	    totalParamsB: 3,
	    hiddenSize: 3072,
	    numLayers: 30,
	    numHeads: 24,
	    numKVHeads: 2,
	    contextLength: 16000
	  },
	  // Jamba (AI21 Mamba-Transformer Hybrid)
	  'jamba_1.5_large_398b': {
	    totalParamsB: 398,
	    hiddenSize: 8192,
	    numLayers: 88,
	    numHeads: 64,
	    numKVHeads: 8,
	    isMoE: true,
	    numExperts: 16,
	    activeExperts: 2,
	    activeParamsB: 94,
	    contextLength: 256000,
	    isMambaHybrid: true
	  },
	  'jamba_1.5_mini_52b': {
	    totalParamsB: 52,
	    hiddenSize: 4096,
	    numLayers: 60,
	    numHeads: 32,
	    numKVHeads: 8,
	    isMoE: true,
	    numExperts: 16,
	    activeExperts: 2,
	    activeParamsB: 12,
	    contextLength: 256000,
	    isMambaHybrid: true
	  },
	  // OLMo (AI2 Open Models)
	  'olmo2_13b': {
	    totalParamsB: 13,
	    hiddenSize: 5120,
	    numLayers: 40,
	    numHeads: 40,
	    numKVHeads: 40
	  },
	  'olmo2_7b': {
	    totalParamsB: 7,
	    hiddenSize: 4096,
	    numLayers: 32,
	    numHeads: 32,
	    numKVHeads: 32
	  },
	  // Mistral Small (24B)
	  'mistral_small_24b': {
	    totalParamsB: 24,
	    hiddenSize: 5120,
	    numLayers: 56,
	    numHeads: 32,
	    numKVHeads: 8
	  },
	  // Qwen3 MoE
	  'qwen3_235b_moe': {
	    totalParamsB: 235,
	    hiddenSize: 5120,
	    numLayers: 94,
	    numHeads: 64,
	    numKVHeads: 4,
	    isMoE: true,
	    numExperts: 128,
	    activeExperts: 8,
	    activeParamsB: 22
	  },
	  // Video Generation Models
	  'sora_est': {
	    totalParamsB: 3,
	    hiddenSize: 2048,
	    numLayers: 48,
	    numHeads: 16,
	    isVideoGen: true,
	    flopsPerSecondVideo: 100e12
	  },
	  'runway_gen3': {
	    totalParamsB: 2.5,
	    hiddenSize: 1920,
	    numLayers: 36,
	    numHeads: 16,
	    isVideoGen: true,
	    flopsPerSecondVideo: 80e12
	  },
	  // Audio Models
	  'whisper_large_v3': {
	    totalParamsB: 1.5,
	    hiddenSize: 1280,
	    numLayers: 32,
	    numHeads: 20,
	    isAudioModel: true
	  },
	  'musicgen_large': {
	    totalParamsB: 3.3,
	    hiddenSize: 2048,
	    numLayers: 48,
	    numHeads: 32,
	    isAudioModel: true
	  },
	  // EXO-supported models
	  'kimi_k2': {
	    totalParamsB: 1000,
	    hiddenSize: 8192,
	    numLayers: 80,
	    numHeads: 64,
	    isMoE: true,
	    numExperts: 256,
	    activeExperts: 8,
	    activeParamsB: 32
	  },
	  'llava_1.5_7b': {
	    totalParamsB: 7,
	    hiddenSize: 4096,
	    numLayers: 32,
	    numHeads: 32,
	    hasVision: true
	  },
	  'llava_1.5_13b': {
	    totalParamsB: 13,
	    hiddenSize: 5120,
	    numLayers: 40,
	    numHeads: 40,
	    hasVision: true
	  }
	};
const DEVICE_TEMPLATES = {
  // NVIDIA GPUs (High-End to Low-End)
  'B200': {
    name: 'NVIDIA B200',
    memoryGB: 192,
    localBandwidthGBps: 4800,
    networkBandwidthGBps: 900 / 8,
    computeTFlops: {
      'float32': 950,
      'float16': 1900,
      'bfloat16': 1900,
      'int8': 3800,
      'q4': 5700
    },
    type: 'GPU'
  },
  'H100': {
    name: 'NVIDIA H100',
    memoryGB: 120,
    localBandwidthGBps: 3350,
    networkBandwidthGBps: 450,
    computeTFlops: {
      'float32': 500,
      'float16': 989,
      'bfloat16': 989,
      'int8': 1979,
      'q4': 2500
    },
    type: 'GPU'
  },
  'A100': {
    name: 'NVIDIA A100',
    memoryGB: 80,
    localBandwidthGBps: 1935,
    networkBandwidthGBps: 300,
    computeTFlops: {
      'float32': 156,
      'float16': 312,
      'bfloat16': 312,
      'int8': 624,
      'q4': 1000
    },
    type: 'GPU'
  },
  'RTX 6000': {
    name: 'NVIDIA RTX 6000',
    memoryGB: 48,
    localBandwidthGBps: 2000,
    networkBandwidthGBps: 100 / 8,
    computeTFlops: {
      'float32': 150,
      'float16': 300,
      'bfloat16': 300,
      'int8': 600,
      'q4': 900
    },
    type: 'GPU'
  },
  'RTX 5090': {
    name: 'NVIDIA RTX 5090',
    memoryGB: 32,
    localBandwidthGBps: 1792,    // GDDR7: 28 Gbps x 512-bit
    networkBandwidthGBps: 64,
    pcieGeneration: 5,
    pcieLanes: 16,
    computeTFlops: {
      'float32': 105,
      'float16': 318,
      'bfloat16': 318,
      'int8': 636,
      'q4': 954
    },
    type: 'GPU'
  },
  'RTX 5080': {
    name: 'NVIDIA RTX 5080',
    memoryGB: 16,
    localBandwidthGBps: 960,     // GDDR7: 30 Gbps x 256-bit
    networkBandwidthGBps: 64,
    pcieGeneration: 5,
    pcieLanes: 16,
    computeTFlops: {
      'float32': 57,
      'float16': 171,
      'bfloat16': 171,
      'int8': 342,
      'q4': 513
    },
    type: 'GPU'
  },
  'RTX 5070 Ti': {
    name: 'NVIDIA RTX 5070 Ti',
    memoryGB: 16,
    localBandwidthGBps: 896,     // GDDR7
    networkBandwidthGBps: 64,
    pcieGeneration: 5,
    pcieLanes: 16,
    computeTFlops: {
      'float32': 44,
      'float16': 132,
      'bfloat16': 132,
      'int8': 264,
      'q4': 396
    },
    type: 'GPU'
  },
  'RTX 5070': {
    name: 'NVIDIA RTX 5070',
    memoryGB: 12,
    localBandwidthGBps: 672,     // GDDR7
    networkBandwidthGBps: 64,
    pcieGeneration: 5,
    pcieLanes: 16,
    computeTFlops: {
      'float32': 36,
      'float16': 108,
      'bfloat16': 108,
      'int8': 216,
      'q4': 324
    },
    type: 'GPU'
  },
  'RTX 5060 Ti 16GB': {
    name: 'NVIDIA RTX 5060 Ti 16GB',
    memoryGB: 16,
    localBandwidthGBps: 448,     // GDDR7: 28 Gbps x 128-bit
    networkBandwidthGBps: 32,
    pcieGeneration: 5,
    pcieLanes: 8,
    computeTFlops: {
      'float32': 25,
      'float16': 75,
      'bfloat16': 75,
      'int8': 150,
      'q4': 225
    },
    type: 'GPU'
  },
  'RTX 5060 Ti 8GB': {
    name: 'NVIDIA RTX 5060 Ti 8GB',
    memoryGB: 8,
    localBandwidthGBps: 448,     // GDDR7: 28 Gbps x 128-bit
    networkBandwidthGBps: 32,
    pcieGeneration: 5,
    pcieLanes: 8,
    computeTFlops: {
      'float32': 25,
      'float16': 75,
      'bfloat16': 75,
      'int8': 150,
      'q4': 225
    },
    type: 'GPU'
  },
  'DGX Spark': {
    name: 'DGX Spark',
    memoryGB: 128,
    localBandwidthGBps: 273,
    networkBandwidthGBps: 50/8,
    computeTFlops: {
      'float32': 50,
      'float16': 100,
      'int8': 200,
      'q4': 830
    },
    type: 'GPU'
  },
  'RTX 4090': {
    name: 'NVIDIA RTX 4090',
    memoryGB: 24,
    localBandwidthGBps: 1008,
    networkBandwidthGBps: 32,
    pcieGeneration: 4,
    pcieLanes: 16,
    computeTFlops: {
      'float32': 83,
      'float16': 166,
      'bfloat16': 166,
      'int8': 332,
      'q4': 500
    },
    type: 'GPU'
  },
  'RTX 4080 Super': {
    name: 'NVIDIA RTX 4080 Super',
    memoryGB: 16,
    localBandwidthGBps: 736,     // GDDR6X: 23 Gbps x 256-bit
    networkBandwidthGBps: 32,
    pcieGeneration: 4,
    pcieLanes: 16,
    computeTFlops: {
      'float32': 52,
      'float16': 104,
      'bfloat16': 104,
      'int8': 208,
      'q4': 312
    },
    type: 'GPU'
  },
  'RTX 4080': {
    name: 'NVIDIA RTX 4080',
    memoryGB: 16,
    localBandwidthGBps: 717,     // GDDR6X: 22.4 Gbps x 256-bit
    networkBandwidthGBps: 32,
    pcieGeneration: 4,
    pcieLanes: 16,
    computeTFlops: {
      'float32': 49,
      'float16': 98,
      'bfloat16': 98,
      'int8': 196,
      'q4': 294
    },
    type: 'GPU'
  },
  'RTX 4070 Ti Super': {
    name: 'NVIDIA RTX 4070 Ti Super',
    memoryGB: 16,
    localBandwidthGBps: 672,     // GDDR6X: 21 Gbps x 256-bit
    networkBandwidthGBps: 32,
    pcieGeneration: 4,
    pcieLanes: 16,
    computeTFlops: {
      'float32': 40,
      'float16': 80,
      'bfloat16': 80,
      'int8': 160,
      'q4': 240
    },
    type: 'GPU'
  },
  'Titan RTX + NVMe Gen3': {
    name: 'Titan RTX + NVMe Gen3',
    memoryGB: 4000,
    localBandwidthGBps: 3.5,
    networkBandwidthGBps: 1 / 8,
    computeTFlops: {
      'float32': 16.3,
      'float16': 32.6,
      'int8': 65.2,
      'q4': 98
    },
    type: 'GPU/NVMe'
  },
  'RTX 4070': {
    name: 'RTX 4070',
    memoryGB: 12,
    localBandwidthGBps: 504,
    networkBandwidthGBps: 32,
    computeTFlops: {
      'float32': 29,
      'float16': 58,
      'int8': 116,
      'q4': 175
    },
    type: 'GPU'
  },
  'RTX 4060': {
    name: 'NVIDIA RTX 4060 8GB',
    memoryGB: 8,
    localBandwidthGBps: 272,
    networkBandwidthGBps: 16,
    computeTFlops: {
      'float32': 15,
      'float16': 30,
      'bfloat16': 30,
      'int8': 60,
      'q4': 90
    },
    type: 'GPU'
  },
  'RTX 4060 Mobile': {
    name: 'NVIDIA RTX 4060 Mobile 8GB',
    memoryGB: 8,
    localBandwidthGBps: 168,
    networkBandwidthGBps: 8,
    computeTFlops: {
      'float32': 10,
      'float16': 20,
      'bfloat16': 20,
      'int8': 40,
      'q4': 60
    },
    type: 'Mobile GPU'
  },
  'RTX 3060': {
    name: 'NVIDIA RTX 3060 12GB',
    memoryGB: 12,
    localBandwidthGBps: 360,
    networkBandwidthGBps: 16,
    computeTFlops: {
      'float32': 13,
      'float16': 26,
      'bfloat16': 26,
      'int8': 52,
      'q4': 78
    },
    type: 'GPU'
  },
  'RTX 3050': {
    name: 'NVIDIA RTX 3050 8GB',
    memoryGB: 8,
    localBandwidthGBps: 224,
    networkBandwidthGBps: 16,
    computeTFlops: {
      'float32': 9,
      'float16': 18,
      'bfloat16': 18,
      'int8': 36,
      'q4': 54
    },
    type: 'GPU'
  },
  'RTX 2060': {
    name: 'NVIDIA RTX 2060 6GB',
    memoryGB: 6,
    localBandwidthGBps: 336,
    networkBandwidthGBps: 16,
    computeTFlops: {
      'float32': 6.5,
      'float16': 13,
      'bfloat16': 6.5,
      'int8': 26,
      'q4': 39
    },
    type: 'GPU'
  },
  'GTX 1650': {
    name: 'NVIDIA GTX 1650 4GB',
    memoryGB: 4,
    localBandwidthGBps: 128,
    networkBandwidthGBps: 16,
    computeTFlops: {
      'float32': 3,
      'float16': 6,
      'bfloat16': 3,
      'int8': 12,
      'q4': 18
    },
    type: 'GPU'
  },
  'GTX 1060': {
    name: 'NVIDIA GTX 1060 6GB',
    memoryGB: 6,
    localBandwidthGBps: 192,
    networkBandwidthGBps: 16,
    computeTFlops: {
      'float32': 4.4,
      'float16': 8.8,
      'bfloat16': 4.4,
      'int8': 17.6,
      'q4': 26
    },
    type: 'GPU'
  },
  'GT 730': {
    name: 'NVIDIA GT 730 2GB',
    memoryGB: 2,
    localBandwidthGBps: 40,
    networkBandwidthGBps: 5,
    computeTFlops: {
      'float32': 0.3,
      'float16': 0.6,
      'bfloat16': 0.3,
      'int8': 1.2,
      'q4': 1.8
    },
    type: 'GPU'
  },
  
  // AMD GPUs and CPUs
  'AMD MI300X': {
    name: 'AMD MI300X',
    memoryGB: 192,
    localBandwidthGBps: 5200,
    networkBandwidthGBps: 400 / 8,
    pcieGeneration: 5,
    pcieLanes: 16,
    computeTFlops: {
      'float32': 380,
      'float16': 760,
      'bfloat16': 760,
      'int8': 1520,
      'q4': 2280
    },
    type: 'GPU'
  },
  'RX 7900 XTX': {
    name: 'AMD RX 7900 XTX',
    memoryGB: 24,
    localBandwidthGBps: 960,     // GDDR6: 20 Gbps x 384-bit
    networkBandwidthGBps: 32,
    pcieGeneration: 4,
    pcieLanes: 16,
    computeTFlops: {
      'float32': 61,
      'float16': 122,
      'bfloat16': 61,
      'int8': 244,
      'q4': 366
    },
    type: 'GPU'
  },
  'RX 7900 XT': {
    name: 'AMD RX 7900 XT',
    memoryGB: 20,
    localBandwidthGBps: 800,     // GDDR6: 20 Gbps x 320-bit
    networkBandwidthGBps: 32,
    pcieGeneration: 4,
    pcieLanes: 16,
    computeTFlops: {
      'float32': 52,
      'float16': 104,
      'bfloat16': 52,
      'int8': 208,
      'q4': 312
    },
    type: 'GPU'
  },
  'RX 9070 XT': {
    name: 'AMD RX 9070 XT',
    memoryGB: 16,
    localBandwidthGBps: 650,     // Estimated GDDR6
    networkBandwidthGBps: 64,
    pcieGeneration: 5,
    pcieLanes: 16,
    computeTFlops: {
      'float32': 32,
      'float16': 64,
      'bfloat16': 32,
      'int8': 128,
      'q4': 192
    },
    type: 'GPU'
  },
  'Threadripper Pro 7995WX': {
    name: 'AMD Threadripper Pro 7995WX',
    memoryGB: 1024,
    localBandwidthGBps: 300,
    networkBandwidthGBps: 100 / 8,
    computeTFlops: {
      'float32': 8,
      'float16': 16,
      'bfloat16': 16,
      'int8': 32,
      'q4': 48
    },
    type: 'CPU'
  },
  'AMD EPYC CPU (High-End)': {
    name: 'AMD EPYC CPU (High-End)',
    memoryGB: 256,
    localBandwidthGBps: 90,
    networkBandwidthGBps: 25/8,
    computeTFlops: {
      'float32': 2.5,
      'bfloat16': 5,
      'float16': 5,
      'int8': 10,
      'q4': 15
    },
    type: 'CPU'
  },
  'RX 7600': {
    name: 'AMD RX 7600 8GB',
    memoryGB: 8,
    localBandwidthGBps: 288,
    networkBandwidthGBps: 16,
    computeTFlops: {
      'float32': 11,
      'float16': 22,
      'bfloat16': 11,
      'int8': 44,
      'q4': 66
    },
    type: 'GPU'
  },
  'RX 6600': {
    name: 'AMD RX 6600 8GB',
    memoryGB: 8,
    localBandwidthGBps: 224,
    networkBandwidthGBps: 16,
    computeTFlops: {
      'float32': 9.8,
      'float16': 19.6,
      'bfloat16': 9.8,
      'int8': 39.2,
      'q4': 59
    },
    type: 'GPU'
  },
  'RX 580': {
    name: 'AMD RX 580 8GB',
    memoryGB: 8,
    localBandwidthGBps: 256,
    networkBandwidthGBps: 16,
    computeTFlops: {
      'float32': 6.2,
      'float16': 12.4,
      'bfloat16': 6.2,
      'int8': 24.8,
      'q4': 37
    },
    type: 'GPU'
  },
  'AMD Ryzen Integrated Graphics': {
    name: 'AMD Ryzen Integrated Graphics',
    memoryGB: 16,
    localBandwidthGBps: 50,
    networkBandwidthGBps: 2.5 / 8.0,
    computeTFlops: {
      'float32': 0.5,
      'bfloat16': 1.0,
      'float16': 1.0,
      'int8': 2.0,
      'q4': 3.0
    },
    type: 'Integrated GPU'
  },
  'AMD Strix Halo (Ryzen AI Max+ 395)': {
    name: 'AMD Strix Halo (Ryzen AI Max+ 395)',
    memoryGB: 128,
    localBandwidthGBps: 256,            // 256-bit LPDDR5X ~256 GB/s
    networkBandwidthGBps: 10 / 8,
    pcieGeneration: 5,
    pcieLanes: 16,
    computeTFlops: {
      'float32': 14,                     // RDNA 3.5 40 CUs
      'float16': 28,
      'bfloat16': 28,
      'int8': 56,
      'q4': 84
    },
    npuTops: 50,                         // XDNA 2 NPU
    type: 'APU'
  },
  'AMD Strix Halo (Ryzen AI Max+ 388)': {
    name: 'AMD Strix Halo (Ryzen AI Max+ 388)',
    memoryGB: 96,
    localBandwidthGBps: 256,
    networkBandwidthGBps: 10 / 8,
    pcieGeneration: 5,
    pcieLanes: 16,
    computeTFlops: {
      'float32': 14,
      'float16': 28,
      'bfloat16': 28,
      'int8': 56,
      'q4': 84
    },
    npuTops: 50,
    type: 'APU'
  },
  'AMD Strix Point (Ryzen AI 9 HX 370)': {
    name: 'AMD Strix Point (Ryzen AI 9 HX 370)',
    memoryGB: 64,
    localBandwidthGBps: 120,            // LPDDR5X-7500
    networkBandwidthGBps: 2.5 / 8,
    computeTFlops: {
      'float32': 5,                      // RDNA 3.5 16 CUs
      'float16': 10,
      'bfloat16': 10,
      'int8': 20,
      'q4': 30
    },
    npuTops: 50,
    type: 'APU'
  },

  // Intel
  'Intel Arc B580': {
    name: 'Intel Arc B580 12GB',
    memoryGB: 12,
    localBandwidthGBps: 456,            // GDDR6 19 Gbps x 192-bit
    networkBandwidthGBps: 32,
    pcieGeneration: 4,
    pcieLanes: 8,
    computeTFlops: {
      'float32': 11,
      'float16': 22,
      'bfloat16': 22,
      'int8': 44,
      'q4': 66
    },
    type: 'GPU'
  },
  'Intel Arc A770': {
    name: 'Intel Arc A770 16GB',
    memoryGB: 16,
    localBandwidthGBps: 560,            // GDDR6 17.5 Gbps x 256-bit
    networkBandwidthGBps: 32,
    pcieGeneration: 4,
    pcieLanes: 16,
    computeTFlops: {
      'float32': 20,
      'float16': 40,
      'bfloat16': 40,
      'int8': 80,
      'q4': 120
    },
    type: 'GPU'
  },
  'Intel Arc A750': {
    name: 'Intel Arc A750 8GB',
    memoryGB: 8,
    localBandwidthGBps: 512,
    networkBandwidthGBps: 32,
    pcieGeneration: 4,
    pcieLanes: 16,
    computeTFlops: {
      'float32': 17,
      'float16': 34,
      'bfloat16': 34,
      'int8': 68,
      'q4': 102
    },
    type: 'GPU'
  },

  // NVIDIA DGX / AI PCs
  'NVIDIA DGX Spark (GB10)': {
    name: 'NVIDIA DGX Spark (GB10)',
    memoryGB: 128,
    localBandwidthGBps: 273,            // LPDDR5X-9400 unified memory
    networkBandwidthGBps: 10 / 8,
    computeTFlops: {
      'float32': 125,                    // Grace Blackwell Superchip
      'float16': 250,
      'bfloat16': 250,
      'int8': 500,
      'q4': 1000                         // 1 PFLOP at FP4
    },
    type: 'AI PC'
  },
  'NVIDIA DGX Station (GB300)': {
    name: 'NVIDIA DGX Station (GB300)',
    memoryGB: 768,
    localBandwidthGBps: 16000,          // HBM3e across 4 GPUs
    networkBandwidthGBps: 400 / 8,
    computeTFlops: {
      'float32': 800,
      'float16': 1600,
      'bfloat16': 1600,
      'int8': 3200,
      'q4': 4800
    },
    type: 'AI Workstation'
  },

  // Cloud / Inference Accelerators
  'Groq LPU Rack (Llama 70B)': {
    name: 'Groq LPU Rack (576 chips)',
    memoryGB: 576,                       // ~1GB per chip
    localBandwidthGBps: 46080,           // 80 TB/s per chip x 576
    networkBandwidthGBps: 100,
    computeTFlops: {
      'float32': 450,
      'float16': 900,
      'bfloat16': 900,
      'int8': 1800,
      'q4': 2700
    },
    type: 'LPU Rack'
  },
  'Cerebras CS-3 (WSE-3)': {
    name: 'Cerebras CS-3 (WSE-3)',
    memoryGB: 44,                        // 44GB on-chip SRAM
    localBandwidthGBps: 21000,           // 21 PB/s on-chip
    networkBandwidthGBps: 2400 / 8,      // 12x 200G links
    computeTFlops: {
      'float32': 125,
      'float16': 250,
      'bfloat16': 250,
      'int8': 500,
      'q4': 750
    },
    type: 'Wafer Scale'
  },
  'AWS Inferentia2 (inf2.xlarge)': {
    name: 'AWS Inferentia2 (inf2.xlarge)',
    memoryGB: 32,
    localBandwidthGBps: 820,
    networkBandwidthGBps: 25 / 8,
    computeTFlops: {
      'float32': 47,
      'float16': 190,
      'bfloat16': 190,
      'int8': 380,
      'q4': 570
    },
    type: 'Cloud Accelerator'
  },
  'AWS Trainium2 (trn2.xlarge)': {
    name: 'AWS Trainium2 (trn2.xlarge)',
    memoryGB: 64,
    localBandwidthGBps: 1600,
    networkBandwidthGBps: 100 / 8,
    computeTFlops: {
      'float32': 95,
      'float16': 380,
      'bfloat16': 380,
      'int8': 760,
      'q4': 1140
    },
    type: 'Cloud Accelerator'
  },
  'Google TPU v5e': {
    name: 'Google TPU v5e',
    memoryGB: 16,
    localBandwidthGBps: 820,
    networkBandwidthGBps: 50 / 8,
    computeTFlops: {
      'float32': 98,
      'float16': 197,
      'bfloat16': 197,
      'int8': 394,
      'q4': 591
    },
    type: 'TPU'
  },
  'Google TPU v5p': {
    name: 'Google TPU v5p',
    memoryGB: 95,
    localBandwidthGBps: 2765,
    networkBandwidthGBps: 100 / 8,
    computeTFlops: {
      'float32': 230,
      'float16': 459,
      'bfloat16': 459,
      'int8': 918,
      'q4': 1377
    },
    type: 'TPU'
  },

  // Apple
  'Mac M3 Ultra (512)': {
    name: 'Mac M3 Ultra (512GB)',
    memoryGB: 512,
    localBandwidthGBps: 819,
    networkBandwidthGBps: 40/8,
    computeTFlops: {
      'float32': 35,
      'bfloat16': 70,
      'float16': 70,
      'int8': 140,
      'q4': 210
    },
    type: 'CPU/Integrated GPU'
  },
  'Mac M2 Ultra (192GB)': {
    name: 'Mac M2 Ultra (192GB)',
    memoryGB: 192,
    localBandwidthGBps: 800,
    networkBandwidthGBps: 40/8,
    computeTFlops: {
      'float32': 16,
      'bfloat16': 32,
      'float16': 32,
      'int8': 64,
      'q4': 96
    },
    type: 'CPU/Integrated GPU'
  },
  'Mac M4 Max (128)': {
    name: 'Mac M4 Max (128)',
    memoryGB: 128,
    localBandwidthGBps: 546,
    networkBandwidthGBps: 40/8,
    computeTFlops: {
      'float32': 18,
      'bfloat16': 35,
      'float16': 35,
      'int8': 70,
      'q4': 105
    },
    type: 'CPU/Integrated GPU'
  },
  'Mac Mini M2 (10G Ethernet)': {
    name: 'Mac Mini M2 (10G)',
    memoryGB: 16,
    localBandwidthGBps: 68,
    networkBandwidthGBps: 10/8,
    computeTFlops: {
      'float32': 2,
      'bfloat16': 4,
      'float16': 4,
      'int8': 8,
      'q4': 12
    },
    type: 'CPU/Integrated GPU'
  },
  
  // Intel
  'Intel Gaudi3': {
    name: 'Intel Gaudi3',
    memoryGB: 128,
    localBandwidthGBps: 3600,
    networkBandwidthGBps: 400 / 8,
    computeTFlops: {
      'float32': 320,
      'float16': 640,
      'bfloat16': 640,
      'int8': 1280,
      'q4': 1920
    },
    type: 'AI Accelerator'
  },
  'Intel Xeon CPU (High-End)': {
    name: 'Intel Xeon CPU (High-End)',
    memoryGB: 256,
    localBandwidthGBps: 80,
    networkBandwidthGBps: 25/8,
    computeTFlops: {
      'float32': 2,
      'bfloat16': 4,
      'float16': 4,
      'int8': 8,
      'q4': 12
    },
    type: 'CPU'
  },
  'Arc A770': {
    name: 'Intel Arc A770 16GB',
    memoryGB: 16,
    localBandwidthGBps: 560,
    networkBandwidthGBps: 16,
    computeTFlops: {
      'float32': 8.5,
      'float16': 17,
      'bfloat16': 17,
      'int8': 34,
      'q4': 51
    },
    type: 'GPU'
  },
  'Arc A380': {
    name: 'Intel Arc A380 6GB',
    memoryGB: 6,
    localBandwidthGBps: 192,
    networkBandwidthGBps: 8,
    computeTFlops: {
      'float32': 3.2,
      'float16': 6.4,
      'bfloat16': 6.4,
      'int8': 12.8,
      'q4': 19.2
    },
    type: 'GPU'
  },
  
  // Google
  'Google TPU v5p': {
    name: 'Google TPU v5p',
    memoryGB: 256,
    localBandwidthGBps: 3200,
    networkBandwidthGBps: 400 / 8,
    computeTFlops: {
      'float32': 450,
      'float16': 900,
      'bfloat16': 900,
      'int8': 1800,
      'q4': 2700
    },
    type: 'AI Accelerator'
  },
  
  // Cerebras
  'Cerebras WSE-3': {
    name: 'Cerebras WSE-3',
    memoryGB: 40000,
    localBandwidthGBps: 20000,
    networkBandwidthGBps: 800 / 8,
    computeTFlops: {
      'float32': 2400,
      'float16': 4800,
      'bfloat16': 4800,
      'int8': 9600,
      'q4': 14400
    },
    type: 'Wafer-Scale AI'
  },
  
  // AWS
  'AWS Inferentia2': {
    name: 'AWS Inferentia2',
    memoryGB: 32,
    localBandwidthGBps: 1000,
    networkBandwidthGBps: 100 / 8,
    computeTFlops: {
      'float32': 180,
      'float16': 360,
      'bfloat16': 360, 
      'int8': 720,
      'q4': 1080
    },
    type: 'AI Accelerator'
  },
  
  // Groq
  'Groq LPU-1': {
    name: 'Groq LPU-1',
    memoryGB: 80,
    localBandwidthGBps: 2500,
    networkBandwidthGBps: 200 / 8,
    computeTFlops: {
      'float32': 300,
      'float16': 600,
      'bfloat16': 600,
      'int8': 1200,
      'q4': 1800
    },
    type: 'AI Accelerator'
  },
  
  // NVMe/Storage Solutions
  'NVMe 4xRAID 5090 (Gen5)': {
    name: 'NVMe 4xRAID GPU (Gen5)',
    memoryGB: 8000,
    localBandwidthGBps: 32,
    networkBandwidthGBps: 40 / 8,
    computeTFlops: {
      'float32': 105,
      'float16': 210,
      'int8': 420,
      'q4': 630
    },
    type: 'GPU/NVMe'
  },
  'NVMe CPU (Gen5)': {
    name: 'NVMe CPU (Gen5)',
    memoryGB: 2000,
    localBandwidthGBps: 14,
    networkBandwidthGBps: 40 / 8,
    computeTFlops: {
      'float32': 2,
      'float16': 4,
      'int8': 8,
      'q4': 12
    },
    type: 'CPU/NVMe'
  },
  
  // Other devices
  'Rockchip 3588': {
    name: 'RK3588',
    memoryGB: 32,
    localBandwidthGBps: 38.4,
    networkBandwidthGBps: 2.5/8,
    computeTFlops: {
      'float16': 0.9,
      'int8': 1.8,
      'q4': 6
    },
    type: 'SBC'
  },
  'Raspberry Pi 5 (5G Ethernet)': {
    name: 'Raspberry Pi 5 (5G)',
    memoryGB: 8,
    localBandwidthGBps: 34,
    networkBandwidthGBps: 5 / 8.0,
    computeTFlops: {
      'float32': 0.1,
      'bfloat16': 0.2,
      'float16': 0.2,
      'int8': 0.4,
      'q4': 0.6
    },
    type: 'CPU/Integrated GPU'
  },
  'Desktop PC (2.5G Ethernet)': {
    name: 'Desktop PC (2.5G)',
    memoryGB: 32,
    localBandwidthGBps: 50,
    networkBandwidthGBps: 2.5 / 8.0,
    computeTFlops: {
      'float32': 1,
      'float16': 2,
      'int8': 4,
      'q4': 6
    },
    type: 'CPU/Integrated GPU'
  },
  
  // Custom
  // System RAM Overflow Targets
  'DDR5-7200 System RAM': {
    name: 'DDR5-7200 System RAM',
    memoryGB: 128,
    localBandwidthGBps: 115,     // Dual-channel DDR5-7200
    networkBandwidthGBps: 0,
    pcieGeneration: 0,
    pcieLanes: 0,
    computeTFlops: {
      'float32': 0,
      'float16': 0,
      'bfloat16': 0,
      'int8': 0,
      'q4': 0
    },
    type: 'System RAM'
  },
  'DDR5-6400 System RAM': {
    name: 'DDR5-6400 System RAM',
    memoryGB: 128,
    localBandwidthGBps: 102,     // Dual-channel DDR5-6400
    networkBandwidthGBps: 0,
    pcieGeneration: 0,
    pcieLanes: 0,
    computeTFlops: {
      'float32': 0,
      'float16': 0,
      'bfloat16': 0,
      'int8': 0,
      'q4': 0
    },
    type: 'System RAM'
  },
  'DDR5-5600 System RAM': {
    name: 'DDR5-5600 System RAM',
    memoryGB: 64,
    localBandwidthGBps: 90,      // Dual-channel DDR5-5600
    networkBandwidthGBps: 0,
    pcieGeneration: 0,
    pcieLanes: 0,
    computeTFlops: {
      'float32': 0,
      'float16': 0,
      'bfloat16': 0,
      'int8': 0,
      'q4': 0
    },
    type: 'System RAM'
  },
  'Custom': {
    name: 'Custom Device',
    memoryGB: 192,
    localBandwidthGBps: 50,
    networkBandwidthGBps: 2.5 / 8,
    pcieGeneration: 4,
    pcieLanes: 16,
    computeTFlops: {
      'float32': 2,
      'float16': 4,
      'bfloat16': 4,
      'int8': 8,
      'q4': 12
    },
    type: 'Custom'
  }
};
        let devices = [{
            id: 1,
            name: 'NVIDIA RTX 5090',
            template: 'RTX 5090',
            ...DEVICE_TEMPLATES["RTX 5090"]
        }];
	    
        let utilizationChart = null;
        function getModelConfig() {
            return {
                modelPreset: document.getElementById('modelPreset').value,
                quantizationType: document.getElementById('quantizationType').value,
                totalParamsB: parseFloat(document.getElementById('totalParamsB').value),
                batchSize: parseFloat(document.getElementById('batchSize').value),
                seqLength: parseFloat(document.getElementById('seqLength').value),
                hiddenSize: parseFloat(document.getElementById('hiddenSize').value),
                numLayers: parseFloat(document.getElementById('numLayers').value),
                numHeads: parseFloat(document.getElementById('numHeads').value),
                dtype: document.getElementById('dtype').value,
                parallelismStrategy: document.getElementById('parallelismStrategy').value
            };
        }
	function calculateMemoryBreakdown(modelConfig, dtypeSize, deviceCount, isPipeline, deviceIndex ) {
		const paramsMemory = modelConfig.totalParamsB * 1e9 * dtypeSize;
		const kvCacheSizePerLayer = 2 * modelConfig.batchSize * modelConfig.seqLength * 
			(modelConfig.hiddenSize / modelConfig.numHeads) * 
			modelConfig.numHeads * dtypeSize;
		const kvCacheMemory = kvCacheSizePerLayer * modelConfig.numLayers;
		const activationSizePerLayer = modelConfig.batchSize * modelConfig.seqLength * 
			modelConfig.hiddenSize * dtypeSize;
		const activationMemory = activationSizePerLayer * modelConfig.numLayers;
		const attentionMemory = modelConfig.batchSize * (modelConfig.seqLength ** 2) * 
			modelConfig.numHeads * dtypeSize;

		// Calculate total available memory across all devices
		const totalSystemMemory = devices.reduce((sum, device) => sum + device.memoryGB * 1e9, 0);
		
		// Get this device's memory capacity
		const deviceMemory = devices[deviceIndex].memoryGB * 1e9;
		
		// Calculate proportion of total memory for this device
		const memoryProportion = deviceMemory / totalSystemMemory;

		// Adjust memory allocation based on device's proportion of total memory
		const adjustedParamsMemory = isPipeline ? 
			paramsMemory * memoryProportion : // Pipeline: divide based on memory proportion
			paramsMemory; // Tensor: replicate most params
			
		const adjustedKVCacheMemory = kvCacheMemory * memoryProportion;
		const adjustedActivationMemory = activationMemory * memoryProportion;
		const adjustedAttentionMemory = attentionMemory * memoryProportion;

		return {
			paramsMemory: adjustedParamsMemory,
			kvCacheMemory: adjustedKVCacheMemory,
			activationMemory: adjustedActivationMemory,
			attentionMemory: adjustedAttentionMemory,
			total: adjustedParamsMemory + adjustedKVCacheMemory + 
				   adjustedActivationMemory + adjustedAttentionMemory
		};
	}
	
	function calculateNetworkTraffic(modelConfig, dtypeSize, deviceCount, isPipeline) {
		if (deviceCount <= 1) return 0;

		const batchSize = modelConfig.batchSize;
		const seqLength = modelConfig.seqLength;
		const hiddenSize = modelConfig.hiddenSize;
		
		if (isPipeline) {
			// Pipeline parallelism: Forward and backward activation passing
			const activationSize = batchSize * seqLength * hiddenSize * dtypeSize;
			// Each layer boundary requires passing activations forward and gradients back
			return activationSize * 2 * (deviceCount - 1);
		} else {
			// Tensor parallelism: All-reduce for gradients and activations
			const gradientSize = modelConfig.totalParamsB * 1e9 * dtypeSize / deviceCount;
			const activationSize = batchSize * seqLength * hiddenSize * dtypeSize;
			// All-reduce communication pattern: (n-1)/n * data_size * 2
			return (deviceCount - 1) * (gradientSize + activationSize) * 2 / deviceCount;
		}
	}

        // Improved FLOPs calculation (more accurate for forward and backward pass)
        function calculateTransformerFlops(modelConfig) {
            const { batchSize, seqLength, hiddenSize, numLayers, numHeads } = modelConfig;

            // Forward pass FLOPs for one layer
            let forwardFlops = 0;

            // Self-Attention
            forwardFlops += 3 * (batchSize * seqLength * hiddenSize * hiddenSize); // Q, K, V projections
            forwardFlops += batchSize * numHeads * seqLength * seqLength * (hiddenSize / numHeads); // Attention scores
            forwardFlops += batchSize * numHeads * seqLength * (hiddenSize / numHeads) * (hiddenSize / numHeads); // Weighted sum
            forwardFlops += batchSize * seqLength * hiddenSize * hiddenSize; // Output projection

            // Feed-Forward Network
            forwardFlops += batchSize * seqLength * hiddenSize * (4 * hiddenSize); // First layer
            forwardFlops += batchSize * seqLength * (4 * hiddenSize) * hiddenSize; // Second layer

            // Inference only needs forward pass (not backward)
            return forwardFlops * numLayers;
        }

        function calculateDecodeFlops(modelConfig) {
            const { batchSize, seqLength, hiddenSize, numLayers, numHeads } = modelConfig;

            // Decode FLOPs are dominated by KV cache interactions and are roughly:
            let decodeFlops = 0;

            // KV Cache interaction in self-attention (per layer)
            decodeFlops += 2 * batchSize * seqLength * hiddenSize * hiddenSize; 

            // Feed-Forward Network (remains similar to prefill per token)
            decodeFlops += batchSize * hiddenSize * (4 * hiddenSize); // First layer
            decodeFlops += batchSize * (4 * hiddenSize) * hiddenSize; // Second layer

            return decodeFlops * numLayers;
        }

        /**
         * Calculate effective memory bandwidth considering PCIe overflow
         *
         * Key insight for LLM inference at batch=1:
         * - Performance is dominated by memory bandwidth
         * - tokens/sec ≈ memory_bandwidth / model_size_bytes
         *
         * When model overflows to secondary storage:
         * time_per_token = (local_portion / local_bw) + (overflow_portion / overflow_bw)
         */
        function calculateEffectiveBandwidth(device, modelSizeBytes, allDevices, modelConfig) {
            const deviceMemoryBytes = device.memoryGB * 1e9;

            // For MoE models, use active parameters instead of total
            let effectiveModelSize = modelSizeBytes;
            if (modelConfig && modelConfig.activeParamsB) {
                effectiveModelSize = modelConfig.activeParamsB * 1e9 * (modelSizeBytes / (modelConfig.totalParamsB * 1e9));
            }

            // Case 1: Model fits entirely in device memory
            if (effectiveModelSize <= deviceMemoryBytes) {
                return {
                    effectiveBandwidthGBps: device.localBandwidthGBps,
                    localPortion: effectiveModelSize,
                    overflowPortion: 0,
                    overflowBandwidthGBps: 0,
                    bottleneckReason: 'none',
                    hasOverflow: false
                };
            }

            // Case 2: Model overflows - need to determine overflow bandwidth
            const localPortion = deviceMemoryBytes;
            const overflowPortion = effectiveModelSize - deviceMemoryBytes;

            let overflowBandwidthGBps;
            let bottleneckReason;

            // Check if there's an overflow target specified
            if (device.overflowTarget === 'ddr5') {
                // Overflow to system DDR5 RAM
                overflowBandwidthGBps = device.ddr5BandwidthGBps || OVERFLOW_BANDWIDTH['ddr5_7200'];
                bottleneckReason = 'DDR5 RAM overflow';
            } else if (device.overflowTarget && typeof device.overflowTarget === 'number') {
                // Overflow to another device via network/interconnect
                const targetDevice = allDevices.find(d => d.id === device.overflowTarget);
                if (targetDevice) {
                    // Use the slower of the two network bandwidths
                    const sourceBw = device.networkBandwidthGBps || 32;
                    const targetBw = targetDevice.networkBandwidthGBps || 32;
                    overflowBandwidthGBps = Math.min(sourceBw, targetBw);
                    // Show interconnect type if available
                    const interconnectLabel = device.interconnectType
                        ? device.interconnectType.replace(/_/g, ' ')
                        : `${overflowBandwidthGBps} GB/s`;
                    bottleneckReason = `${interconnectLabel} to ${targetDevice.name}`;
                } else {
                    // Fallback: use device's own network bandwidth
                    overflowBandwidthGBps = device.networkBandwidthGBps || 32;
                    bottleneckReason = 'Network overflow';
                }
            } else {
                // Default: assume overflow to system RAM
                // This represents the typical case where model doesn't fit in VRAM
                overflowBandwidthGBps = OVERFLOW_BANDWIDTH['ddr5_7200'];
                bottleneckReason = 'System RAM overflow (auto)';
            }

            // Calculate weighted effective bandwidth
            // Time = (local_portion / local_bw) + (overflow_portion / overflow_bw)
            // Effective_BW = total_size / time
            const localTimeMs = (localPortion / (device.localBandwidthGBps * 1e9)) * 1000;
            const overflowTimeMs = (overflowPortion / (overflowBandwidthGBps * 1e9)) * 1000;
            const totalTimeMs = localTimeMs + overflowTimeMs;
            const effectiveBandwidthGBps = (effectiveModelSize / (totalTimeMs / 1000)) / 1e9;

            return {
                effectiveBandwidthGBps,
                localPortion,
                overflowPortion,
                overflowBandwidthGBps,
                bottleneckReason,
                localTimeMs,
                overflowTimeMs,
                totalTimeMs,
                hasOverflow: true,
                overflowGB: overflowPortion / 1e9
            };
        }

        /**
         * Calculate theoretical decode token rate for LLM inference
         *
         * Key insight: LLM inference at batch=1 is memory-bandwidth-bound
         * tokens/sec = effective_bandwidth / bytes_per_token
         *
         * For autoregressive decoding, each token requires reading the model weights
         * bytes_per_token ≈ model_size_bytes (must read all weights per token)
         */
        function calculateDecodeTokenRate(device, modelConfig, dtypeSize, allDevices) {
            const modelSizeBytes = modelConfig.totalParamsB * 1e9 * dtypeSize;

            const bandwidthInfo = calculateEffectiveBandwidth(device, modelSizeBytes, allDevices, modelConfig);

            // For decode (batch=1): each token requires reading entire model
            // tokens/sec = bandwidth / model_size
            const theoreticalTokensPerSec = (bandwidthInfo.effectiveBandwidthGBps * 1e9) / modelSizeBytes;

            // Apply efficiency factor for real-world conditions
            // This accounts for: kernel launch overhead, memory access patterns,
            // KV cache operations, attention computation, etc.
            // 0.80-0.85 efficiency is typical for well-optimized LLM inference (llama.cpp, vLLM)
            const efficiencyFactor = 0.82;
            const actualTokensPerSec = theoreticalTokensPerSec * efficiencyFactor;

            return {
                tokensPerSecond: actualTokensPerSec,
                theoreticalMax: theoreticalTokensPerSec,
                modelSizeGB: modelSizeBytes / 1e9,
                ...bandwidthInfo
            };
        }

        /**
         * Calculate image generation metrics (compute-bound, not memory-bound)
         */
        function calculateImageGenMetrics(device, modelConfig, dtypeSize) {
            // Image generation is compute-bound, not memory-bound
            // Time = compute_flops / device_tflops

            const modelFlops = modelConfig.flopsPerImage || 2.5e12; // Default to SDXL
            const deviceTflops = device.computeTFlops[dtypeSize] || device.computeTFlops['float16'] || 10;

            const timePerImage = modelFlops / (deviceTflops * 1e12);
            const imagesPerSecond = 1 / timePerImage;

            return {
                imagesPerSecond,
                timePerImage,
                bottleneck: 'compute',
                computeUtilization: 100 // Image gen is compute-bound
            };
        }

        /**
         * Get PCIe bandwidth for a given generation and lane count
         */
        function getPcieBandwidth(gen, lanes) {
            const pcieBw = PCIE_BANDWIDTH[`pcie${gen || 4}`];
            return pcieBw ? pcieBw[`x${lanes || 16}`] : 32;
        }

        /**
         * Apply an interconnect preset to set network bandwidth
         */
        function applyInterconnectPreset(deviceId, presetKey) {
            if (!presetKey || !INTERCONNECT_BANDWIDTH[presetKey]) return;

            const bandwidth = INTERCONNECT_BANDWIDTH[presetKey];
            updateDevice(deviceId, 'networkBandwidthGBps', bandwidth);
            updateDevice(deviceId, 'interconnectType', presetKey);
        }

        /**
         * Find the optimal parallelism strategy for current configuration
         * Returns: { strategy, reasoning, results: [{strategy, rate, valid, reason}] }
         */
        function findOptimalStrategy() {
            const baseConfig = getModelConfig();
            const modelPreset = MODEL_PRESETS[baseConfig.modelPreset] || {};
            const deviceCount = devices.length;
            const seqLength = baseConfig.seqLength || 2048;
            const isMoE = modelPreset.isMoE || false;

            // Define all strategies and their validity conditions
            const strategies = [
                {
                    value: 'pipeline',
                    name: 'Pipeline Parallelism (PP)',
                    valid: true,
                    reason: 'Valid for any configuration'
                },
                {
                    value: 'tensor',
                    name: 'Tensor Parallelism (TP)',
                    valid: deviceCount >= 2,
                    reason: deviceCount >= 2 ? 'Requires 2+ devices' : 'Need 2+ devices for tensor parallelism'
                },
                {
                    value: 'data',
                    name: 'Data Parallelism (DP)',
                    valid: deviceCount >= 2,
                    reason: deviceCount >= 2 ? 'Valid for multi-device throughput' : 'Need 2+ devices for data parallelism'
                },
                {
                    value: 'expert',
                    name: 'Expert Parallelism (EP)',
                    valid: isMoE && deviceCount >= 2,
                    reason: isMoE ? (deviceCount >= 2 ? 'Valid for MoE models' : 'Need 2+ devices') : 'Only valid for MoE models'
                },
                {
                    value: 'sequence',
                    name: 'Sequence Parallelism (SP)',
                    valid: deviceCount >= 2 && seqLength > 4096,
                    reason: (deviceCount >= 2 && seqLength > 4096) ? 'Valid for long sequences' : 'Need 2+ devices and seq_len > 4096'
                },
                {
                    value: 'context',
                    name: 'Context Parallelism (CP)',
                    valid: deviceCount >= 2 && seqLength > 8192,
                    reason: (deviceCount >= 2 && seqLength > 8192) ? 'Valid for very long contexts' : 'Need 2+ devices and seq_len > 8192'
                },
                {
                    value: 'hybrid_tp_pp',
                    name: 'Hybrid TP+PP',
                    valid: deviceCount >= 4,
                    reason: deviceCount >= 4 ? 'Valid for 4+ devices' : 'Need 4+ devices for hybrid TP+PP'
                },
                {
                    value: 'hybrid_tp_dp',
                    name: 'Hybrid TP+DP',
                    valid: deviceCount >= 4,
                    reason: deviceCount >= 4 ? 'Valid for 4+ devices' : 'Need 4+ devices for hybrid TP+DP'
                }
            ];

            const results = [];
            let bestStrategy = null;
            let bestRate = -1;
            let bestHasOverflow = true;

            // Test each strategy
            strategies.forEach(strat => {
                if (!strat.valid) {
                    results.push({
                        strategy: strat.value,
                        name: strat.name,
                        rate: 0,
                        valid: false,
                        reason: strat.reason,
                        hasOverflow: false
                    });
                    return;
                }

                // Temporarily set the strategy and calculate metrics
                const tempConfig = { ...baseConfig, parallelismStrategy: strat.value };
                const dtypeSize = DTYPE_SIZES[tempConfig.quantizationType];
                const isPipeline = strat.value === 'pipeline';

                // Calculate metrics for each device
                let totalRate = 0;
                let minRate = Infinity;
                let hasOverflow = false;
                let memoryOverflow = false;

                devices.forEach((device, deviceIndex) => {
                    const memoryBreakdown = calculateMemoryBreakdown(tempConfig, dtypeSize, deviceCount, isPipeline, deviceIndex);
                    const modelPresetData = MODEL_PRESETS[tempConfig.modelPreset] || {};
                    const extendedConfig = { ...tempConfig, ...modelPresetData };
                    const tokenRateInfo = calculateDecodeTokenRate(device, extendedConfig, dtypeSize, devices);

                    if (tokenRateInfo.hasOverflow) hasOverflow = true;
                    if (memoryBreakdown.total > device.memoryGB * 1e9) memoryOverflow = true;

                    totalRate += tokenRateInfo.tokensPerSecond;
                    minRate = Math.min(minRate, tokenRateInfo.tokensPerSecond);
                });

                // Calculate system rate based on parallelism type
                let systemRate;
                if (strat.value === 'data') {
                    // Data parallelism: each device handles separate requests
                    systemRate = totalRate;
                } else if (strat.value === 'tensor' || strat.value === 'hybrid_tp_pp' || strat.value === 'hybrid_tp_dp') {
                    // Tensor parallelism: all devices work on same token
                    systemRate = minRate;
                } else if (strat.value === 'pipeline') {
                    // Pipeline: for batch=1, limited by slowest; for batched, approaches sum
                    const batchSize = tempConfig.batchSize || 1;
                    systemRate = batchSize > 1 ? totalRate * 0.8 : minRate;
                } else if (strat.value === 'expert') {
                    // Expert parallelism: sum with routing overhead
                    systemRate = totalRate * 0.9;
                } else {
                    // Sequence/Context parallelism: similar to tensor
                    systemRate = minRate;
                }

                results.push({
                    strategy: strat.value,
                    name: strat.name,
                    rate: systemRate,
                    valid: true,
                    reason: strat.reason,
                    hasOverflow: hasOverflow,
                    memoryOverflow: memoryOverflow
                });

                // Determine if this is the best strategy
                // Prefer strategies without overflow, then highest rate
                const isBetter =
                    (!hasOverflow && bestHasOverflow) ||
                    (hasOverflow === bestHasOverflow && systemRate > bestRate);

                if (isBetter) {
                    bestRate = systemRate;
                    bestStrategy = strat.value;
                    bestHasOverflow = hasOverflow;
                }
            });

            // Generate reasoning
            let reasoning = '';
            if (deviceCount === 1) {
                reasoning = 'Single device: parallelism has no effect';
                bestStrategy = 'pipeline';
            } else if (bestStrategy === 'data') {
                reasoning = 'Data parallelism maximizes throughput for independent requests';
            } else if (bestStrategy === 'tensor') {
                reasoning = 'Tensor parallelism optimal for shared inference with high-bandwidth interconnect';
            } else if (bestStrategy === 'expert' && isMoE) {
                reasoning = 'Expert parallelism leverages MoE architecture for efficient distribution';
            } else if (bestStrategy === 'hybrid_tp_pp') {
                reasoning = 'Hybrid TP+PP balances memory and compute across many devices';
            } else if (bestStrategy === 'pipeline') {
                reasoning = 'Pipeline parallelism provides good throughput with lower interconnect requirements';
            }

            return {
                strategy: bestStrategy,
                reasoning: reasoning,
                results: results.sort((a, b) => b.rate - a.rate)
            };
        }

        // Global variable to store AUTO results for display
        let lastAutoResults = null;

        // Global variable to store EXO phase split results
        let lastPhaseSplitResults = null;

        /**
         * Estimate TDP (Thermal Design Power) for a device
         * Returns watts based on device template/name
         */
        function estimateDeviceTDP(device) {
            const name = (device.template || device.name || '').toLowerCase();

            // Datacenter GPUs
            if (name.includes('b200')) return 700;
            if (name.includes('h100')) return 700;
            if (name.includes('h200')) return 700;
            if (name.includes('a100')) return 400;
            if (name.includes('l40s')) return 350;
            if (name.includes('l40')) return 300;

            // Consumer NVIDIA GPUs
            if (name.includes('5090')) return 575;
            if (name.includes('5080')) return 360;
            if (name.includes('5070 ti')) return 300;
            if (name.includes('5070')) return 250;
            if (name.includes('5060 ti')) return 180;
            if (name.includes('5060')) return 150;
            if (name.includes('4090')) return 450;
            if (name.includes('4080')) return 320;
            if (name.includes('4070 ti')) return 285;
            if (name.includes('4070')) return 200;
            if (name.includes('4060 ti')) return 165;
            if (name.includes('4060')) return 115;
            if (name.includes('3090')) return 350;
            if (name.includes('3080')) return 320;
            if (name.includes('3070')) return 220;
            if (name.includes('rtx 6000')) return 300;

            // AMD GPUs
            if (name.includes('mi300x')) return 750;
            if (name.includes('mi250x')) return 500;
            if (name.includes('7900 xtx')) return 355;
            if (name.includes('7900 xt')) return 315;

            // Apple Silicon (whole system estimate for inference load)
            if (name.includes('m4 max') || name.includes('m3 max')) return 80;
            if (name.includes('m4 pro') || name.includes('m3 pro')) return 50;
            if (name.includes('m4 ultra') || name.includes('m3 ultra')) return 150;
            if (name.includes('m4') || name.includes('m3')) return 25;
            if (name.includes('m2 ultra')) return 150;
            if (name.includes('m2 max')) return 70;
            if (name.includes('m2')) return 22;
            if (name.includes('m1 ultra')) return 120;
            if (name.includes('m1 max')) return 60;
            if (name.includes('m1')) return 20;

            // Intel CPUs
            if (name.includes('xeon')) return 350;
            if (name.includes('core') && name.includes('ultra')) return 125;

            // Default estimate based on compute capability
            const tflops = device.computeTFlops?.float16 || device.computeTFlops?.float32 || 10;
            // Rough estimate: ~0.3W per TFLOP for modern GPUs
            return Math.max(50, Math.min(800, tflops * 0.3));
        }

        /**
         * Calculate power draw and cost for a given configuration
         * @param {Array} devicesArray - Array of device configurations
         * @param {number} tokensPerSecond - Token generation rate
         * @returns {Object} Power and cost estimates
         */
        function calculatePowerAndCost(devicesArray, tokensPerSecond) {
            // Electricity cost per kWh (US average ~$0.12, datacenter ~$0.06)
            const electricityCostPerKWh = 0.12;

            let totalTDP = 0;
            const devicePower = devicesArray.map(device => {
                const tdp = estimateDeviceTDP(device);
                totalTDP += tdp;
                return { name: device.name || device.template, tdp };
            });

            // During inference, GPUs typically use 50-80% of TDP
            const inferenceLoadFactor = 0.65;
            const actualPowerWatts = totalTDP * inferenceLoadFactor;

            // Cost calculations
            const powerKW = actualPowerWatts / 1000;
            const costPerHour = powerKW * electricityCostPerKWh;
            const tokensPerHour = tokensPerSecond * 3600;
            const costPer1KTokens = tokensPerHour > 0 ? (costPerHour / tokensPerHour) * 1000 : 0;
            const costPer1MTokens = costPer1KTokens * 1000;

            return {
                totalTDP,
                actualPowerWatts,
                powerKW,
                costPerHour,
                costPer1KTokens,
                costPer1MTokens,
                devicePower,
                tokensPerKWh: powerKW > 0 ? tokensPerHour / powerKW : 0
            };
        }

        /**
         * Calculate EXO-style phase split optimization
         * Routes prefill to compute-heavy devices, decode to bandwidth-heavy devices
         */
        function calculateEXOPhaseSplit(devicesArray, modelConfig) {
            if (devicesArray.length < 2) {
                return null;
            }

            const dtypeSize = DTYPE_SIZES[modelConfig.quantizationType];

            // Calculate compute-to-bandwidth ratio for each device
            const deviceRatios = devicesArray.map((device, idx) => {
                const tflops = device.computeTFlops?.[modelConfig.quantizationType] || device.computeTFlops?.float16 || 10;
                const bandwidth = device.localBandwidthGBps || 100;
                // Higher ratio = more compute-heavy (good for prefill)
                // Lower ratio = more bandwidth-heavy (good for decode)
                const ratio = tflops / bandwidth;

                return {
                    device,
                    index: idx,
                    tflops,
                    bandwidth,
                    ratio,
                    name: device.name || device.template
                };
            });

            // Sort by ratio (highest first = most compute-heavy)
            const sorted = [...deviceRatios].sort((a, b) => b.ratio - a.ratio);

            // Split: first half for prefill, second half for decode
            const splitPoint = Math.ceil(sorted.length / 2);
            const prefillDevices = sorted.slice(0, splitPoint);
            const decodeDevices = sorted.slice(splitPoint).length > 0 ? sorted.slice(splitPoint) : sorted;

            // Calculate prefill rate (compute-bound)
            const modelPreset = MODEL_PRESETS[modelConfig.modelPreset] || {};
            const extendedConfig = { ...modelConfig, ...modelPreset };
            const totalPrefillFlops = calculateTransformerFlops(extendedConfig);

            // Sum of prefill device TFLOPS
            const prefillTflops = prefillDevices.reduce((sum, d) => sum + d.tflops, 0);
            const prefillTimeSeconds = totalPrefillFlops / (prefillTflops * 1e12);
            const prefillRate = (modelConfig.batchSize * modelConfig.seqLength) / prefillTimeSeconds;

            // Calculate decode rate (bandwidth-bound)
            const modelSizeBytes = (extendedConfig.activeParamsB || extendedConfig.totalParamsB) * 1e9 * dtypeSize;
            const decodeBandwidth = decodeDevices.reduce((sum, d) => sum + d.bandwidth, 0);
            const theoreticalDecodeRate = (decodeBandwidth * 1e9) / modelSizeBytes;
            const decodeRate = theoreticalDecodeRate * 0.82; // Apply efficiency factor

            // Calculate KV cache transfer overhead
            const kvCacheSize = 2 * modelConfig.batchSize * modelConfig.seqLength *
                (modelConfig.hiddenSize / modelConfig.numHeads) * modelConfig.numHeads *
                dtypeSize * modelConfig.numLayers;

            // Assume minimum network bandwidth between prefill and decode devices
            const minNetworkBw = Math.min(
                ...prefillDevices.map(d => d.device.networkBandwidthGBps || 10),
                ...decodeDevices.map(d => d.device.networkBandwidthGBps || 10)
            );
            const kvTransferTime = kvCacheSize / (minNetworkBw * 1e9);

            return {
                prefillDevices: prefillDevices.map(d => d.name),
                decodeDevices: decodeDevices.map(d => d.name),
                prefillRate: prefillRate,
                decodeRate: decodeRate,
                kvTransferTimeMs: kvTransferTime * 1000,
                totalPrefillTflops: prefillTflops,
                totalDecodeBandwidth: decodeBandwidth,
                reasoning: `Prefill on ${prefillDevices.map(d => d.name).join(', ')} (${prefillTflops.toFixed(0)} TFLOPS), Decode on ${decodeDevices.map(d => d.name).join(', ')} (${decodeBandwidth.toFixed(0)} GB/s)`
            };
        }

	// Load devices from localStorage
	function loadDevices() {
	    const savedDevices = localStorage.getItem('savedDevices');
	    if (savedDevices) {
	        try {
	            let loadedDevices = JSON.parse(savedDevices);

	            // Migrate old devices to new schema with PCIe fields
	            loadedDevices = loadedDevices.map(device => {
	                // Add default PCIe fields if missing
	                const migratedDevice = {
	                    ...device,
	                    pcieGeneration: device.pcieGeneration || 4,
	                    pcieLanes: device.pcieLanes || 16,
	                    overflowTarget: device.overflowTarget || null,
	                    ddr5BandwidthGBps: device.ddr5BandwidthGBps || 115
	                };

	                // Ensure computeTFlops has bfloat16 entry
	                if (migratedDevice.computeTFlops && !migratedDevice.computeTFlops.bfloat16) {
	                    migratedDevice.computeTFlops.bfloat16 = migratedDevice.computeTFlops.float16 || migratedDevice.computeTFlops.float32 * 2;
	                }

	                return migratedDevice;
	            });

	            devices = loadedDevices;
	            updateDeviceDisplay();
	            updateSystemAnalysis();
	        } catch (e) {
	            console.error('Error loading saved devices:', e);
	        }
	    }
	}
	
	// Save devices to localStorage
	function saveDevices() {
	    localStorage.setItem('savedDevices', JSON.stringify(devices));
	}
	
	// Add a new empty device with a default template
	function addDevice() {
	    const newId = devices.length > 0 ? Math.max(...devices.map(d => d.id)) + 1 : 1;
	    const defaultTemplate = 'RTX 5090';
	    const newDevice = {
	        id: newId,
	        name: `Device ${newId}`,
	        template: defaultTemplate,
	        ...DEVICE_TEMPLATES[defaultTemplate]
	    };
	    devices.push(newDevice);
	    updateDeviceDisplay();
	    updateSystemAnalysis();
	    saveDevices();
	}

	// Load scenario preset configurations
	function loadScenarioPreset(scenario) {
	    if (!scenario) return;

	    switch(scenario) {
	        case '5090_ddr5':
	            devices = [{
	                id: 1,
	                name: 'RTX 5090 (Primary)',
	                template: 'RTX 5090',
	                ...DEVICE_TEMPLATES['RTX 5090'],
	                overflowTarget: 'ddr5',
	                ddr5BandwidthGBps: 115
	            }];
	            break;

	        case '5090_5060ti_x1':
	            devices = [
	                {
	                    id: 1,
	                    name: 'RTX 5090 (Primary)',
	                    template: 'RTX 5090',
	                    ...DEVICE_TEMPLATES['RTX 5090'],
	                    networkBandwidthGBps: 4,
	                    interconnectType: 'pcie5_x1',
	                    overflowTarget: 2
	                },
	                {
	                    id: 2,
	                    name: 'RTX 5060 Ti (Secondary via x1)',
	                    template: 'RTX 5060 Ti 16GB',
	                    ...DEVICE_TEMPLATES['RTX 5060 Ti 16GB'],
	                    networkBandwidthGBps: 4,
	                    interconnectType: 'pcie5_x1'
	                }
	            ];
	            break;

	        case '5090_5060ti_x8':
	            devices = [
	                {
	                    id: 1,
	                    name: 'RTX 5090 (x8)',
	                    template: 'RTX 5090',
	                    ...DEVICE_TEMPLATES['RTX 5090'],
	                    networkBandwidthGBps: 32,
	                    interconnectType: 'pcie5_x8',
	                    overflowTarget: 2
	                },
	                {
	                    id: 2,
	                    name: 'RTX 5060 Ti (x8)',
	                    template: 'RTX 5060 Ti 16GB',
	                    ...DEVICE_TEMPLATES['RTX 5060 Ti 16GB'],
	                    networkBandwidthGBps: 32,
	                    interconnectType: 'pcie5_x8'
	                }
	            ];
	            break;

	        case '5090_oculink':
	            devices = [
	                {
	                    id: 1,
	                    name: 'RTX 5090 (Primary)',
	                    template: 'RTX 5090',
	                    ...DEVICE_TEMPLATES['RTX 5090'],
	                    networkBandwidthGBps: 8,
	                    interconnectType: 'oculink_x4',
	                    overflowTarget: 2
	                },
	                {
	                    id: 2,
	                    name: 'eGPU via Oculink (x4)',
	                    template: 'RTX 5060 Ti 16GB',
	                    ...DEVICE_TEMPLATES['RTX 5060 Ti 16GB'],
	                    networkBandwidthGBps: 8,
	                    interconnectType: 'oculink_x4'
	                }
	            ];
	            break;

	        case 'dual_5090':
	            devices = [
	                {
	                    id: 1,
	                    name: 'RTX 5090 #1',
	                    template: 'RTX 5090',
	                    ...DEVICE_TEMPLATES['RTX 5090'],
	                    networkBandwidthGBps: 64,
	                    interconnectType: 'pcie5_x16',
	                    overflowTarget: 2
	                },
	                {
	                    id: 2,
	                    name: 'RTX 5090 #2',
	                    template: 'RTX 5090',
	                    ...DEVICE_TEMPLATES['RTX 5090'],
	                    networkBandwidthGBps: 64,
	                    interconnectType: 'pcie5_x16'
	                }
	            ];
	            break;

	        case '4090_ddr5':
	            devices = [{
	                id: 1,
	                name: 'RTX 4090 (Primary)',
	                template: 'RTX 4090',
	                ...DEVICE_TEMPLATES['RTX 4090'],
	                overflowTarget: 'ddr5',
	                ddr5BandwidthGBps: 115
	            }];
	            break;

	        case '4090_4080':
	            devices = [
	                {
	                    id: 1,
	                    name: 'RTX 4090 (x8)',
	                    template: 'RTX 4090',
	                    ...DEVICE_TEMPLATES['RTX 4090'],
	                    networkBandwidthGBps: 16,
	                    interconnectType: 'pcie4_x8',
	                    overflowTarget: 2
	                },
	                {
	                    id: 2,
	                    name: 'RTX 4080 (x8)',
	                    template: 'RTX 4080',
	                    ...DEVICE_TEMPLATES['RTX 4080'],
	                    networkBandwidthGBps: 16,
	                    interconnectType: 'pcie4_x8'
	                }
	            ];
	            break;

	        case 'm4_max':
	            devices = [{
	                id: 1,
	                name: 'Mac M4 Max',
	                template: 'Mac M4 Max (128)',
	                ...DEVICE_TEMPLATES['Mac M4 Max (128)']
	            }];
	            break;

	        case 'm3_ultra':
	            devices = [{
	                id: 1,
	                name: 'Mac M3 Ultra',
	                template: 'Mac M3 Ultra (512)',
	                ...DEVICE_TEMPLATES['Mac M3 Ultra (512)']
	            }];
	            break;

	        case 'h100_single':
	            devices = [{
	                id: 1,
	                name: 'NVIDIA H100',
	                template: 'H100',
	                ...DEVICE_TEMPLATES['H100']
	            }];
	            break;

	        case 'h100_x8':
	            devices = [];
	            for (let i = 1; i <= 8; i++) {
	                devices.push({
	                    id: i,
	                    name: `H100 #${i}`,
	                    template: 'H100',
	                    ...DEVICE_TEMPLATES['H100'],
	                    networkBandwidthGBps: 450,
	                    interconnectType: 'nvlink4'
	                });
	            }
	            break;

	        default:
	            return;
	    }

	    updateDeviceDisplay();
	    updateSystemAnalysis();
	    saveDevices();
	    document.getElementById('scenarioPreset').value = '';  // Reset dropdown
	}

	// Clone an existing device
	function cloneDevice(id) {
	    const sourceDevice = devices.find(d => d.id === id);
	    if (!sourceDevice) return;
	    
	    const newId = Math.max(...devices.map(d => d.id)) + 1;
	    const newDevice = {
	        ...JSON.parse(JSON.stringify(sourceDevice)), // Deep clone
	        id: newId,
	        name: `${sourceDevice.name} (Clone)`
	    };
	    
	    devices.push(newDevice);
	    updateDeviceDisplay();
	    updateSystemAnalysis();
	    saveDevices();
	}
	
	// Remove a device
	function removeDevice(id) {
	    if (devices.length > 1) {
	        devices = devices.filter(d => d.id !== id);
	        updateDeviceDisplay();
	        updateSystemAnalysis();
	        saveDevices();
	    }
	}
	
	// Device library management
	function saveToLibrary(id) {
	    const device = devices.find(d => d.id === id);
	    if (!device) return;
	    
	    // Get existing library
	    let deviceLibrary = [];
	    try {
	        const savedLibrary = localStorage.getItem('deviceLibrary');
	        if (savedLibrary) {
	            deviceLibrary = JSON.parse(savedLibrary);
	        }
	    } catch (e) {
	        console.error('Error loading device library:', e);
	    }
	    
	    // Create a copy to save
	    const deviceToSave = {
	        ...JSON.parse(JSON.stringify(device)),
	        template: 'Custom', // Force as custom when saved
	        id: Date.now() // Use timestamp as unique ID in library
	    };
	    
	    // Add to library
	    deviceLibrary.push(deviceToSave);
	    localStorage.setItem('deviceLibrary', JSON.stringify(deviceLibrary));
	    
	    alert(`Device "${device.name}" saved to library!`);
	    updateDeviceDisplay(); // Refresh to show updated library
	}
        function calculateMetrics() {
			let modelConfig = getModelConfig();
			const optimizationMode = document.getElementById('optimizationMode')?.value || 'none';

			// Handle AUTO mode - find optimal strategy first
			if (modelConfig.parallelismStrategy === 'auto') {
				lastAutoResults = findOptimalStrategy();
				// Use the optimal strategy for actual calculations
				modelConfig = { ...modelConfig, parallelismStrategy: lastAutoResults.strategy };
			} else {
				lastAutoResults = null;
			}

			// Handle EXO Phase Split optimization
			if (optimizationMode === 'exo_phase_split' && devices.length >= 2) {
				lastPhaseSplitResults = calculateEXOPhaseSplit(devices, modelConfig);
			} else {
				lastPhaseSplitResults = null;
			}

			const dtypeSize = DTYPE_SIZES[modelConfig.quantizationType];
			const isPipeline = modelConfig.parallelismStrategy === 'pipeline';
			const deviceCount = devices.length;

			return devices.map((device, deviceIndex) => {
				// Calculate per-device memory and network requirements
				const memoryBreakdown = calculateMemoryBreakdown(modelConfig, dtypeSize, deviceCount, isPipeline, deviceIndex);
				const networkTraffic = calculateNetworkTraffic(modelConfig, dtypeSize, deviceCount, isPipeline);
				
				const quantSpeedFactor = {
					'float32': 1.0,
					'bfloat16': 2.0,
					'float16': 2.0,
					'int8': 2.5,
					'q4': 3.0
				}[modelConfig.quantizationType] || 1.0;

				const totalPrefillFlops = calculateTransformerFlops(modelConfig) / (deviceCount * quantSpeedFactor);
				const theoreticalPrefillComputeTime = totalPrefillFlops / (device.computeTFlops[modelConfig.quantizationType] * 1e12);


				// Network time calculation
				const networkTime = networkTraffic / (device.networkBandwidthGBps * 1e9);
				
				// Memory traffic calculation adjusted for distributed setup
				// For inference, we only need forward pass (no gradients), so 1x activations not 3x
				const memoryTrafficScale = modelConfig.quantizationType === 'q4' ? 1.15 : 1.0; // Q4 has some overhead
				const totalPrefillMemoryTraffic = memoryTrafficScale * (
					memoryBreakdown.paramsMemory +
					(2 * memoryBreakdown.kvCacheMemory) +
					(1 * memoryBreakdown.activationMemory) +  // Only forward pass, not backward
					(1 * memoryBreakdown.attentionMemory)
				);

				const prefillBandwidthLimitedTime = totalPrefillMemoryTraffic / (device.localBandwidthGBps * 1e9);

				// Include network time in actual execution time
				// Prefill overhead accounts for kernel launch, memory access patterns, etc.
				const overheadFactorPrefill = 1.10;
				const actualPrefillTime = Math.max(
					theoreticalPrefillComputeTime,
					prefillBandwidthLimitedTime,
					networkTime
				) * overheadFactorPrefill;

				// Calculate utilizations
				const prefillComputeUtilization = (theoreticalPrefillComputeTime / actualPrefillTime) * 100;
				const prefillLocalBandwidthUtilization = (prefillBandwidthLimitedTime / actualPrefillTime) * 100;
				const networkBandwidthUtilization = (networkTime / actualPrefillTime) * 100;
				const memoryUtilization = (memoryBreakdown.total / (device.memoryGB * 1e9)) * 100;

				// Calculate bottleneck factor
				const bottleneckFactor = Math.max(
					1,
					prefillComputeUtilization / 100,
					memoryUtilization / 100,
					prefillLocalBandwidthUtilization / 100,
					networkBandwidthUtilization / 100
				);

				// Calculate token throughput
				const prefillTokensPerSecond = modelConfig.batchSize * modelConfig.seqLength / actualPrefillTime;

				// Decode calculations remain similar but scaled by device count
				const decodeFlops = calculateDecodeFlops(modelConfig) / deviceCount;
				const theoreticalDecodeTime = decodeFlops / (device.computeTFlops[modelConfig.quantizationType] * 1e12);

				const decodeMemoryTrafficPerToken = (
					memoryBreakdown.paramsMemory + 
					(2 * modelConfig.seqLength * (modelConfig.hiddenSize / modelConfig.numHeads) * 
					 modelConfig.numHeads * dtypeSize) * modelConfig.numLayers / deviceCount + 
					(4 * modelConfig.hiddenSize * dtypeSize) * modelConfig.numLayers / deviceCount
				) / modelConfig.seqLength;

				const decodeBandwidthLimitedTime = decodeMemoryTrafficPerToken / (device.localBandwidthGBps * 1e9);
				// Decode overhead factor: 1.3 is more realistic than 1.5 (was too pessimistic)
				const overheadFactorDecode = 1.3;
				const actualDecodeTime = Math.max(theoreticalDecodeTime, decodeBandwidthLimitedTime) *
					overheadFactorDecode;

				// NEW: Calculate overflow-aware token rate using the new bandwidth model
				const modelPreset = MODEL_PRESETS[modelConfig.modelPreset] || {};
				const extendedModelConfig = { ...modelConfig, ...modelPreset };
				const tokenRateInfo = calculateDecodeTokenRate(device, extendedModelConfig, dtypeSize, devices);

				// Check if this is an image generation model
				const isImageGen = modelPreset.isImageGen || false;
				let imageGenInfo = null;
				if (isImageGen) {
					imageGenInfo = calculateImageGenMetrics(device, extendedModelConfig, modelConfig.quantizationType);
				}

				return {
					name: device.name,
					memoryUtilization,
					localBandwidthUtilization: prefillLocalBandwidthUtilization,
					networkBandwidthUtilization,
					computeUtilization: prefillComputeUtilization,
					rawMemoryUtilization: memoryUtilization,
					rawLocalBandwidthUtilization: prefillLocalBandwidthUtilization,
					rawNetworkBandwidthUtilization: networkBandwidthUtilization,
					rawComputeUtilization: prefillComputeUtilization,
					bottleneckFactor,
					isBottleneck: bottleneckFactor > 1,
					prefillTokensPerSecond,
					decodeTokensPerSecond: tokenRateInfo.tokensPerSecond,
					theoreticalMaxTokensPerSecond: tokenRateInfo.theoreticalMax,
					decodeComputeUtilization: (theoreticalDecodeTime / actualDecodeTime) * 100,
					decodeLocalBandwidthUtilization: (decodeBandwidthLimitedTime / actualDecodeTime) * 100,
					decodeBottleneckFactor: Math.max(
						theoreticalDecodeTime / actualDecodeTime,
						decodeBandwidthLimitedTime / actualDecodeTime
					),
					// NEW: Overflow information
					hasOverflow: tokenRateInfo.hasOverflow,
					overflowGB: tokenRateInfo.overflowGB || 0,
					overflowBandwidthGBps: tokenRateInfo.overflowBandwidthGBps || 0,
					effectiveBandwidthGBps: tokenRateInfo.effectiveBandwidthGBps,
					overflowBottleneckReason: tokenRateInfo.bottleneckReason || 'none',
					modelSizeGB: tokenRateInfo.modelSizeGB,
					// Image generation info
					isImageGen,
					imagesPerSecond: imageGenInfo ? imageGenInfo.imagesPerSecond : null,
					timePerImage: imageGenInfo ? imageGenInfo.timePerImage : null
				};
			});
		}

        function updateSystemAnalysis() {
            const metrics = calculateMetrics();
            console.log(metrics);
            const analysisContainer = document.getElementById('systemAnalysis');
            let systemAnalysisHtml = '';
            metrics.forEach((m, idx) => {
                const device = devices[idx];
                systemAnalysisHtml += `
                    <div style="margin-bottom: 1rem; padding: 1rem; background: var(--background); border-radius: var(--radius); border: 1px solid var(--border);">
                        <h3 style="margin-top: 0;">${m.name}</h3>
                        <div style="font-size: 0.8rem; color: var(--text-secondary); margin-bottom: 0.5rem;">
                            Model Size: ${m.modelSizeGB ? m.modelSizeGB.toFixed(1) : 'N/A'} GB | Device Memory: ${device.memoryGB} GB
                        </div>
                        <div class="device-grid" style="font-size: 0.875rem;">
                            <div>Memory Usage: ${m.memoryUtilization.toFixed(1)}%</div>
                            <div>Compute Usage: ${m.computeUtilization.toFixed(1)}%</div>
                            <div>Local BW Usage: ${m.localBandwidthUtilization.toFixed(1)}%</div>
                            <div>Network BW Usage: ${m.networkBandwidthUtilization.toFixed(1)}%</div>
                `;

                // Show image gen metrics if applicable
                if (m.isImageGen && m.imagesPerSecond) {
                    systemAnalysisHtml += `
                            <div style="grid-column: span 2;"><strong>🖼️ Images/Second:</strong> ${m.imagesPerSecond.toFixed(2)}</div>
                            <div style="grid-column: span 2;"><strong>Time per Image:</strong> ${m.timePerImage.toFixed(2)}s</div>
                    `;
                } else {
                    systemAnalysisHtml += `
                            <div><strong>Prefill Rate:</strong> ${m.prefillTokensPerSecond.toFixed(1)} tok/s</div>
                            <div><strong>Decode Rate:</strong> ${m.decodeTokensPerSecond.toFixed(1)} tok/s</div>
                    `;
                }

                systemAnalysisHtml += `</div>`;

                // Show overflow warning if applicable
                if (m.hasOverflow) {
                    systemAnalysisHtml += `
                        <div style="background: rgba(255, 165, 0, 0.15); border: 1px solid #ffa500; border-radius: 4px; padding: 0.75rem; margin-top: 0.75rem;">
                            <div style="color: #ffa500; font-weight: 600; margin-bottom: 0.25rem;">
                                ⚠️ Memory Overflow Detected
                            </div>
                            <div style="font-size: 0.8rem; color: var(--text-secondary);">
                                <strong>${m.overflowGB.toFixed(1)} GB</strong> overflows to ${m.overflowBottleneckReason}<br>
                                Overflow bandwidth: <strong>${m.overflowBandwidthGBps.toFixed(1)} GB/s</strong><br>
                                Effective bandwidth: <strong>${m.effectiveBandwidthGBps.toFixed(1)} GB/s</strong> (vs ${device.localBandwidthGBps} GB/s local)
                            </div>
                        </div>
                    `;
                }

                // Determine bottleneck for reporting
                let bottleneckType = "None";
                let bottleneckValue = 1;
                if (m.isBottleneck) {
                    if (m.computeUtilization > bottleneckValue) {
                        bottleneckType = "Compute";
                        bottleneckValue = m.computeUtilization;
                    }
                    if (m.memoryUtilization > bottleneckValue) {
                        bottleneckType = "Memory";
                        bottleneckValue = m.memoryUtilization;
                    }
                    if (m.localBandwidthUtilization > bottleneckValue) {
                        bottleneckType = "Local Bandwidth";
                        bottleneckValue = m.localBandwidthUtilization;
                    }
                    if (m.networkBandwidthUtilization > bottleneckValue) {
                        bottleneckType = "Network Bandwidth";
                        bottleneckValue = m.networkBandwidthUtilization;
                    }
                    systemAnalysisHtml += `
                        <div style="color: #ff4444; font-size: 0.875rem; margin-top: 0.5rem;">
                            ⛔ Prefill limited by <strong>${bottleneckType}</strong> (${m.bottleneckFactor.toFixed(1)}x slower)
                        </div>
                    `;
                }
                // Decode bottleneck (simplified for reporting)
                if (m.decodeBottleneckFactor > 1 && !m.isImageGen) {
                    let decodeBottleneckType = "None";
                    if (m.decodeComputeUtilization > m.decodeLocalBandwidthUtilization) {
                        decodeBottleneckType = "Compute";
                    } else {
                        decodeBottleneckType = "Local Bandwidth";
                    }
                    systemAnalysisHtml += `
                        <div style="color: #ff4444; font-size: 0.875rem; margin-top: 0.25rem;">
                            ⛔ Decode limited by <strong>${decodeBottleneckType}</strong> (${m.decodeBottleneckFactor.toFixed(1)}x slower)
                        </div>
                    `;
                }
                systemAnalysisHtml += `</div>`;
            });

            // Add System Total Summary
            if (metrics.length > 0) {
                const modelConfig = getModelConfig();
                // Get effective parallelism (AUTO mode uses the selected strategy)
                const isAutoMode = modelConfig.parallelismStrategy === 'auto';
                const parallelism = isAutoMode && lastAutoResults ? lastAutoResults.strategy : (modelConfig.parallelismStrategy || 'pipeline');
                const totalMemory = devices.reduce((sum, d) => sum + d.memoryGB, 0);
                const isImageGen = metrics[0].isImageGen;

                let systemRate, systemRateLabel, rateExplanation;

                if (isImageGen) {
                    // Image gen: sum for throughput
                    systemRate = metrics.reduce((sum, m) => sum + (m.imagesPerSecond || 0), 0);
                    systemRateLabel = `${systemRate.toFixed(2)} img/s`;
                    rateExplanation = 'Combined throughput';
                } else {
                    // LLM inference
                    const decodeRates = metrics.map(m => m.decodeTokensPerSecond || 0);
                    const minRate = Math.min(...decodeRates);
                    const sumRate = decodeRates.reduce((a, b) => a + b, 0);

                    if (parallelism === 'data') {
                        // Data Parallelism: each device handles different requests
                        systemRate = sumRate;
                        rateExplanation = 'Sum (each device handles separate requests)';
                    } else if (parallelism === 'tensor' || parallelism === 'hybrid_tp_pp' || parallelism === 'hybrid_tp_dp') {
                        // Tensor Parallelism: all devices work on same inference
                        systemRate = minRate;
                        rateExplanation = 'Shared inference (limited by slowest device)';
                    } else if (parallelism === 'pipeline') {
                        // Pipeline: throughput can approach sum, latency is sum of stages
                        // For interactive (batch=1), limited by slowest; for batched, approaches sum
                        const batchSize = parseFloat(modelConfig.batchSize) || 1;
                        if (batchSize > 1) {
                            systemRate = sumRate * 0.8; // ~80% efficiency with pipelining
                            rateExplanation = 'Pipeline throughput (~80% efficiency)';
                        } else {
                            systemRate = minRate;
                            rateExplanation = 'Pipeline (batch=1, limited by slowest stage)';
                        }
                    } else if (parallelism === 'expert') {
                        // Expert parallelism: sum with routing overhead
                        systemRate = sumRate * 0.9;
                        rateExplanation = 'Expert parallelism (~90% efficiency)';
                    } else {
                        // Default: use min for safety
                        systemRate = minRate;
                        rateExplanation = 'Limited by slowest device';
                    }
                    systemRateLabel = `${systemRate.toFixed(1)} tok/s`;
                }

                // Find system bottleneck
                let systemBottleneck = 'None';
                const avgMemUtil = metrics.reduce((sum, m) => sum + m.memoryUtilization, 0) / metrics.length;
                const avgBwUtil = metrics.reduce((sum, m) => sum + m.localBandwidthUtilization, 0) / metrics.length;
                const anyOverflow = metrics.some(m => m.hasOverflow);

                if (anyOverflow) systemBottleneck = 'Memory Overflow';
                else if (avgMemUtil > 90) systemBottleneck = 'Memory Capacity';
                else if (avgBwUtil > 80) systemBottleneck = 'Bandwidth';

                // Build AUTO mode section if applicable
                let autoModeHtml = '';
                if (isAutoMode && lastAutoResults) {
                    autoModeHtml = `
                        <div style="margin-top: 0.75rem; padding: 0.75rem; background: rgba(74, 222, 128, 0.1); border: 1px solid #4ade80; border-radius: 6px;">
                            <div style="color: #4ade80; font-weight: 600; margin-bottom: 0.5rem;">
                                🎯 AUTO Selected: ${parallelism.replace('_', ' ').toUpperCase()}
                            </div>
                            <div style="font-size: 0.8rem; color: var(--text-secondary); margin-bottom: 0.5rem;">
                                ${lastAutoResults.reasoning}
                            </div>
                            <details style="font-size: 0.75rem;">
                                <summary style="cursor: pointer; color: var(--text-secondary);">View all strategies compared</summary>
                                <div style="margin-top: 0.5rem; max-height: 200px; overflow-y: auto;">
                                    <table style="width: 100%; border-collapse: collapse; font-size: 0.7rem;">
                                        <tr style="border-bottom: 1px solid var(--border);">
                                            <th style="text-align: left; padding: 0.25rem;">Strategy</th>
                                            <th style="text-align: right; padding: 0.25rem;">Rate</th>
                                            <th style="text-align: left; padding: 0.25rem;">Status</th>
                                        </tr>
                                        ${lastAutoResults.results.map(r => `
                                            <tr style="border-bottom: 1px solid var(--border); ${r.strategy === parallelism ? 'background: rgba(74, 222, 128, 0.15);' : ''}">
                                                <td style="padding: 0.25rem;">${r.name}${r.strategy === parallelism ? ' ✓' : ''}</td>
                                                <td style="text-align: right; padding: 0.25rem;">${r.valid ? r.rate.toFixed(1) + ' tok/s' : '-'}</td>
                                                <td style="padding: 0.25rem; color: ${r.valid ? (r.hasOverflow ? '#fbbf24' : '#4ade80') : '#f87171'};">
                                                    ${r.valid ? (r.hasOverflow ? 'Overflow' : 'OK') : r.reason}
                                                </td>
                                            </tr>
                                        `).join('')}
                                    </table>
                                </div>
                            </details>
                        </div>
                    `;
                }

                // Build EXO Phase Split section if applicable
                let phaseSplitHtml = '';
                if (lastPhaseSplitResults) {
                    phaseSplitHtml = `
                        <div style="margin-top: 0.75rem; padding: 0.75rem; background: rgba(168, 85, 247, 0.1); border: 1px solid #a855f7; border-radius: 6px;">
                            <div style="color: #a855f7; font-weight: 600; margin-bottom: 0.5rem;">
                                ⚡ EXO Phase Split Optimization
                            </div>
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 0.5rem; font-size: 0.8rem;">
                                <div>
                                    <strong>Prefill:</strong> ${lastPhaseSplitResults.prefillDevices.join(', ')}
                                    <div style="color: #4ade80; font-size: 0.9rem;">${lastPhaseSplitResults.prefillRate.toFixed(0)} tok/s</div>
                                </div>
                                <div>
                                    <strong>Decode:</strong> ${lastPhaseSplitResults.decodeDevices.join(', ')}
                                    <div style="color: #4ade80; font-size: 0.9rem;">${lastPhaseSplitResults.decodeRate.toFixed(1)} tok/s</div>
                                </div>
                            </div>
                            <div style="font-size: 0.75rem; color: var(--text-secondary); margin-top: 0.25rem;">
                                KV Cache Transfer: ~${lastPhaseSplitResults.kvTransferTimeMs.toFixed(1)} ms
                            </div>
                        </div>
                    `;
                }

                // Calculate power and cost estimates
                const powerCost = calculatePowerAndCost(devices, systemRate);
                let powerCostHtml = '';
                if (!isImageGen && systemRate > 0) {
                    powerCostHtml = `
                        <div style="margin-top: 0.75rem; padding: 0.75rem; background: rgba(251, 191, 36, 0.1); border: 1px solid #fbbf24; border-radius: 6px;">
                            <div style="color: #fbbf24; font-weight: 600; margin-bottom: 0.5rem;">
                                ⚡ Power & Cost Estimate
                            </div>
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 0.5rem; font-size: 0.8rem;">
                                <div>
                                    <strong>Est. Power Draw:</strong>
                                    <div style="color: #fff; font-size: 0.9rem;">${powerCost.actualPowerWatts.toFixed(0)} W</div>
                                    <div style="color: var(--text-secondary); font-size: 0.7rem;">TDP: ${powerCost.totalTDP} W</div>
                                </div>
                                <div>
                                    <strong>Cost (@ $0.12/kWh):</strong>
                                    <div style="color: #fff; font-size: 0.9rem;">$${powerCost.costPer1MTokens.toFixed(4)}/M tok</div>
                                    <div style="color: var(--text-secondary); font-size: 0.7rem;">$${powerCost.costPerHour.toFixed(3)}/hr</div>
                                </div>
                            </div>
                            <div style="font-size: 0.7rem; color: var(--text-secondary); margin-top: 0.25rem;">
                                Efficiency: ${(powerCost.tokensPerKWh / 1000).toFixed(1)}K tok/kWh
                            </div>
                        </div>
                    `;
                }

                systemAnalysisHtml += `
                    <div style="margin-top: 1rem; padding: 1rem; background: linear-gradient(135deg, rgba(99, 102, 241, 0.15), rgba(79, 70, 229, 0.1)); border-radius: var(--radius); border: 2px solid var(--primary);">
                        <h3 style="margin: 0 0 0.75rem 0; color: var(--primary);">📊 System Total</h3>
                        <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 0.5rem; font-size: 0.9rem;">
                            <div><strong>Total Memory:</strong> ${totalMemory} GB</div>
                            <div><strong>Devices:</strong> ${devices.length}</div>
                            <div style="grid-column: span 2;">
                                <strong>${isImageGen ? 'System Throughput:' : 'System Decode Rate:'}</strong>
                                <span style="color: #4ade80; font-size: 1.25rem; font-weight: bold; margin-left: 0.5rem;">${systemRateLabel}</span>
                            </div>
                            <div style="grid-column: span 2; font-size: 0.8rem; color: var(--text-secondary);">
                                ${rateExplanation} (${isAutoMode ? 'AUTO → ' : ''}${parallelism.replace('_', ' ').toUpperCase()})
                            </div>
                            ${systemBottleneck !== 'None' ? `
                            <div style="grid-column: span 2; color: #fbbf24;">
                                ⚠️ System Bottleneck: <strong>${systemBottleneck}</strong>
                            </div>` : ''}
                        </div>
                        ${autoModeHtml}
                        ${phaseSplitHtml}
                        ${powerCostHtml}
                    </div>
                `;
            }

            analysisContainer.innerHTML = systemAnalysisHtml;
            updateAlerts(metrics);
            updateChart(metrics);

            // Update model summary in collapsed header
            if (typeof updateModelSummary === 'function') {
                updateModelSummary();
            }
        }
	    
	function updateDeviceDisplay() {
	    const currentQuant = document.getElementById('quantizationType').value;
	    const devicesContainer = document.getElementById('devices');
	    
	    // Get saved device library
	    let deviceLibrary = [];
	    try {
	        const savedLibrary = localStorage.getItem('deviceLibrary');
	        if (savedLibrary) {
	            deviceLibrary = JSON.parse(savedLibrary);
	        }
	    } catch (e) {
	        console.error('Error loading device library:', e);
	    }
	    
	    // Create custom library options
	    const libraryOptions = deviceLibrary.map(device => 
	        `<option value="lib_${device.id}">${device.name}</option>`
	    ).join('');
	    
	    devicesContainer.innerHTML = devices.map(device => `
	        <div class="device ${device.template === 'Custom' || device.isEdited ? 'custom-device' : ''}">
	            ${device.template === 'Custom' || device.isEdited ? '<div class="custom-badge">Custom</div>' : ''}
	            <div style="display: flex; justify-content: space-between; margin-bottom: 0.5rem;">
	                <input class="input" title="Give this device a name" style="width: auto; flex-grow: 1; margin-right: 0.5rem;" value="${device.name}"
	                       onchange="updateDevice(${device.id}, 'name', this.value)">
	                <div class="device-actions">
	                    <button class="button" 
	                            onclick="cloneDevice(${device.id})"
	                            title="Clone this device">
	                        Clone
	                    </button>
	                    <button class="button" 
	                            onclick="saveToLibrary(${device.id})"
	                            title="Save to library">
	                        Save
	                    </button>
	                    <button class="button button-destructive"
	                            onclick="removeDevice(${device.id})"
	                            ${devices.length === 1 ? 'disabled' : ''}
	                            title="Remove this device">
	                        Remove
	                    </button>
	                </div>
	            </div>
	            <select class="input" onchange="handleDeviceTemplateChange(${device.id}, this)">
	                <option value="Custom" ${device.template === 'Custom' ? 'selected' : ''}>
	                    Custom Device
	                </option>
	                <optgroup label="Preset Library">
	                    ${Object.keys(DEVICE_TEMPLATES)
	                        .filter(key => key !== 'Custom')
	                        .map(template => `
	                            <option value="${template}" ${device.template === template && !device.isEdited ? 'selected' : ''}>
	                                ${DEVICE_TEMPLATES[template].name}
	                            </option>
	                        `).join('')}
	                </optgroup>
	                ${deviceLibrary.length > 0 ? 
	                    `<optgroup label="My Saved Devices">
	                        ${libraryOptions}
	                    </optgroup>` : ''}
	            </select>
	            <div class="device-grid">
	                <div>
	                    <label>Memory GB</label>
	                    <input type="number" class="input" value="${device.memoryGB}"
	                           onchange="updateDevice(${device.id}, 'memoryGB', parseFloat(this.value) || 0)"
	                           oninput="updateDevice(${device.id}, 'memoryGB', parseFloat(this.value) || 0)">
	                </div>
	                <div>
	                    <label>Local Bandwidth GB/s</label>
	                    <input type="number" class="input" value="${device.localBandwidthGBps}"
	                           onchange="updateDevice(${device.id}, 'localBandwidthGBps', parseFloat(this.value) || 0)"
	                           oninput="updateDevice(${device.id}, 'localBandwidthGBps', parseFloat(this.value) || 0)">
	                </div>
	                <div>
	                    <label>Network/Interconnect BW (GB/s)</label>
	                    <input type="number" class="input" value="${device.networkBandwidthGBps || 32}"
	                           onchange="updateDevice(${device.id}, 'networkBandwidthGBps', parseFloat(this.value) || 0)"
	                           oninput="updateDevice(${device.id}, 'networkBandwidthGBps', parseFloat(this.value) || 0)">
	                </div>
	                <div>
	                    <label>Interconnect Preset</label>
	                    <select class="input" onchange="applyInterconnectPreset(${device.id}, this.value)">
	                        <option value="">Custom</option>
	                        <optgroup label="PCIe">
	                            <option value="pcie3_x1">PCIe 3.0 x1 (1 GB/s)</option>
	                            <option value="pcie3_x4">PCIe 3.0 x4 (4 GB/s)</option>
	                            <option value="pcie3_x8">PCIe 3.0 x8 (8 GB/s)</option>
	                            <option value="pcie3_x16">PCIe 3.0 x16 (16 GB/s)</option>
	                            <option value="pcie4_x1">PCIe 4.0 x1 (2 GB/s)</option>
	                            <option value="pcie4_x4">PCIe 4.0 x4 (8 GB/s)</option>
	                            <option value="pcie4_x8">PCIe 4.0 x8 (16 GB/s)</option>
	                            <option value="pcie4_x16">PCIe 4.0 x16 (32 GB/s)</option>
	                            <option value="pcie5_x1">PCIe 5.0 x1 (4 GB/s)</option>
	                            <option value="pcie5_x4">PCIe 5.0 x4 (16 GB/s)</option>
	                            <option value="pcie5_x8">PCIe 5.0 x8 (32 GB/s)</option>
	                            <option value="pcie5_x16">PCIe 5.0 x16 (64 GB/s)</option>
	                            <option value="pcie6_x16">PCIe 6.0 x16 (128 GB/s)</option>
	                        </optgroup>
	                        <optgroup label="Thunderbolt / USB4">
	                            <option value="thunderbolt3">Thunderbolt 3 (5 GB/s)</option>
	                            <option value="thunderbolt4">Thunderbolt 4 (5 GB/s)</option>
	                            <option value="thunderbolt5">Thunderbolt 5 (10 GB/s)</option>
	                            <option value="usb4_40">USB4 40Gbps (5 GB/s)</option>
	                            <option value="usb4_80">USB4 80Gbps (10 GB/s)</option>
	                            <option value="usb4_120">USB4 120Gbps (15 GB/s)</option>
	                        </optgroup>
	                        <optgroup label="Oculink">
	                            <option value="oculink_x4">Oculink x4 (8 GB/s)</option>
	                            <option value="oculink_x8">Oculink x8 (16 GB/s)</option>
	                        </optgroup>
	                        <optgroup label="Ethernet">
	                            <option value="1gbe">1 GbE (0.125 GB/s)</option>
	                            <option value="10gbe">10 GbE (1.25 GB/s)</option>
	                            <option value="25gbe">25 GbE (3.1 GB/s)</option>
	                            <option value="40gbe">40 GbE (5 GB/s)</option>
	                            <option value="100gbe">100 GbE (12.5 GB/s)</option>
	                            <option value="200gbe">200 GbE (25 GB/s)</option>
	                            <option value="400gbe">400 GbE (50 GB/s)</option>
	                            <option value="800gbe">800 GbE (100 GB/s)</option>
	                        </optgroup>
	                        <optgroup label="InfiniBand">
	                            <option value="ib_fdr">InfiniBand FDR (6.8 GB/s)</option>
	                            <option value="ib_edr">InfiniBand EDR (12.5 GB/s)</option>
	                            <option value="ib_hdr">InfiniBand HDR (25 GB/s)</option>
	                            <option value="ib_ndr">InfiniBand NDR (50 GB/s)</option>
	                            <option value="ib_xdr">InfiniBand XDR (100 GB/s)</option>
	                        </optgroup>
	                        <optgroup label="NVLink / Proprietary">
	                            <option value="nvlink3">NVLink 3.0 A100 (300 GB/s)</option>
	                            <option value="nvlink4">NVLink 4.0 H100 (450 GB/s)</option>
	                            <option value="nvlink5">NVLink 5.0 B200 (900 GB/s)</option>
	                            <option value="nvswitch">NVSwitch (450 GB/s)</option>
	                            <option value="infinity_fabric">AMD Infinity Fabric (100 GB/s)</option>
	                            <option value="ultrafusion">Apple UltraFusion (2500 GB/s)</option>
	                        </optgroup>
	                    </select>
	                </div>
	                <div>
	                    <label>Compute TFlops (${currentQuant})</label>
	                    <input type="number" class="input" value="${device.computeTFlops[currentQuant]}"
	                           onchange="updateDevice(${device.id}, 'computeTFlops', parseFloat(this.value) || 0)"
	                           oninput="updateDevice(${device.id}, 'computeTFlops', parseFloat(this.value) || 0)">
	                </div>
	                <div>
	                    <label>Overflow Target</label>
	                    <select class="input" onchange="updateDevice(${device.id}, 'overflowTarget', this.value === 'none' ? null : (this.value === 'ddr5' ? 'ddr5' : parseInt(this.value)))">
	                        <option value="none" ${!device.overflowTarget ? 'selected' : ''}>None (model must fit)</option>
	                        <option value="ddr5" ${device.overflowTarget === 'ddr5' ? 'selected' : ''}>DDR5 System RAM (~115 GB/s)</option>
	                        ${devices.filter(d => d.id !== device.id).map(d =>
	                            `<option value="${d.id}" ${device.overflowTarget === d.id ? 'selected' : ''}>
	                                ${d.name} (via PCIe)
	                            </option>`
	                        ).join('')}
	                    </select>
	                </div>
	            </div>
	        </div>
	    `).join('');
	    
	    // Add the "Add New Device" button at the bottom
	    devicesContainer.innerHTML += `
	        <div style="display: flex; justify-content: center; margin-top: 1rem;">
	            <button class="button" onclick="addDevice()" style="width: 100%;">
	                + Add New Device
	            </button>
	        </div>
	    `;
	}
        function updateAlerts(metrics) {
            const alertsContainer = document.getElementById('alerts');
            let alertsHtml = '';
            if (metrics.some(m => m.isBottleneck)) {
                alertsHtml += `
                    <div class="alert">
                        <div style="font-weight: 600; margin-bottom: 0.5rem;">
                            Configuration Issues:
                        </div>
                        ${metrics.filter(m => m.isBottleneck).map(m => `
                            <div style="margin-left: 1rem;">
                                • ${m.name}:
                                ${m.memoryUtilization > 100 ?
                    ` Requires ${(m.memoryUtilization / 100).toFixed(1)}x more memory` : ''}
                                ${m.localBandwidthUtilization > 100 ?
                    ` Requires ${(m.localBandwidthUtilization / 100).toFixed(1)}x more local bandwidth` : ''}
                                ${m.networkBandwidthUtilization > 100 && getModelConfig().parallelismStrategy === 'pipeline' ?
                    ` Requires ${(m.networkBandwidthUtilization / 100).toFixed(1)}x more network bandwidth` : ''}
                                ${m.computeUtilization > 100 ?
                    ` Requires ${(m.computeUtilization / 100).toFixed(1)}x more compute` : ''}
                            </div>
                        `).join('')}
                    </div>
                `;
            }
            alertsContainer.innerHTML = alertsHtml;
        }
	    
        function updateChart(metrics) {
            const ctx = document.getElementById('utilizationChart').getContext('2d');
            const chartData = {
                labels: metrics.map(m => m.name),
                datasets: [
                    {
                        label: 'Memory',
                        data: metrics.map(m => m.memoryUtilization),
                        backgroundColor: '#8884d8'
                    },
                    {
                        label: 'Local Bandwidth',
                        data: metrics.map(m => m.localBandwidthUtilization),
                        backgroundColor: '#82ca9d'
                    },
                    {
                        label: 'Network Bandwidth',
                        data: metrics.map(m => m.networkBandwidthUtilization),
                        backgroundColor: '#ffc658'
                    },
                    {
                        label: 'Compute',
                        data: metrics.map(m => m.computeUtilization),
                        backgroundColor: '#ff7300'
                    }
                ]
            };
            if (utilizationChart) {
                utilizationChart.destroy();
            }
            utilizationChart = new Chart(ctx, {
                type: 'bar',
                data: chartData,
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    scales: {
                        y: {
                            beginAtZero: true,
                            max: 120,
                            title: {
                                display: true,
                                text: 'Utilization %'
                            }
                        }
                    },
                    plugins: {
                        tooltip: {
                            callbacks: {
                                label: function (context) {
                                    const index = context.dataIndex;
                                    const metric = metrics[index];
                                    const datasetLabel = context.dataset.label;
                                    let actualValue, rawValue;
                                    switch (datasetLabel) {
                                        case 'Memory':
                                            actualValue = metric.memoryUtilization;
                                            rawValue = metric.rawMemoryUtilization;
                                            break;
                                        case 'Local Bandwidth':
                                            actualValue = metric.localBandwidthUtilization;
                                            rawValue = metric.rawLocalBandwidthUtilization;
                                            break;
                                        case 'Network Bandwidth':
                                            actualValue = metric.networkBandwidthUtilization;
                                            rawValue = metric.rawNetworkBandwidthUtilization;
                                            break;
                                        case 'Compute':
                                            actualValue = metric.computeUtilization;
                                            rawValue = metric.rawComputeUtilization;
                                            break;
                                    }
                                    if (rawValue !== actualValue) {
                                        return `${datasetLabel}: ${actualValue.toFixed(1)}% (theoretical max: ${rawValue.toFixed(1)}%)`;
                                    }
                                    return `${datasetLabel}: ${actualValue.toFixed(1)}%`;
                                }
                            }
                        }
                    }
                }
            });
        }
        function addDevice() {
            const lastDevice = devices[devices.length - 1];
            const newDevice = {
                id: devices.length + 1,
                name: `Device ${devices.length + 1}`,
                template: lastDevice.template,
                ...DEVICE_TEMPLATES[lastDevice.template === 'Custom' ? 'Custom' : lastDevice.template],
                memoryGB: lastDevice.memoryGB,
                localBandwidthGBps: lastDevice.localBandwidthGBps,
                networkBandwidthGBps: lastDevice.networkBandwidthGBps,
                computeTFlops: lastDevice.computeTFlops
            };
            devices.push(newDevice);
            updateDeviceDisplay();
            updateSystemAnalysis();
        }
        function removeDevice(id) {
            if (devices.length > 1) {
                devices = devices.filter(d => d.id !== id);
                updateDeviceDisplay();
                updateSystemAnalysis();
            }
        }
	    
	function updateDevice(id, field, value) {
	    const device = devices.find(d => d.id === id);
	    if (!device) return;

	    // Fields that don't trigger "Custom" mode (connection settings, not device specs)
	    const connectionFields = ['networkBandwidthGBps', 'interconnectType', 'overflowTarget', 'ddr5BandwidthGBps', 'name', 'template'];
	    const isConnectionField = connectionFields.includes(field);

	    // Check if the value is actually different before marking as edited
	    let valueChanged = false;

	    if (field === 'computeTFlops') {
	        const currentQuant = document.getElementById('quantizationType').value;
	        valueChanged = device.computeTFlops[currentQuant] !== value;
	    } else if (!isConnectionField) {
	        valueChanged = device[field] !== value;
	    }

	    devices = devices.map(d => {
	        if (d.id !== id) return d;

	        let updatedDevice = { ...d };

	        if (field === 'template') {
	            if (value !== 'Custom') {
	                // Update entire device with template
	                updatedDevice = {
	                    ...updatedDevice,
	                    ...DEVICE_TEMPLATES[value],
	                    template: value, // Keep the template name
	                    name: DEVICE_TEMPLATES[value].name, // Use template name
	                    isEdited: false
	                };
	            } else {
	                updatedDevice.template = 'Custom';
	                updatedDevice.name = 'Custom Device';
	                updatedDevice.isEdited = true;
	            }
	        } else if (field === 'computeTFlops') {
	            // For custom devices, use the entered value as base and scale for other precisions
	            const currentQuant = document.getElementById('quantizationType').value;
	            const baseValue = value / {
	                'float32': 1,
	                'bfloat16': 2,
	                'float16': 2,
	                'int8': 4,
	                'q4': 6
	            }[currentQuant];

	            updatedDevice.computeTFlops = {
	                'float32': baseValue,
	                'bfloat16': baseValue * 2,
	                'float16': baseValue * 2,
	                'int8': baseValue * 4,
	                'q4': baseValue * 6
	            };

	            // Mark as edited if value changed and not already custom
	            if (valueChanged && !updatedDevice.isEdited && updatedDevice.template !== 'Custom') {
	                updatedDevice.isEdited = true;
	                // Add "- Custom" to the name if not already there
	                if (!updatedDevice.name.includes('- Custom')) {
	                    updatedDevice.name = `${updatedDevice.name} - Custom`;
	                }
	            }
	        } else {
	            updatedDevice[field] = value;

	            // Only mark as edited for hardware spec changes (not connection fields)
	            if (valueChanged && !isConnectionField && !updatedDevice.isEdited && updatedDevice.template !== 'Custom') {
	                updatedDevice.isEdited = true;
	                // Add "- Custom" to the name
	                if (!updatedDevice.name.includes('- Custom')) {
	                    updatedDevice.name = `${updatedDevice.name} - Custom`;
	                }
	            }
	        }

	        return updatedDevice;
	    });

	    updateDeviceDisplay();
	    updateSystemAnalysis();
	    saveDevices();
	}
	
	function updateModelConfigFromPreset() {
		const modelPresetId = document.getElementById('modelPreset').value;
		if (modelPresetId && MODEL_PRESETS[modelPresetId]) {
			const preset = MODEL_PRESETS[modelPresetId];
			for (const key in preset) {
				if (document.getElementById(key)) {
					document.getElementById(key).value = preset[key];
				}
			}
		} else if (modelPresetId === "") {
			// Reset to empty values if no preset selected
			const defaultKeys = ['totalParamsB', 'hiddenSize', 'numLayers', 'numHeads'];
			defaultKeys.forEach(key => {
				if (document.getElementById(key)) {
					document.getElementById(key).value = '';
				}
			});
		}
		updateSystemAnalysis();
	}


        document.getElementById('modelPreset').addEventListener('change', updateModelConfigFromPreset);
        document.getElementById('quantizationType').addEventListener('change', () => {
			// Update display for custom devices
			updateDeviceDisplay();
			updateSystemAnalysis();
		});
        document.querySelectorAll('#modelConfig input, #modelConfig select').forEach(input => {
            if (input.id !== 'modelPreset' && input.id !== 'quantizationType' && input.id !== 'dtype') {
                input.addEventListener('change', updateSystemAnalysis);
            }
        });
		updateModelConfigFromPreset();
        updateDeviceDisplay();
        updateSystemAnalysis();
		
		        const rawData = `Model	Quantization	Framework	Hardware	Batch Size	Sequence Length	Token Rate (Batch)	Token Rate (Single)	Source
Llama 3.1 8b	FP16		Groq				100 t/s	https://www.vellum.ai/llm-leaderboard
Llama 3.1 70b	FP16		Groq				40 t/s	https://www.vellum.ai/llm-leaderboard
Llama 3.1 405b	FP16		Groq				10 t/s	https://www.vellum.ai/llm-leaderboard
Claude 3 Opus					4096		25 t/s	https://www.vellum.ai/llm-leaderboard
GPT-4					8192		125 t/s	https://www.vellum.ai/llm-leaderboard
Claude 3.5 Sonnet						170.4 t/s		https://yourgpt.ai/tools/llm-comparison-and-leaderboard
llama3-8b			SambaNova's RDU (Reconfigurable Dataflow Unit) system with 8-chips		4096	430 t/s		https://blog.lancedb.com/tokens-per-second-is-not-all-you-need/
Mistral 7B	FP16	vLLM	A10G	1			30.9 t/s	https://lightning.ai/lightning-ai/studios/optimized-llm-inference-api-for-mistral-7b-using-vllm
Mistral 7B	FP16	vLLM	4 x A10G	1			64.5 t/s	https://lightning.ai/lightning-ai/studios/optimized-llm-inference-api-for-mistral-7b-using-vllm
Mistral 7B	AWQ 4-bit	vLLM	A10G	1			86.1 t/s	https://lightning.ai/lightning-ai/studios/optimized-llm-inference-api-for-mistral-7b-using-vllm
Mistral 7B	FP16			1	80x100		170 t/s	https://www.baseten.co/blog/benchmarking-fast-mistral-7b-inference/
Mistral-8x7B	Q8_0		RTX 4090				6.55 t/s	https://www.reddit.com/r/LocalLLaMA/comments/18jslmf/tokens_per_second_mistral_8x7b_performance/
Mistral-8x7B	Q5_K_M		RTX 4090				13.16 t/s	https://www.reddit.com/r/LocalLLaMA/comments/18jslmf/tokens_per_second_mistral_8x7b_performance/
Mistral-8x7B	Q4_K_M		RTX 4090				23.06 t/s	https://www.reddit.com/r/LocalLLaMA/comments/18jslmf/tokens_per_second_mistral_8x7b_performance/
Mistral-8x7B	Q8_0		AMD 7950X3D				3.95 t/s	https://www.reddit.com/r/LocalLLaMA/comments/18jslmf/tokens_per_second_mistral_8x7b_performance/
Mistral-8x7B	Q4_K_M		2 x RTX 3090				48.26 t/s	https://www.reddit.com/r/LocalLLaMA/comments/18jslmf/tokens_per_second_mistral_8x7b_performance/
Mistral 7B	Q4		Raspberry Pi 5				2 t/s	https://dmatora.github.io/LLM-inference-speed-benchmarks/
Mistral 7B	Q4		i7-7700HQ (CPU)				3 t/s	https://dmatora.github.io/LLM-inference-speed-benchmarks/
Mistral 7B	Q4		Apple M1 (CPU)				12 t/s	https://dmatora.github.io/LLM-inference-speed-benchmarks/
Mistral 7B	Q4		RTX 4060 Ti				44 t/s	https://dmatora.github.io/LLM-inference-speed-benchmarks/
Mistral 7B	Q4		Tesla P40				45 t/s	https://dmatora.github.io/LLM-inference-speed-benchmarks/
Mistral 7B	Q4		Apple M1 Max (CPU)				58 t/s	https://dmatora.github.io/LLM-inference-speed-benchmarks/
Mistral 7B	Q4		Nvidia RTX 3060				59 t/s	https://dmatora.github.io/LLM-inference-speed-benchmarks/
Mistral 7B	Q4		Apple M1 Ultra (CPU)				70 t/s	https://dmatora.github.io/LLM-inference-speed-benchmarks/
Mistral 7B	Q4		Nvidia RTX 4070				70 t/s	https://dmatora.github.io/LLM-inference-speed-benchmarks/
Mistral 7B	Q4		Nvidia RTX 3090				120 t/s	https://dmatora.github.io/LLM-inference-speed-benchmarks/
Mistral 7B	Q4		Nvidia RTX 4090				140 t/s	https://dmatora.github.io/LLM-inference-speed-benchmarks/
Mistral SM 3.1 24B	8bit		M3 Ultra				26.23 t/s	https://x.com/ivanfioravanti/status/1902375006228902041
Mistral SM 3.1 24B	8bit		M2 Ultra				27.16 t/s	https://x.com/ivanfioravanti/status/1902375006228902041
Mistral SM 3.1 24B	8bit		M4 Max				19.24 t/s	https://x.com/ivanfioravanti/status/1902375006228902041
Llama 2 70B	FP16		2 x Tesla P40				3 t/s	https://dmatora.github.io/LLM-inference-speed-benchmarks/
Llama 2 70B	Q4		Apple M1 Max (CPU)				6 t/s	https://dmatora.github.io/LLM-inference-speed-benchmarks/
CodeLlama 70B	Q5_K_M	llama.cpp	Apple M2 Ultra (CPU)				8.72 t/s	https://obrienlabs.medium.com/running-the-70b-llama-2-llm-locally-on-metal-via-llama-cpp-on-mac-studio-m2-ultra-32b3179e9cbe
Llama 2 7B	FP16	vLLM	NVIDIA H100 SXM	1	2048	1000 t/s		https://blog.ori.co/benchmarking-llama-3.1-8b-instruct-on-nvidia-h100-and-a100-chips-with-the-vllm-inferencing-engine
Llama 2 7B	FP16	vLLM	NVIDIA A100	1	2048	500 t/s		https://blog.ori.co/benchmarking-llama-3.1-8b-instruct-on-nvidia-h100-and-a100-chips-with-the-vllm-inferencing-engine
GPT-J 6B	FP8	TensorRT-LLM	NVIDIA H100	64	128	10,907 t/s		https://nvidia.github.io/TensorRT-LLM/blogs/H100vsA100.html
GPT-J 6B	FP16	TensorRT-LLM	NVIDIA A100	64	128	3,679 t/s		https://nvidia.github.io/TensorRT-LLM/blogs/H100vsA100.html
DeepSeek V3	INT8	Exo	2 x Mac M3 Ultra 512GB				11 t/s	https://x.com/alexocheema/status/1899735281781411907
DeepSeek V3	FP16		NVIDIA H800 x 2048					https://www.infoq.com/news/2025/01/deepseek-v3-llm/
RTX 5090 Benchmarks
Llama 3.1 8B	Q4	llama.cpp	RTX 5090				213 t/s	https://localllm.in/blog/best-gpus-llm-inference-2025
Llama 3.1 8B	FP16	llama.cpp	RTX 5090	1	128		213 t/s	https://www.phoronix.com/review/nvidia-rtx5090-llama-cpp/2
Llama 3 8B	Q4	llama.cpp	RTX 5090	1	128		213 t/s	https://www.runpod.io/blog/rtx-5090-llm-benchmarks
DeepSeek R1 32B	Q4	Ollama	RTX 5090				61 t/s	https://www.databasemart.com/blog/ollama-gpu-benchmark-rtx5090
Qwen2-0.5B	FP16	vLLM	RTX 5090	1024		65000 t/s		https://www.runpod.io/blog/rtx-5090-llm-benchmarks
Phi-3 Mini 4K	FP16	vLLM	RTX 5090	1024		6400 t/s		https://www.runpod.io/blog/rtx-5090-llm-benchmarks
Llama 3.3 70B	Q4	Ollama	2 x RTX 5090				27 t/s	https://www.databasemart.com/blog/ollama-gpu-benchmark-rtx5090-2
Llama 3.3 70B	Q4	Ollama	RTX 5090 + DDR5				~15 t/s	https://localllm.in/blog/best-gpus-llm-inference-2025
RTX 5090 vs RTX 4090 Comparison
Llama 3.1 8B	Q4	llama.cpp	RTX 4090				127 t/s	https://localllm.in/blog/best-gpus-llm-inference-2025
Llama 3.1 8B	Q4	llama.cpp	RTX 4090				140 t/s	https://dmatora.github.io/LLM-inference-speed-benchmarks/
Llama 3 8B	Q4	TensorRT-LLM	RTX 4090				150 t/s	https://developer.nvidia.com/blog/accelerating-llms-with-llama-cpp-on-nvidia-rtx-systems/
Llama 3 70B	IQ2_XS	Ollama	RTX 4090				60 t/s	https://www.ubaada.com/post/1b867b28
Llama 3 70B	Q4	llama.cpp	2 x RTX 4090				~35 t/s	https://www.arsturn.com/blog/multi-gpu-showdown-benchmarking-vllm-llama-cpp-ollama-for-maximum-performance
RTX 40-Series Mid-Range
Mistral 7B	Q4	llama.cpp	RTX 4080 Super				72 t/s	https://www.pugetsystems.com/labs/articles/llm-inference-consumer-gpu-performance/
Llama 3 8B	Q4	llama.cpp	RTX 4080 Super				~85 t/s	https://www.hardware-corner.net/gpu-ranking-local-llm/
Llama 3 8B	Q4	llama.cpp	RTX 4080				106 t/s	https://www.hardware-corner.net/gpu-ranking-local-llm/
Llama 3 8B	Q4	llama.cpp	RTX 4070 Ti Super				~80 t/s	https://www.hardware-corner.net/gpu-ranking-local-llm/
Llama 3 8B	Q4	llama.cpp	RTX 4070 Ti				82 t/s	https://www.hardware-corner.net/gpu-ranking-local-llm/
DeepSeek R1 Benchmarks
DeepSeek R1 671B	FP8	TensorRT-LLM	8 x Blackwell		30000 t/s		https://developer.nvidia.com/blog/nvidia-blackwell-delivers-world-record-deepseek-r1-inference-performance/
DeepSeek R1 671B	FP8	TensorRT-LLM	8 x Blackwell				250 t/s	https://developer.nvidia.com/blog/nvidia-blackwell-delivers-world-record-deepseek-r1-inference-performance/
DeepSeek R1 671B	FP8	SambaNova	SambaNova Cloud				198 t/s	https://sambanova.ai/press/fastest-deepseek-r1-671b-with-highest-efficiency
DeepSeek R1 671B	FP8	vLLM	16 x NVIDIA H100	1	2048		~54x slower than 8B	https://techcommunity.microsoft.com/blog/azurehighperformancecomputingblog/performance-analysis-of-deepseek-r1-ai-inference-using-vllm-on-nd-h100-v5/4449351
DeepSeek R1 671B	1.58bit	llama.cpp	2 x H100 80GB				14 t/s	https://unsloth.ai/blog/deepseekr1-dynamic
DeepSeek R1 671B	1.58bit	llama.cpp	Consumer GPU + CPU				9 t/s	https://github.com/ggml-org/llama.cpp/issues/11474
DeepSeek R1 Distill 7B	Q4	Ollama	M2 Pro MacBook				19.9 t/s	https://dev.to/ocodista/deepseek-r1-7bs-performance-on-a-developers-macbook-3mg2
DeepSeek R1 Distill 7B	Q4		Raspberry Pi 5				200 t/s	https://www.nextbigfuture.com/2025/01/open-source-deepseek-r1-runs-at-200-tokens-per-second-on-raspberry-pi.html
Apple Silicon Benchmarks
Llama 3 8B	Q4_K_M	MLX	M4 Max				96-100 t/s	https://github.com/ggml-org/llama.cpp/discussions/4167
Llama 3 8B	Q4_K_M	MLX	M3 Ultra				76 t/s	https://github.com/ggml-org/llama.cpp/discussions/4167
Llama 3.1 70B	Q4	MLX	M4 Max 128GB				30-45 t/s	https://introl.com/blog/local-llm-hardware-pricing-guide-2025
Llama 3.1 70B	Q4	MLX	M3 Ultra				~35 t/s	https://scalastic.io/en/apple-silicon-vs-nvidia-cuda-ai-2025/
Mistral 7B	Q4	llama.cpp	M1 Ultra				70 t/s	https://dmatora.github.io/LLM-inference-speed-benchmarks/
Mistral 7B	Q4	llama.cpp	M1 Max				58 t/s	https://dmatora.github.io/LLM-inference-speed-benchmarks/
Mistral 7B	Q4	llama.cpp	M1				12 t/s	https://dmatora.github.io/LLM-inference-speed-benchmarks/
CodeLlama 34B	Q4	MLX	M4 Max 64GB				~25 t/s	https://forums.macrumors.com/threads/m4-max-studio-128gb-llm-testing.2453816/
AMD GPU Benchmarks
Llama 3 8B	Q4	ROCm	RX 7900 XTX				78 t/s	https://github.com/ggml-org/llama.cpp/discussions/10879
Llama 2 7B	Q4_0	llama.cpp	RX 7900 XTX + RX 7900 XT				107 t/s	https://github.com/ggml-org/llama.cpp/discussions/10879
Llama 2 7B	Q4_0	llama.cpp	RX 7900 XTX	1	3968	2408 t/s		https://github.com/ggml-org/llama.cpp/discussions/10879
Llama 3 70B	Q4	llama.cpp	2 x RX 7900 XTX				17 t/s	https://huggingface.co/abacusai/Smaug-Llama-3-70B-Instruct/discussions/8
Llama 3 70B	Q4	llama.cpp	2 x RX 7900 XTX (no offload)				122 t/s	https://www.techreviewer.com/tech-specs/amd-rx-7900-xtx-gpu-for-llms/
NVIDIA H100 Enterprise Benchmarks
Llama 3.1 8B	FP16	vLLM	H100 SXM	1	2048	1000 t/s		https://www.ori.co/blog/benchmarking-llama-3.1-8b-instruct-on-nvidia-h100-and-a100-chips-with-the-vllm-inferencing-engine
Llama 3.1 8B	FP8	vLLM	H100				~12500 t/s	https://research.aimultiple.com/inference-engines/
Llama 3.3 70B	FP16	Ollama	H100 80GB				20 t/s	https://www.databasemart.com/blog/ollama-gpu-benchmark-h100
Llama 3.3 70B	Q4_K_M	llama.cpp	H100 PCIe 80GB				25 t/s	https://github.com/XiongjieDai/GPU-Benchmarks-on-LLM-Inference
Llama 2 70B	FP16	vLLM	4 x H100	1	2048		5+ t/s	https://developer.nvidia.com/blog/achieving-top-inference-performance-with-the-nvidia-h100-tensor-core-gpu-and-nvidia-tensorrt-llm/
Llama 2 70B	FP16	vLLM	8 x H100 DGX		5+ infer/s		https://developer.nvidia.com/blog/achieving-top-inference-performance-with-the-nvidia-h100-tensor-core-gpu-and-nvidia-tensorrt-llm/
NVIDIA A100 Benchmarks
Llama 3.1 8B	FP16	vLLM	A100	1	2048	500 t/s		https://www.ori.co/blog/benchmarking-llama-3.1-8b-instruct-on-nvidia-h100-and-a100-chips-with-the-vllm-inferencing-engine
Llama 3.3 70B	Q4	Ollama	2 x A100 40GB				~18 t/s	https://www.databasemart.com/blog/ollama-gpu-benchmark-rtx5090-2
Mistral Large Benchmarks
Mistral Large 123B	Q5_K_M	llama.cpp	AMD EPYC 9554 (CPU)				3.75 t/s	https://ahelpme.com/ai/llamacpp-ai/llama-bench-the-mistral-large-123b-and-amd-epyc-9554-cpu/
Mistral Large 123B	FP16	Cerebras	Cerebras WSE				1100 t/s	https://www.cerebras.ai/blog/mistral-le-chat
Mistral Large 123B	INT4	TensorRT-LLM	RTX 4090				~200 t/s	https://www.johal.in/mistral-large-2-123b-frontier-model-2025/
Qwen 2.5 Benchmarks
Qwen 2.5 72B	Q4	vLLM	4 x A6000		449 t/s		https://www.databasemart.com/blog/vllm-gpu-benchmark-a6000-4
Qwen 2.5 72B	Q4	Ollama	2 x RTX 5090				~25 t/s	https://www.databasemart.com/blog/ollama-gpu-benchmark-rtx5090-2
Qwen 2.5 Coder 32B	FP16		H100				25.5 t/s	https://medium.com/@wltsankalpa/benchmarking-qwen-models-across-nvidia-gpus-t4-l4-h100-architectures-finding-your-sweet-spot-a59a0adf9043
Framework Comparisons
Llama 3 8B	FP16	TensorRT-LLM	RTX 4090				~255 t/s	https://www.jan.ai/post/benchmarking-nvidia-tensorrt-llm
Llama 3 8B	FP16	llama.cpp	RTX 4090				~150 t/s	https://www.jan.ai/post/benchmarking-nvidia-tensorrt-llm
Llama 3 8B	FP16	vLLM	RTX 4090				~130 t/s	https://www.jan.ai/post/benchmarking-nvidia-tensorrt-llm
Groq LPU Benchmarks
Llama 3 70B	FP16	Groq LPU	Groq Rack (576 chips)				284 t/s	https://groq.com/blog/groq-lpu-inference-engine-crushes-first-public-llm-benchmark
Llama 3.3 70B	FP16	Groq LPU	Groq Rack				276 t/s	https://groq.com/blog/new-ai-inference-speed-benchmark-for-llama-3-3-70b-powered-by-groq
Llama 3.3 70B	FP16	Groq LPU + SpecDec	Groq Rack				1660 t/s	https://groq.com/blog/groq-first-generation-14nm-chip-just-got-a-6x-speed-boost-introducing-llama-3-1-70b-speculative-decoding-on-groqcloud
Mixtral 8x7B	FP16	Groq LPU	Groq Rack				~500 t/s	https://groq.com/blog/groq-lpu-inference-engine-crushes-first-public-llm-benchmark
AMD MI300X Benchmarks
Llama 3.1 70B	FP16	vLLM	8 x MI300X				38.7 t/s	https://rocm.docs.amd.com/en/latest/how-to/performance-validation/mi300x/vllm-benchmark.html
Llama 3.1 70B	FP8	vLLM	8 x MI300X				144 t/s	https://blogs.oracle.com/cloud-infrastructure/llm-performance-results-amd-instinct-mi300x-gpus
Llama 3.1 70B	FP8	vLLM + SpecDec	8 x MI300X				~300 t/s	https://rocm.blogs.amd.com/artificial-intelligence/spec_decode_mi300x/README.html
DeepSeek R1 671B	FP8	vLLM	8 x MI300X				~15 t/s	https://rocm.blogs.amd.com/software-tools-optimization/llama4-performance-b/README.html
Qwen 2.5 72B	FP16	vLLM	8 x MI300X				~25 t/s	https://rocm.docs.amd.com/en/latest/how-to/performance-validation/mi300x/vllm-benchmark.html
NVIDIA DGX Spark (GB10) Benchmarks
Llama 3.1 8B	Q4	llama.cpp	DGX Spark (GB10)				~50 t/s	https://www.nvidia.com/en-us/products/workstations/dgx-spark/
DeepSeek R1 32B	Q4	Ollama	DGX Spark (GB10)				~18 t/s	https://robert-mcdermott.medium.com/the-nvidia-dgx-spark-0e2ca7833c2c
Llama 3.1 70B	Q4	llama.cpp	DGX Spark (GB10)				~12 t/s	https://intuitionlabs.ai/articles/nvidia-dgx-spark-review
AMD Strix Halo Benchmarks
Llama 3.1 8B	Q4	llama.cpp	Ryzen AI Max+ 395				~65 t/s	https://wccftech.com/amd-ryzen-ai-max-395-strix-halo-mini-pc-tested-powerful-apu-up-to-140w-power-128-gb-variable-memory-igpu/
Llama 3.1 70B	Q4	llama.cpp	Ryzen AI Max+ 395 (128GB)				~8 t/s	https://www.amd.com/en/blogs/2025/amd-ryzen-ai-max-395-processor-breakthrough-ai-.html
DeepSeek R1 32B	Q4	llama.cpp	Ryzen AI Max+ 395				~20 t/s	https://www.theregister.com/2025/12/25/amd_strix_halo_nvidia_spark/
Intel Arc B580 Benchmarks
Llama 3 8B	Q4	IPEX-LLM	Arc B580				20 t/s	https://www.techreviewer.com/tech-specs/intel-b580-gpu-for-llms/
Llama 2 7B	INT4	IPEX-LLM	Arc B580				~18 t/s	https://github.com/intel/ipex-llm
Mistral 7B	Q4	IPEX-LLM	Arc B580				~15 t/s	https://www.propelrc.com/intel-arc-b580-and-a770-for-local-ai-software/
Gemma 3 Benchmarks
Gemma 3 27B	Q4	Ollama	RTX 5090				50 t/s	https://github.com/ollama/ollama/issues/9701
Gemma 3 27B	Q4	Ollama	RTX 4090				~35 t/s	https://huggingface.co/google/gemma-3-27b-it/discussions/39
Gemma 3 12B	Q4	Ollama	RTX 5090				~80 t/s	https://github.com/ollama/ollama/issues/9701
Gemma 2 27B	Q4	Ollama	RTX 5090				76 t/s	https://github.com/ollama/ollama/issues/9701
Gemma 2 9B	Q4	Ollama	RTX 5090				150 t/s	https://github.com/ollama/ollama/issues/9701
Phi-4 Benchmarks
Phi-4 14B	Q4	llama.cpp	RTX 4090				~55 t/s	https://ollama.com/library/phi4
Phi-4 Mini 3.8B	Q4	llama.cpp	RTX 4090				~120 t/s	https://huggingface.co/microsoft/phi-4
Phi-4 Mini 3.8B	Q4	llama.cpp	M4 Max				~80 t/s	https://huggingface.co/microsoft/Phi-4-mini-instruct
Mixtral 8x22B Benchmarks
Mixtral 8x22B	FP16	vLLM	4 x H100				46.5 t/s	https://artificialanalysis.ai/models/mistral-8x22b-instruct
Mixtral 8x22B	Q4	llama.cpp	2 x RTX 4090				~12 t/s	https://huggingface.co/mistralai/Mixtral-8x22B-v0.1
Mixtral 8x22B	Q4	Ollama	RTX 5090 + DDR5				~8 t/s	https://localllm.in/blog/best-gpus-llm-inference-2025
Cerebras / Specialized Hardware
Mistral Large 123B	FP16	Cerebras	Cerebras CS-3 (WSE-3)				1100 t/s	https://www.cerebras.ai/blog/mistral-le-chat
Llama 3.1 70B	FP16	SambaNova	SambaNova Cloud				~200 t/s	https://sambanova.ai/press/fastest-deepseek-r1-671b-with-highest-efficiency
Speculative Decoding Comparisons
Llama 3.1 70B	FP16	vLLM + SpecDec	H100				~80 t/s	https://developer.nvidia.com/blog/an-introduction-to-speculative-decoding-for-reducing-latency-in-ai-inference/
Llama 3.1 70B	FP16	vLLM	H100				~35 t/s	https://developer.nvidia.com/blog/an-introduction-to-speculative-decoding-for-reducing-latency-in-ai-inference/
Llama 3.1 8B	FP16	AMD PARD	AMD EPYC + MI300X				380 t/s	https://www.amd.com/en/developer/resources/technical-articles/2025/speculative-llm-inference-on-the-5th-gen-amd-epyc-processors-wit.html`;

        const data = rawData.split('\n')
            .filter(line => line.trim())
            .map(line => {
                const [model, quantization, framework, hardware, batchSize, seqLength, tokenRateBatch, tokenRateSingle, source] = line.split('\t');
                return {
                    model, quantization, framework, hardware, batchSize, 
                    seqLength, tokenRateBatch, tokenRateSingle, source
                };
            });

        // Function to render the table
        const renderTable = (data) => {
            const tbody = document.querySelector('#llmTable tbody');
            tbody.innerHTML = '';
            
            data.forEach(row => {
                const tr = document.createElement('tr');
                Object.values(row).forEach(value => {
                    const td = document.createElement('td');
                    td.textContent = value || '';
                    if (value && value.startsWith('http')) {
                        const a = document.createElement('a');
                        a.href = value;
                        a.textContent = 'Link';
                        a.target = '_blank';
                        td.textContent = '';
                        td.appendChild(a);
                    }
                    tr.appendChild(td);
                });
                tbody.appendChild(tr);
            });
        };

        // Sorting function
        const sortTable = (column, direction = 'asc') => {
            const sortedData = [...data].sort((a, b) => {
                let aVal = a[column] || '';
                let bVal = b[column] || '';
                
                // Extract numerical values from strings like "100 t/s"
                if (aVal.includes('t/s')) {
                    aVal = parseFloat(aVal.replace(' t/s', '').replace(',', ''));
                }
                if (bVal.includes('t/s')) {
                    bVal = parseFloat(bVal.replace(' t/s', '').replace(',', ''));
                }
                
                // Convert to numbers if possible
                if (!isNaN(aVal) && !isNaN(bVal)) {
                    aVal = parseFloat(aVal);
                    bVal = parseFloat(bVal);
                }
                
                if (direction === 'asc') {
                    return aVal > bVal ? 1 : -1;
                } else {
                    return aVal < bVal ? 1 : -1;
                }
            });
            
            renderTable(sortedData);
        };

	function handleDeviceTemplateChange(id, selectElement) {
	    const value = selectElement.value;
	    
	    // Check if this is a library device
	    if (value.startsWith('lib_')) {
	        const libraryId = value.replace('lib_', '');
	        
	        try {
	            const savedLibrary = localStorage.getItem('deviceLibrary');
	            if (savedLibrary) {
	                const deviceLibrary = JSON.parse(savedLibrary);
	                const libraryDevice = deviceLibrary.find(d => d.id.toString() === libraryId);
	                
	                if (libraryDevice) {
	                    // Apply the library device properties
	                    devices = devices.map(d => {
	                        if (d.id !== id) return d;
	                        
	                        // Create a new device with library device properties
	                        return {
	                            ...JSON.parse(JSON.stringify(libraryDevice)),
	                            id: d.id, // Keep the original ID
	                            isEdited: false
	                        };
	                    });
	                    
	                    updateDeviceDisplay();
	                    updateSystemAnalysis();
	                    saveDevices();
	                }
	            }
	        } catch (e) {
	            console.error('Error loading device from library:', e);
	        }
	    } else {
	        // Normal template handling
	        // Get the template name first
	        const templateName = value !== 'Custom' ? DEVICE_TEMPLATES[value].name : 'Custom Device';
	        
	        // Update device with new template
	        devices = devices.map(d => {
	            if (d.id !== id) return d;
	            
	            const baseDevice = { ...d };
	            
	            if (value !== 'Custom') {
	                // Update device with template and its name
	                return { 
	                    ...baseDevice, 
	                    ...DEVICE_TEMPLATES[value],
	                    id: d.id,
	                    template: value,
	                    name: templateName, // Use template name
	                    isEdited: false
	                };
	            } else {
	                return {
	                    ...baseDevice,
	                    template: 'Custom',
	                    name: 'Custom Device',
	                    isEdited: false
	                };
	            }
	        });
	        
	        updateDeviceDisplay();
	        updateSystemAnalysis();
	        saveDevices();
	    }
	}
        // Filtering function
        const filterTable = () => {
            const modelFilter = document.getElementById('modelFilter').value.toLowerCase();
            const hardwareFilter = document.getElementById('hardwareFilter').value.toLowerCase();
            const quantizationFilter = document.getElementById('quantizationFilter').value.toLowerCase();

            const filteredData = data.filter(row => {
                return (!modelFilter || row.model.toLowerCase().includes(modelFilter)) &&
                       (!hardwareFilter || (row.hardware && row.hardware.toLowerCase().includes(hardwareFilter))) &&
                       (!quantizationFilter || (row.quantization && row.quantization.toLowerCase().includes(quantizationFilter)));
            });

            renderTable(filteredData);
        };

        // Event listeners
        document.querySelectorAll('th[data-sort]').forEach(th => {
            th.addEventListener('click', () => {
                const column = th.dataset.sort;
                const currentDirection = th.classList.contains('asc') ? 'desc' : 'asc';
                
                // Remove all sort indicators
                document.querySelectorAll('th').forEach(el => {
                    el.classList.remove('asc', 'desc');
                });
                
                th.classList.add(currentDirection);
                sortTable(column, currentDirection);
            });
        });

        document.getElementById('modelFilter').addEventListener('input', filterTable);
        document.getElementById('hardwareFilter').addEventListener('input', filterTable);
        document.getElementById('quantizationFilter').addEventListener('input', filterTable);

        // Collapsible section toggle function
        function toggleCollapsible(header) {
            header.classList.toggle('collapsed');
            const content = header.nextElementSibling;
            if (content && content.classList.contains('collapsible-content')) {
                content.classList.toggle('collapsed');
            }
        }
        // Make toggleCollapsible globally available
        window.toggleCollapsible = toggleCollapsible;

        // Update model summary in header
        function updateModelSummary() {
            const modelPreset = document.getElementById('modelPreset').value;
            const quant = document.getElementById('quantizationType').value;
            const totalParams = document.getElementById('totalParamsB').value;

            let modelName = 'Custom';
            if (modelPreset && MODEL_PRESETS[modelPreset]) {
                // Format preset name
                modelName = modelPreset.split('_').map(word =>
                    word.charAt(0).toUpperCase() + word.slice(1)
                ).join(' ');
            } else if (totalParams) {
                modelName = `${totalParams}B`;
            }

            const summary = document.getElementById('modelSummary');
            if (summary) {
                summary.textContent = `${modelName} @ ${quant.toUpperCase()}`;
            }
        }
        // Make updateModelSummary globally available
        window.updateModelSummary = updateModelSummary;

	document.addEventListener('DOMContentLoaded', function() {
	    const modelPresetSelect = document.getElementById('modelPreset');
	    modelPresetSelect.innerHTML = '<option value="">Custom</option>';

	    // Organize models into categories for better UX
	    const modelCategories = {
	        'Llama': ['llama3_8b', 'llama3_70b', 'llama3.3_70b', 'llama3.2_3b', 'llama3.2_1b', 'llama3.2_90b_vision', 'llama3.2_11b_vision'],
	        'DeepSeek': ['deepseek_r1', 'deepseek_v3_671b', 'deepseek_r1_distill_70b', 'deepseek_r1_distill_32b', 'deepseek_r1_distill_14b', 'deepseek_r1_distill_8b', 'deepseek_r1_distill_1.5b'],
	        'Qwen': ['qwen2.5_72b', 'qwen2.5_32b', 'qwen2.5_14b', 'qwen2.5_7b', 'qwen2.5_3b', 'qwen2.5_coder_32b', 'qwen3_235b_moe'],
	        'Gemma': ['gemma3_27b', 'gemma3_12b', 'gemma3_4b', 'gemma3_1b', 'gemma2_27b', 'gemma2_9b', 'gemma2_2b', 'gemma_7b', 'gemma_2b'],
	        'Phi': ['phi4_14b', 'phi4_mini_3.8b', 'phi4_multimodal_5.6b', 'phi3_medium_14b', 'phi3_small_7b', 'phi3_mini_3.8b', 'phi3_14b', 'phi3_3.8b'],
	        'Mistral': ['mistral_7b', 'mistral_large_2_123b', 'mistral_nemo_12b', 'mistral_small_24b', 'mistral_small_3.1_24b'],
	        'Mixtral (MoE)': ['mixtral_8x7b', 'mixtral_8x22b'],
	        'Other MoE': ['dbrx_132b', 'arctic_480b', 'jamba_1.5_large_398b', 'jamba_1.5_mini_52b', 'kimi_k2'],
	        'Falcon': ['falcon_40b', 'falcon2_180b', 'falcon2_11b'],
	        'Code Models': ['starcoder2_15b', 'starcoder2_7b', 'starcoder2_3b'],
	        'Vision Models': ['llava_1.5_7b', 'llava_1.5_13b'],
	        'Open Source': ['olmo2_13b', 'olmo2_7b', 'glm4_9b', 'command_r_plus_104b', 'yi_34b', 'yi_large_200b', 'bloom_176b', 'gpt_neox_20b', 'mpt_30b', 'vicuna_13b'],
	        'Image Generation': ['sdxl_base', 'flux1_dev', 'sd3_medium'],
	        'Video Generation': ['sora_est', 'runway_gen3'],
	        'Audio Models': ['whisper_large_v3', 'musicgen_large'],
	        'Test Models': ['large_model_400b', 'very_large_model_1kb']
	    };

	    const formatName = (key) => key.split('_').map(word =>
	        word.charAt(0).toUpperCase() + word.slice(1)
	    ).join(' ').replace(/([0-9]+[a-z])/i, match => match.toUpperCase());

	    // Track which models we've added to avoid duplicates
	    const addedModels = new Set();

	    // Add categorized models
	    Object.entries(modelCategories).forEach(([category, modelKeys]) => {
	        const optgroup = document.createElement('optgroup');
	        optgroup.label = category;

	        modelKeys.forEach(key => {
	            if (MODEL_PRESETS[key] && !addedModels.has(key)) {
	                addedModels.add(key);
	                const option = document.createElement('option');
	                option.value = key;
	                option.textContent = formatName(key);
	                if (key === 'llama3_8b') option.selected = true;
	                optgroup.appendChild(option);
	            }
	        });

	        if (optgroup.children.length > 0) {
	            modelPresetSelect.appendChild(optgroup);
	        }
	    });

	    // Add any remaining models not in categories
	    const uncategorized = Object.keys(MODEL_PRESETS).filter(key => !addedModels.has(key));
	    if (uncategorized.length > 0) {
	        const optgroup = document.createElement('optgroup');
	        optgroup.label = 'Other';
	        uncategorized.forEach(key => {
	            const option = document.createElement('option');
	            option.value = key;
	            option.textContent = formatName(key);
	            optgroup.appendChild(option);
	        });
	        modelPresetSelect.appendChild(optgroup);
	    }
	    
	    renderTable(data);

	    // Check for load devices button (may not exist)
	    const loadBtn = document.getElementById('loadDevicesBtn');
	    if (loadBtn) {
	        if(localStorage.getItem('savedDevices')) {
	            loadBtn.removeAttribute('disabled');
	        } else {
	            loadBtn.setAttribute('disabled', true);
	        }
	    }

	    // Initial topology draw
	    setTimeout(updateTopology, 100);
	});

        // Topology Visualization
        function updateTopology() {
            const canvas = document.getElementById('topologyCanvas');
            const container = document.getElementById('topologyContainer');
            if (!canvas || !container) return;

            // Set canvas size
            const rect = container.getBoundingClientRect();
            const dpr = window.devicePixelRatio || 1;
            const height = Math.max(300, devices.length * 100 + 60);
            canvas.width = rect.width * dpr;
            canvas.height = height * dpr;
            canvas.style.height = height + 'px';

            const ctx = canvas.getContext('2d');
            ctx.scale(dpr, dpr);

            const width = rect.width;

            // Clear with dark background
            ctx.fillStyle = '#0f0f1a';
            ctx.fillRect(0, 0, width, height);

            if (devices.length === 0) {
                ctx.fillStyle = '#666';
                ctx.font = '14px system-ui';
                ctx.textAlign = 'center';
                ctx.fillText('Add devices to see topology', width/2, height/2);
                return;
            }

            // Calculate node positions - arrange in a grid
            const nodeWidth = 200;
            const nodeHeight = 90;
            const cols = Math.min(3, devices.length);
            const rows = Math.ceil(devices.length / cols);
            const hGap = (width - cols * nodeWidth) / (cols + 1);
            const vGap = 30;

            const nodes = devices.map((device, i) => {
                const col = i % cols;
                const row = Math.floor(i / cols);
                return {
                    id: device.id,
                    x: hGap + col * (nodeWidth + hGap),
                    y: 30 + row * (nodeHeight + vGap),
                    width: nodeWidth,
                    height: nodeHeight,
                    device: device
                };
            });

            // Helper function to infer connection type between devices
            function inferConnectionType(deviceA, deviceB) {
                const nameA = (deviceA.template || deviceA.name || '').toLowerCase();
                const nameB = (deviceB.template || deviceB.name || '').toLowerCase();

                // Check for datacenter GPUs - likely NVLink
                const isNvlinkDevice = (name) => /h100|a100|b200|h200|dgx/.test(name);
                if (isNvlinkDevice(nameA) && isNvlinkDevice(nameB)) {
                    return { type: 'nvlink4', bandwidth: 450 };
                }

                // Check for Apple Silicon - UltraFusion or network
                const isApple = (name) => /mac|m[1-4]/.test(name);
                if (isApple(nameA) && isApple(nameB)) {
                    // If both are "Ultra" variants, could be UltraFusion
                    if (nameA.includes('ultra') && nameB.includes('ultra')) {
                        return { type: 'ultrafusion', bandwidth: 2500 };
                    }
                    return { type: '10gbe', bandwidth: 1.25 };
                }

                // Check for RTX 50xx (PCIe 5.0)
                const isPcie5 = (name) => /rtx 50|5090|5080|5070|5060/.test(name);
                if (isPcie5(nameA) || isPcie5(nameB)) {
                    return { type: 'pcie5_x16', bandwidth: 64 };
                }

                // Default to PCIe 4.0 x16
                return { type: 'pcie4_x16', bandwidth: 32 };
            }

            // Helper to get connection color
            function getConnectionColor(bandwidth) {
                if (bandwidth >= 300) return '#4ade80'; // Green for NVLink
                if (bandwidth >= 32) return '#60a5fa';  // Blue for PCIe 5.0
                if (bandwidth >= 8) return '#fbbf24';   // Yellow for PCIe 4.0
                return '#f87171';                       // Red for slow
            }

            // Track drawn connections to avoid duplicates
            const drawnConnections = new Set();

            // Draw explicit connections (overflowTarget set)
            devices.forEach((device, i) => {
                if (device.overflowTarget) {
                    const sourceNode = nodes[i];
                    let targetX, targetY, targetW = 80, targetH = 40;
                    let bandwidth = 115;
                    let label = 'DDR5';

                    if (device.overflowTarget === 'ddr5') {
                        // Draw to virtual DDR5 node to the right
                        targetX = sourceNode.x + sourceNode.width + 20;
                        targetY = sourceNode.y + (sourceNode.height - targetH) / 2;
                        bandwidth = device.ddr5BandwidthGBps || 115;
                        label = 'DDR5 RAM';

                        // Draw DDR5 box
                        ctx.fillStyle = 'rgba(248, 113, 113, 0.15)';
                        ctx.strokeStyle = '#f87171';
                        ctx.lineWidth = 2;
                        ctx.beginPath();
                        ctx.roundRect(targetX, targetY, targetW, targetH, 6);
                        ctx.fill();
                        ctx.stroke();

                        ctx.fillStyle = '#f87171';
                        ctx.font = 'bold 10px system-ui';
                        ctx.textAlign = 'center';
                        ctx.fillText(label, targetX + targetW/2, targetY + 16);
                        ctx.font = '9px system-ui';
                        ctx.fillText(`${bandwidth} GB/s`, targetX + targetW/2, targetY + 30);

                        // Mark as drawn
                        drawnConnections.add(`${i}-ddr5`);
                    } else {
                        const targetIdx = devices.findIndex(d => d.id === device.overflowTarget);
                        const targetNode = nodes.find(n => n.id === device.overflowTarget);
                        if (targetNode && targetIdx !== -1) {
                            targetX = targetNode.x;
                            targetY = targetNode.y;
                            targetW = targetNode.width;
                            targetH = targetNode.height;
                            // Use networkBandwidthGBps for connection bandwidth
                            bandwidth = device.networkBandwidthGBps || 32;
                            // Show interconnect type if available
                            if (device.interconnectType) {
                                label = device.interconnectType.replace(/_/g, ' ').toUpperCase();
                            } else {
                                label = `${bandwidth} GB/s`;
                            }

                            // Mark as drawn (both directions)
                            const connKey = [Math.min(i, targetIdx), Math.max(i, targetIdx)].join('-');
                            drawnConnections.add(connKey);
                        } else {
                            return;
                        }
                    }

                    // Connection color based on bandwidth
                    const color = getConnectionColor(bandwidth);

                    // Draw connection line (solid for explicit connections)
                    ctx.strokeStyle = color;
                    ctx.lineWidth = 3;
                    ctx.setLineDash([]); // Solid line for explicit

                    const sx = sourceNode.x + sourceNode.width;
                    const sy = sourceNode.y + sourceNode.height / 2;
                    const tx = targetX;
                    const ty = targetY + targetH / 2;

                    ctx.beginPath();
                    ctx.moveTo(sx, sy);
                    // Curved line
                    const cpx = (sx + tx) / 2;
                    ctx.quadraticCurveTo(cpx, sy, cpx, (sy + ty) / 2);
                    ctx.quadraticCurveTo(cpx, ty, tx, ty);
                    ctx.stroke();

                    // Draw arrow head
                    const angle = Math.atan2(ty - (sy + ty) / 2, tx - cpx);
                    ctx.fillStyle = color;
                    ctx.beginPath();
                    ctx.moveTo(tx, ty);
                    ctx.lineTo(tx - 10 * Math.cos(angle - Math.PI / 6), ty - 10 * Math.sin(angle - Math.PI / 6));
                    ctx.lineTo(tx - 10 * Math.cos(angle + Math.PI / 6), ty - 10 * Math.sin(angle + Math.PI / 6));
                    ctx.closePath();
                    ctx.fill();

                    // Bandwidth label
                    const midX = (sx + tx) / 2;
                    const midY = (sy + ty) / 2;
                    ctx.fillStyle = '#0f0f1a';
                    ctx.fillRect(midX - 35, midY - 10, 70, 18);
                    ctx.fillStyle = color;
                    ctx.font = 'bold 9px system-ui';
                    ctx.textAlign = 'center';
                    ctx.fillText(`${bandwidth} GB/s`, midX, midY + 3);
                }
            });

            // Auto-draw inferred connections for multi-device setups
            if (devices.length > 1) {
                for (let i = 0; i < devices.length; i++) {
                    for (let j = i + 1; j < devices.length; j++) {
                        const connKey = `${i}-${j}`;
                        if (drawnConnections.has(connKey)) continue;

                        const sourceNode = nodes[i];
                        const targetNode = nodes[j];
                        const deviceA = devices[i];
                        const deviceB = devices[j];

                        // Infer connection type
                        const inferred = inferConnectionType(deviceA, deviceB);
                        // Use actual network bandwidth if set, otherwise inferred
                        const bandwidth = Math.min(
                            deviceA.networkBandwidthGBps || inferred.bandwidth,
                            deviceB.networkBandwidthGBps || inferred.bandwidth
                        );
                        const color = getConnectionColor(bandwidth);

                        // Calculate connection points (center to center for mesh)
                        const sx = sourceNode.x + sourceNode.width / 2;
                        const sy = sourceNode.y + sourceNode.height / 2;
                        const tx = targetNode.x + targetNode.width / 2;
                        const ty = targetNode.y + targetNode.height / 2;

                        // Draw dashed line for inferred connections
                        ctx.strokeStyle = color;
                        ctx.lineWidth = 2;
                        ctx.globalAlpha = 0.5;
                        ctx.setLineDash([6, 4]);

                        ctx.beginPath();
                        ctx.moveTo(sx, sy);
                        ctx.lineTo(tx, ty);
                        ctx.stroke();

                        ctx.setLineDash([]);
                        ctx.globalAlpha = 1.0;

                        // Small bandwidth label at midpoint
                        const midX = (sx + tx) / 2;
                        const midY = (sy + ty) / 2;
                        ctx.fillStyle = 'rgba(15, 15, 26, 0.8)';
                        ctx.fillRect(midX - 25, midY - 8, 50, 14);
                        ctx.fillStyle = color;
                        ctx.globalAlpha = 0.7;
                        ctx.font = '8px system-ui';
                        ctx.textAlign = 'center';
                        ctx.fillText(`${bandwidth} GB/s`, midX, midY + 3);
                        ctx.globalAlpha = 1.0;
                    }
                }
            }

            // Draw nodes
            nodes.forEach((node) => {
                const device = node.device;
                const template = DEVICE_TEMPLATES[device.template] || {};
                const currentQuant = document.getElementById('quantizationType')?.value || 'q4';
                const tflops = device.computeTFlops?.[currentQuant] || 0;

                // Gradient background
                const gradient = ctx.createLinearGradient(node.x, node.y, node.x, node.y + node.height);
                gradient.addColorStop(0, 'rgba(59, 130, 246, 0.2)');
                gradient.addColorStop(1, 'rgba(59, 130, 246, 0.05)');
                ctx.fillStyle = gradient;
                ctx.strokeStyle = '#3b82f6';
                ctx.lineWidth = 2;
                ctx.beginPath();
                ctx.roundRect(node.x, node.y, node.width, node.height, 10);
                ctx.fill();
                ctx.stroke();

                // Device icon/type badge
                const typeLabel = template.type || 'Custom';
                ctx.fillStyle = 'rgba(255,255,255,0.1)';
                ctx.beginPath();
                ctx.roundRect(node.x + 8, node.y + 8, 50, 16, 4);
                ctx.fill();
                ctx.fillStyle = '#888';
                ctx.font = '9px system-ui';
                ctx.textAlign = 'left';
                ctx.fillText(typeLabel.substring(0, 8), node.x + 12, node.y + 19);

                // Device name
                ctx.fillStyle = '#fff';
                ctx.font = 'bold 13px system-ui';
                ctx.textAlign = 'center';
                const displayName = device.name.length > 24 ? device.name.substring(0, 22) + '...' : device.name;
                ctx.fillText(displayName, node.x + node.width/2, node.y + 38);

                // Specs line
                ctx.fillStyle = '#aaa';
                ctx.font = '11px system-ui';
                ctx.fillText(`${device.memoryGB} GB | ${device.localBandwidthGBps} GB/s`, node.x + node.width/2, node.y + 55);

                // Network/Interconnect info - always show
                ctx.font = '10px system-ui';
                ctx.fillStyle = '#888';
                const netBw = device.networkBandwidthGBps || 32;
                const netInfo = device.interconnectType
                    ? `${device.interconnectType.replace(/_/g, ' ')} (${netBw} GB/s)`
                    : `Net: ${netBw} GB/s`;
                ctx.fillText(netInfo, node.x + node.width/2, node.y + 72)

                // TFLOPs badge
                if (tflops > 0) {
                    ctx.fillStyle = 'rgba(74, 222, 128, 0.2)';
                    ctx.beginPath();
                    ctx.roundRect(node.x + node.width - 55, node.y + 8, 47, 16, 4);
                    ctx.fill();
                    ctx.fillStyle = '#4ade80';
                    ctx.font = 'bold 9px system-ui';
                    ctx.textAlign = 'center';
                    ctx.fillText(`${tflops}T`, node.x + node.width - 32, node.y + 19);
                }

                // Overflow indicator
                if (device.overflowTarget) {
                    ctx.fillStyle = 'rgba(251, 191, 36, 0.3)';
                    ctx.beginPath();
                    ctx.arc(node.x + node.width - 10, node.y + node.height - 10, 6, 0, Math.PI * 2);
                    ctx.fill();
                    ctx.fillStyle = '#fbbf24';
                    ctx.beginPath();
                    ctx.arc(node.x + node.width - 10, node.y + node.height - 10, 4, 0, Math.PI * 2);
                    ctx.fill();
                }
            });

            // Show legend for connection types
            if (devices.length > 1) {
                const hasExplicitConnections = devices.some(d => d.overflowTarget);
                ctx.font = '9px system-ui';
                ctx.textAlign = 'left';
                const legendY = height - 15;
                const legendX = 10;

                // Connection legend
                ctx.fillStyle = '#666';
                ctx.fillText('Connections:', legendX, legendY);

                // Solid = explicit
                ctx.strokeStyle = '#60a5fa';
                ctx.lineWidth = 2;
                ctx.setLineDash([]);
                ctx.beginPath();
                ctx.moveTo(legendX + 70, legendY - 4);
                ctx.lineTo(legendX + 90, legendY - 4);
                ctx.stroke();
                ctx.fillStyle = '#888';
                ctx.fillText('Explicit', legendX + 95, legendY);

                // Dashed = inferred
                ctx.strokeStyle = '#60a5fa';
                ctx.globalAlpha = 0.5;
                ctx.setLineDash([4, 3]);
                ctx.beginPath();
                ctx.moveTo(legendX + 145, legendY - 4);
                ctx.lineTo(legendX + 165, legendY - 4);
                ctx.stroke();
                ctx.setLineDash([]);
                ctx.globalAlpha = 1.0;
                ctx.fillStyle = '#888';
                ctx.fillText('Inferred', legendX + 170, legendY);

                if (!hasExplicitConnections) {
                    ctx.fillStyle = '#666';
                    ctx.font = 'italic 9px system-ui';
                    ctx.textAlign = 'right';
                    ctx.fillText('Set "Overflow Target" for explicit data flow', width - 10, legendY);
                }
            }
        }

        // Override updateDeviceDisplay to also update topology
        const _originalUpdateDeviceDisplay = updateDeviceDisplay;
        updateDeviceDisplay = function() {
            _originalUpdateDeviceDisplay();
            setTimeout(updateTopology, 50);
        };

        // Handle window resize
        window.addEventListener('resize', () => setTimeout(updateTopology, 100));

    </script>
</body>
</html>
